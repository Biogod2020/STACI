{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/xinyiz/pamrats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is adapted from https://github.com/tkipf/gae/blob/master/gae/train.py and https://github.com/tkipf/pygcn/blob/master/pygcn/train.py##\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import scanpy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "import image.loadImage as loadImage\n",
    "import gae.gae.optimizer as optimizer\n",
    "import image.modelsCNN as modelsCNN\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.colors\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSizes={}\n",
    "imageSizes['disease13']=(22210, 22344)\n",
    "imageSizes['control13']=(22355, 18953)\n",
    "imageSizes['disease8']=(22294, 19552)\n",
    "imageSizes['control8']=(22452, 19616)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "use_cuda=True\n",
    "fastmode=False #Validate during training pass\n",
    "seed=3\n",
    "epochs=10000\n",
    "saveFreq=10\n",
    "lr=0.0005 #initial learning rate\n",
    "lr_adv=0.001\n",
    "weight_decay=0 #Weight for L2 loss on embedding matrix.\n",
    "\n",
    "batchsize=16\n",
    "\n",
    "dropout=0.01\n",
    "testNodes=0.1 #fraction of total nodes for testing\n",
    "valNodes=0.05 #fraction of total nodes for validation\n",
    "# randFeatureSubset=None\n",
    "model_str='alexnet_regrs'\n",
    "\n",
    "kernel_size=4\n",
    "stride=2\n",
    "padding=1\n",
    "\n",
    "hidden1=64 #Number of channels in hidden layer 1\n",
    "hidden2=128 \n",
    "hidden3=256\n",
    "hidden4=512\n",
    "hidden5=512\n",
    "fc_dim1=512*25*25\n",
    "fc_dim2=1024\n",
    "\n",
    "pretrainedAE=None #{'name':'controlphy5XAbin_01_dca','epoch':9990}\n",
    "# training_samples=['control13','disease13','disease8','control8']\n",
    "training_samples=['disease13','control13']\n",
    "targetBatch=None\n",
    "switchFreq=1\n",
    "diamThresh_mul=800\n",
    "minThresh_mul=12\n",
    "overlap=int(diamThresh_mul*0.7)\n",
    "areaThresh=diamThresh_mul*diamThresh_mul*0.7\n",
    "plaqueMaskName='PlaqueMask'\n",
    "plaqueMaskImg='Maskofplaque.tif'\n",
    "name='cd13regrs_thresh25min12_overlap70area70_01'\n",
    "logsavepath='/mnt/external_ssd/xinyi/log/train_cnnRegrs_starmap/'+name\n",
    "modelsavepath='/mnt/external_ssd/xinyi/models/train_cnnRegrs_starmap/'+name\n",
    "plotsavepath=os.path.join('/mnt/external_ssd/xinyi/plots/train_cnnRegrs_starmap/'+name,'allk20XA_02_dca_over_leiden0.1_epoch9990')\n",
    "if not os.path.exists(plotsavepath):\n",
    "    os.mkdir(plotsavepath)\n",
    "    \n",
    "#Load data\n",
    "sampleidx={'disease13':'AD_mouse9494','control13':'AD_mouse9498','disease8':'AD_mouse9723','control8':'AD_mouse9735'}\n",
    "datadir='/home/xinyiz/2021-01-13-mAD-test-dataset'\n",
    "\n",
    "# Set cuda and seed\n",
    "np.random.seed(seed)\n",
    "if use_cuda and (not torch.cuda.is_available()):\n",
    "    print('cuda not available')\n",
    "    use_cuda=False\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "# Load data\n",
    "# if randFeatureSubset != None:\n",
    "#     idx=np.random.choice(features.shape[1],randFeatureSubset,replace=False)\n",
    "#     features=features[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaqueCentroids={}\n",
    "plaqueCentroids['disease13']=pd.read_csv('/home/xinyiz/2021-01-13-mAD-test-dataset/AD_mouse9494/trimmed_images/'+plaqueMaskName+'.csv', header=0)\n",
    "plaqueCentroids['disease8']=pd.read_csv('/home/xinyiz/2021-01-13-mAD-test-dataset/AD_mouse9723/trimmed_images/'+plaqueMaskName+'.csv', header=0)\n",
    "maxArea=max(np.max(plaqueCentroids['disease13']['Area']),np.max(plaqueCentroids['disease8']['Area']))\n",
    "plaqueCutoffRadius=max(int(np.sqrt(maxArea)/2),int(diamThresh_mul/2))\n",
    "# plaqueSizeFactor=maxArea/100\n",
    "plaqueSizeFactor=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaeClusterPath='/mnt/external_ssd/xinyi/plots/train_gae_starmap/allk20XA_02_dca_over/combinedlogminmax_beforeAct/cluster/leiden_nn10mdist025n_pcs40res0.1epoch9990'\n",
    "with open(gaeClusterPath, 'rb') as input:\n",
    "    gaeclusterlabels = pickle.load(input)\n",
    "\n",
    "plot_samples={'disease13':'AD_mouse9494','control13':'AD_mouse9498','disease8':'AD_mouse9723','control8':'AD_mouse9735'}\n",
    "\n",
    "gaeCoord=None\n",
    "sampleNames=None\n",
    "scaleddata=scanpy.read_h5ad(datadir+'/2020-12-27-starmap-mAD-raw.h5ad')\n",
    "for s in plot_samples.keys():\n",
    "    sampleidx_s=plot_samples[s] \n",
    "    if gaeCoord is None:\n",
    "        gaeCoord=scaleddata.obs.loc[scaleddata.obs['sample']==sampleidx_s,['y','x']].to_numpy()\n",
    "        sampleNames=np.repeat(s,np.sum(scaleddata.obs['sample']==sampleidx_s))\n",
    "    else:\n",
    "        gaeCoord=np.concatenate((gaeCoord,scaleddata.obs.loc[scaleddata.obs['sample']==sampleidx_s,['y','x']].to_numpy()),axis=0)\n",
    "        sampleNames=np.concatenate((sampleNames,np.repeat(s,np.sum(scaleddata.obs['sample']==sampleidx_s))),axis=None)\n",
    "gaeCoord=gaeCoord/0.3\n",
    "scaleddata=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotExprs(exprs,exprs2,savename,embedding,embedding2,savepath,savenameAdd='',norm=None):\n",
    "#     fig, ax = plt.subplots(dpi=400)\n",
    "    if not exprs is None:\n",
    "        plt.scatter(embedding[:,0],embedding[:,1],s=5,c=exprs,cmap='Greys',edgecolors='blue',linewidth=0.2,alpha=1,marker='o',norm=norm)\n",
    "    if not exprs2 is None:\n",
    "        plt.scatter(embedding2[:,0],embedding2[:,1],s=5,c=exprs2,cmap='Greys',edgecolors='green',linewidth=0.2,alpha=1,marker='^',norm=norm)\n",
    "    plt.colorbar(orientation='vertical', shrink = 0.5)\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "#     fig.set_figheight(5)\n",
    "#     fig.set_figwidth(5)\n",
    "    plt.savefig(os.path.join(savepath,savename+savenameAdd+'.jpg'),dpi=400)\n",
    "#     plt.show()\n",
    "    \n",
    "#     fig.clf()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotExprsDiff(exprsPos,exprsNeg,exprs2,savename,embeddingPos,embeddingNeg,embedding2,savepath,savenameAdd='',norm=None):\n",
    "#     fig, ax = plt.subplots(dpi=400)\n",
    "    if not exprsPos is None:\n",
    "        plt.scatter(embeddingPos[:,0],embeddingPos[:,1],s=5,c=exprsPos,cmap='Reds',edgecolors='blue',linewidth=0.2,alpha=1,marker='o',norm=norm)\n",
    "        plt.colorbar(orientation='vertical', shrink = 0.5)\n",
    "    if not exprsNeg is None:\n",
    "        plt.scatter(embeddingNeg[:,0],embeddingNeg[:,1],s=5,c=exprsNeg,cmap=plt.cm.get_cmap('Blues'),edgecolors='blue',linewidth=0.2,alpha=1,marker='o',norm=norm)\n",
    "        plt.colorbar(orientation='vertical', shrink = 0.5)\n",
    "    if not exprs2 is None:\n",
    "        plt.scatter(embedding2[:,0],embedding2[:,1],s=5,c=exprs2,cmap='Greys',edgecolors='green',linewidth=0.2,alpha=1,marker='^',norm=norm)\n",
    "        plt.colorbar(orientation='vertical', shrink = 0.5)\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "#     fig.set_figheight(5)\n",
    "#     fig.set_figwidth(5)\n",
    "    plt.savefig(os.path.join(savepath,savename+savenameAdd+'.jpg'),dpi=1200)\n",
    "#     plt.show()\n",
    "    \n",
    "#     fig.clf()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClusterLabels(clusterlabels,cellCoords,minPt,imgCoords,diamThresh,imgSize):\n",
    "    imgClusters=np.zeros(imgCoords.shape[0])-1\n",
    "    for i in range(imgCoords.shape[0]):\n",
    "        centroid=imgCoords[i]\n",
    "        rowstart=centroid[0]-diamThresh/2\n",
    "        rowend=min(rowstart+diamThresh,imgSize[0])\n",
    "        colstart=centroid[1]-diamThresh/2\n",
    "        colend=min(colstart+diamThresh,imgSize[1])\n",
    "        #find corresponding cluster\n",
    "        clusterIdxRow=np.logical_and(cellCoords[:,0]>=rowstart,cellCoords[:,0]<rowend)\n",
    "        clusterIdxCol=np.logical_and(cellCoords[:,1]>=colstart,cellCoords[:,1]<colend)\n",
    "        clusterRes=clusterlabels[np.logical_and(clusterIdxRow,clusterIdxCol)]\n",
    "        if clusterRes.size==0:\n",
    "            print('no cells')\n",
    "            continue\n",
    "        clusterResMode,modeCounts=stats.mode(clusterRes,axis=None)\n",
    "        if modeCounts[0]/clusterRes.size<minPt:\n",
    "            print('mode less than thresh')\n",
    "            continue\n",
    "        imgClusters[i]=clusterResMode[0]\n",
    "    return imgClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease13\n",
      "plaque1959\n",
      "no plaque2434\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "-1.0\n",
      "test results loss positive: 260084480.0000 loss negative: 0.0000\n",
      "test results loss positive binary: 0.0000 loss negative binary: 0.0000\n",
      "train results loss positive: 9972315.3333 loss negative: 0.0000\n",
      "train results loss positive binary: 1.0000 loss negative binary: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-bbc59ee8ad29>:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss_test_posLoss=np.sum(loss_test_all[posidx])/np.sum(posidx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val results loss positive: nan loss negative: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-bbc59ee8ad29>:88: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss_test_posLoss_binary=np.sum(loss_test_all_binary[posidx])/np.sum(posidx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val results loss positive binary: nan loss negative binary: 0.0000\n",
      "all results loss positive: 72500356.5000 loss negative: 0.0000\n",
      "all results loss positive binary: 0.7500 loss negative binary: 0.0000\n",
      "0.0\n",
      "test results loss positive: 91220876.6654 loss negative: 754544.7354\n",
      "test results loss positive binary: 0.1765 loss negative binary: 0.0667\n",
      "train results loss positive: 8585659.3649 loss negative: 0.0000\n",
      "train results loss positive binary: 0.0588 loss negative binary: 0.0000\n",
      "val results loss positive: 78802178.9917 loss negative: 0.0000\n",
      "val results loss positive binary: 0.0000 loss negative binary: 0.0000\n",
      "all results loss positive: 19001110.5874 loss negative: 87399.0041\n",
      "all results loss positive binary: 0.0636 loss negative binary: 0.0077\n",
      "1.0\n",
      "test results loss positive: 297148038.0560 loss negative: 19205.3887\n",
      "test results loss positive binary: 0.1277 loss negative binary: 0.2500\n",
      "train results loss positive: 20289986.7703 loss negative: 0.0000\n",
      "train results loss positive binary: 0.0913 loss negative binary: 0.0000\n",
      "val results loss positive: 90003079.1250 loss negative: 0.0000\n",
      "val results loss positive binary: 0.2500 loss negative binary: 0.0000\n",
      "all results loss positive: 50117295.7921 loss negative: 1536.4311\n",
      "all results loss positive binary: 0.1014 loss negative binary: 0.0200\n",
      "2.0\n",
      "test results loss positive: 15081800.6846 loss negative: 0.0000\n",
      "test results loss positive binary: 0.3636 loss negative binary: 0.0000\n",
      "train results loss positive: 3399562.6427 loss negative: 0.0000\n",
      "train results loss positive binary: 0.1412 loss negative binary: 0.0000\n",
      "val results loss positive: 204354412.7188 loss negative: 0.0000\n",
      "val results loss positive binary: 0.3333 loss negative binary: 0.0000\n",
      "all results loss positive: 10787130.0032 loss negative: 0.0000\n",
      "all results loss positive binary: 0.1717 loss negative binary: 0.0000\n",
      "3.0\n",
      "test results loss positive: 121811295.6500 loss negative: 0.0000\n",
      "test results loss positive binary: 0.1000 loss negative binary: 0.0000\n",
      "train results loss positive: 6244571.4601 loss negative: 56.9920\n",
      "train results loss positive binary: 0.0481 loss negative binary: 0.0130\n",
      "val results loss positive: 145595817.0227 loss negative: 288591560.0000\n",
      "val results loss positive binary: 0.0000 loss negative binary: 0.5000\n",
      "all results loss positive: 26324352.2910 loss negative: 13580830.9221\n",
      "all results loss positive binary: 0.0522 loss negative binary: 0.0353\n",
      "4.0\n",
      "test results loss positive: 123207592.4825 loss negative: 8289968.1829\n",
      "test results loss positive binary: 0.1224 loss negative binary: 0.1263\n",
      "train results loss positive: 8708340.8632 loss negative: 0.0000\n",
      "train results loss positive binary: 0.0600 loss negative binary: 0.0000\n",
      "val results loss positive: 109551241.4646 loss negative: 9313427.2000\n",
      "val results loss positive binary: 0.0000 loss negative binary: 0.0200\n",
      "all results loss positive: 24212135.9848 loss negative: 1223846.0326\n",
      "all results loss positive binary: 0.0625 loss negative binary: 0.0127\n",
      "5.0\n",
      "test results loss positive: 223992333.2708 loss negative: 1152169.4245\n",
      "test results loss positive binary: 0.1000 loss negative binary: 0.1042\n",
      "train results loss positive: 10051931.6403 loss negative: 5198.4663\n",
      "train results loss positive binary: 0.1597 loss negative binary: 0.0040\n",
      "val results loss positive: 315483885.3611 loss negative: 33713273.1429\n",
      "val results loss positive binary: 0.1667 loss negative binary: 0.0714\n",
      "all results loss positive: 51716222.6050 loss negative: 1683439.3945\n",
      "all results loss positive binary: 0.1538 loss negative binary: 0.0223\n",
      "6.0\n",
      "test results loss positive: 23646015.5000 loss negative: 453869.3125\n",
      "test results loss positive binary: 0.7500 loss negative binary: 0.0625\n",
      "train results loss positive: 1189841.8464 loss negative: 0.0000\n",
      "train results loss positive binary: 0.7000 loss negative binary: 0.0000\n",
      "val results loss positive: 130904.0000 loss negative: 0.0000\n",
      "val results loss positive binary: 1.0000 loss negative binary: 0.0000\n",
      "all results loss positive: 3033413.7842 loss negative: 51870.7786\n",
      "all results loss positive binary: 0.7234 loss negative binary: 0.0071\n",
      "7.0\n",
      "test results loss positive: 886478.3125 loss negative: 10529841.3333\n",
      "test results loss positive binary: 0.0000 loss negative binary: 0.0476\n",
      "train results loss positive: 257332.6953 loss negative: 11362.2780\n",
      "train results loss positive binary: 0.0000 loss negative binary: 0.0027\n",
      "val results loss positive: nan loss negative: 3166738.1429\n",
      "val results loss positive binary: nan loss negative binary: 0.0714\n",
      "all results loss positive: 467047.9010 loss negative: 1227249.0774\n",
      "all results loss positive binary: 0.0000 loss negative binary: 0.0115\n",
      "control13\n",
      "(22355, 18953)\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "-1.0\n",
      "test results loss positive: nan loss negative: 0.0000\n",
      "test results loss positive binary: nan loss negative binary: 0.0000\n",
      "train results loss positive: nan loss negative: 0.0000\n",
      "train results loss positive binary: nan loss negative binary: 0.0000\n",
      "val results loss positive: nan loss negative: 0.0000\n",
      "val results loss positive binary: nan loss negative binary: 0.0000\n",
      "all results loss positive: nan loss negative: 0.0000\n",
      "all results loss positive binary: nan loss negative binary: 0.0000\n",
      "0.0\n",
      "test results loss positive: nan loss negative: 158754.4300\n",
      "test results loss positive binary: nan loss negative binary: 0.0172\n",
      "train results loss positive: nan loss negative: 0.0000\n",
      "train results loss positive binary: nan loss negative binary: 0.0000\n",
      "val results loss positive: nan loss negative: 0.0000\n",
      "val results loss positive binary: nan loss negative binary: 0.0000\n",
      "all results loss positive: nan loss negative: 15593.1532\n",
      "all results loss positive binary: nan loss negative binary: 0.0017\n",
      "1.0\n",
      "test results loss positive: nan loss negative: 402739.7020\n",
      "test results loss positive binary: nan loss negative binary: 0.0320\n",
      "train results loss positive: nan loss negative: 263.2408\n",
      "train results loss positive binary: nan loss negative binary: 0.0010\n",
      "val results loss positive: nan loss negative: 0.0000\n",
      "val results loss positive binary: nan loss negative binary: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all results loss positive: nan loss negative: 44033.6357\n",
      "all results loss positive binary: nan loss negative binary: 0.0044\n",
      "2.0\n",
      "test results loss positive: nan loss negative: 18085.8147\n",
      "test results loss positive binary: nan loss negative binary: 0.0172\n",
      "train results loss positive: nan loss negative: 0.0000\n",
      "train results loss positive binary: nan loss negative binary: 0.0000\n",
      "val results loss positive: nan loss negative: 6099.8548\n",
      "val results loss positive binary: nan loss negative binary: 0.0417\n",
      "all results loss positive: nan loss negative: 2506.0247\n",
      "all results loss positive binary: nan loss negative binary: 0.0042\n",
      "3.0\n",
      "test results loss positive: nan loss negative: 4514290.0952\n",
      "test results loss positive binary: nan loss negative binary: 0.0238\n",
      "train results loss positive: nan loss negative: 11588.7331\n",
      "train results loss positive binary: nan loss negative binary: 0.0029\n",
      "val results loss positive: nan loss negative: 11618.5009\n",
      "val results loss positive binary: nan loss negative binary: 0.0303\n",
      "all results loss positive: nan loss negative: 478536.7538\n",
      "all results loss positive binary: nan loss negative binary: 0.0062\n",
      "4.0\n",
      "test results loss positive: nan loss negative: 241166.1921\n",
      "test results loss positive binary: nan loss negative binary: 0.0227\n",
      "train results loss positive: nan loss negative: 180.7102\n",
      "train results loss positive binary: nan loss negative binary: 0.0007\n",
      "val results loss positive: nan loss negative: 41905.1731\n",
      "val results loss positive binary: nan loss negative binary: 0.0110\n",
      "all results loss positive: nan loss negative: 25796.2151\n",
      "all results loss positive binary: nan loss negative binary: 0.0033\n",
      "5.0\n",
      "test results loss positive: nan loss negative: 1571242.0357\n",
      "test results loss positive binary: nan loss negative binary: 0.0429\n",
      "train results loss positive: nan loss negative: 2282.0498\n",
      "train results loss positive binary: nan loss negative binary: 0.0015\n",
      "val results loss positive: nan loss negative: 2175772.4444\n",
      "val results loss positive binary: nan loss negative binary: 0.0278\n",
      "all results loss positive: nan loss negative: 245270.2322\n",
      "all results loss positive binary: nan loss negative binary: 0.0065\n",
      "6.0\n",
      "test results loss positive: nan loss negative: 0.0000\n",
      "test results loss positive binary: nan loss negative binary: 0.0000\n",
      "train results loss positive: nan loss negative: 0.0000\n",
      "train results loss positive binary: nan loss negative binary: 0.0000\n",
      "val results loss positive: nan loss negative: 0.0000\n",
      "val results loss positive binary: nan loss negative binary: 0.0000\n",
      "all results loss positive: nan loss negative: 0.0000\n",
      "all results loss positive binary: nan loss negative binary: 0.0000\n",
      "7.0\n",
      "test results loss positive: nan loss negative: 0.0000\n",
      "test results loss positive binary: nan loss negative binary: 0.0000\n",
      "train results loss positive: nan loss negative: 0.0000\n",
      "train results loss positive binary: nan loss negative binary: 0.0000\n",
      "val results loss positive: nan loss negative: 0.0000\n",
      "val results loss positive binary: nan loss negative binary: 0.0000\n",
      "all results loss positive: nan loss negative: 0.0000\n",
      "all results loss positive binary: nan loss negative binary: 0.0000\n",
      "8.0\n",
      "test results loss positive: nan loss negative: 0.0000\n",
      "test results loss positive binary: nan loss negative binary: 0.0000\n",
      "train results loss positive: nan loss negative: 0.0000\n",
      "train results loss positive binary: nan loss negative binary: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-bbc59ee8ad29>:34: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss_test_negLoss=np.sum(loss_test_all[negidx])/np.sum(negidx)\n",
      "<ipython-input-10-bbc59ee8ad29>:89: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss_test_negLoss_binary=np.sum(loss_test_all_binary[negidx])/np.sum(negidx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val results loss positive: nan loss negative: nan\n",
      "val results loss positive binary: nan loss negative binary: nan\n",
      "all results loss positive: nan loss negative: 0.0000\n",
      "all results loss positive binary: nan loss negative binary: 0.0000\n",
      "disease8\n",
      "plaque758\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 19.3 GiB for an array with shape (4041, 1, 800, 800) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bbc59ee8ad29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'disease13'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'disease8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtrainInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabelsAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalLabelsAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabelsAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainCoordAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalCoordAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestCoordAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadandsplitPlaque_overlap_regrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaqueMaskImg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplaqueSizeFactor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mareaThresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplaqueCentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplaqueCutoffRadius\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msampleidx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiamThresh_mul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moverlap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalNodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestNodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mifFlip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminCutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminThresh_mul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturnPos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'control13'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'control8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtrainInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabelsAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalLabelsAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabelsAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainCoordAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalCoordAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestCoordAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadandsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleidx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiamThresh_mul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moverlap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalNodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestNodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mifFlip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminCutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminThresh_mul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturnPos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pamrats/image/loadImage.py\u001b[0m in \u001b[0;36mloadandsplitPlaque_overlap_regrs\u001b[0;34m(plaqueImageName, plaqueSizeFactor, areaThresh, coord, cutoffradius, samplename, imagedir, diamThresh, overlap, val, test, ifFlip, minCutoff, seed, split, imagename, minmaxscale, nchannels, returnPos)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;31m#             plaqueRes=np.concatenate((plaqueRes,imagerc.reshape((1,nchannels,imagerc.shape[0],imagerc.shape[1]))),axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;31m#             coord=np.concatenate((coord,np.array([r*stride+diamThresh/2,c*stride+diamThresh/2]).reshape((1,2))),axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0mplaqueRes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaqueRes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0mcoord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoordNeg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mlabelsRes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelsRes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnaddedplaque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 19.3 GiB for an array with shape (4041, 1, 800, 800) and data type float64"
     ]
    }
   ],
   "source": [
    "# lossCE_binary=torch.nn.CrossEntropyLoss(reduction='none')\n",
    "minPt=0\n",
    "savenameAdd='_thresh30'\n",
    "# savenameAdd='_thresh4445'\n",
    "lossThreshSize=30\n",
    "def plotLoss(inputNp,labelsNp,coordNp,name,plotsavepath,savenameAdd=''):\n",
    "    if not os.path.exists(plotsavepath):\n",
    "        os.mkdir(plotsavepath)\n",
    "    \n",
    "    loss_test_all=np.zeros(inputNp.shape[0])\n",
    "    loss_test_all_binary=np.zeros(inputNp.shape[0])\n",
    "    loss_test_all_diff=np.zeros(inputNp.shape[0])\n",
    "    for i in range(inputNp.shape[0]):\n",
    "        testInput=inputNp[[i]]\n",
    "        labels=labelsNp[[i]]\n",
    "        if use_cuda:\n",
    "            testInput=torch.tensor(testInput).cuda().float()\n",
    "            labels=torch.tensor(labels).cuda().float()\n",
    "        pred = model(testInput)\n",
    "        loss_test_all[i]=lossCE(pred.flatten(),labels).item()\n",
    "#         if pred[0]*labels[0]>0: \n",
    "        if labels[0]>0 and pred[0]>lossThreshSize:\n",
    "            loss_test_all_binary[i]=0\n",
    "#         elif pred[0]+labels[0]>0.001: #original\n",
    "        elif pred[0]>lossThreshSize or labels[0]>0:\n",
    "            loss_test_all_binary[i]=1\n",
    "        else:\n",
    "            loss_test_all_binary[i]=0\n",
    "        loss_test_all_diff[i]=pred.flatten()[0]-labels[0]\n",
    "        \n",
    "    posidx=(labelsNp>0)\n",
    "    negidx=labelsNp==0\n",
    "    loss_test_posLoss=np.sum(loss_test_all[posidx])/np.sum(posidx)\n",
    "    loss_test_negLoss=np.sum(loss_test_all[negidx])/np.sum(negidx)\n",
    "    print(name+' results',\n",
    "          'loss positive: {:.4f}'.format(loss_test_posLoss),\n",
    "         'loss negative: {:.4f}'.format(loss_test_negLoss))\n",
    "    \n",
    "    largeridx=np.logical_and(posidx,loss_test_all_diff>=0)\n",
    "    smalleridx=np.logical_and(posidx,loss_test_all_diff<0)\n",
    "    if np.sum(largeridx)>0:\n",
    "        fig, ax = plt.subplots(dpi=400)\n",
    "        fig.set_figheight(2.5)\n",
    "        fig.set_figwidth(10)\n",
    "        plt.xscale('log')\n",
    "        plt.hist(loss_test_all_diff[largeridx],bins=np.logspace(np.log10(np.min(loss_test_all_diff[largeridx])),np.log10(np.max(loss_test_all_diff[largeridx])),51))\n",
    "        plt.savefig(os.path.join(plotsavepath,name+'loss'+s+'_diffHist_positiveLarger'+'.jpg'))\n",
    "        plt.close()\n",
    "    if np.sum(smalleridx)>0:\n",
    "        fig, ax = plt.subplots(dpi=400)\n",
    "        fig.set_figheight(2.5)\n",
    "        fig.set_figwidth(10)\n",
    "        plt.xscale('log')\n",
    "        plt.hist(np.abs(loss_test_all_diff[smalleridx]),bins=np.logspace(np.log10(np.min(np.abs(loss_test_all_diff[smalleridx]))),np.log10(np.max(np.abs(loss_test_all_diff[smalleridx]))),51))\n",
    "        plt.savefig(os.path.join(plotsavepath,name+'loss'+s+'_diffHist_positiveSmaller'+'.jpg'))\n",
    "        plt.close()\n",
    "    if np.sum(negidx)>0:\n",
    "        fig, ax = plt.subplots(dpi=400)\n",
    "        fig.set_figheight(2.5)\n",
    "        fig.set_figwidth(10)\n",
    "        plt.xscale('log')\n",
    "        plt.hist(loss_test_all_diff[negidx]+0.1,bins=np.logspace(np.log10(np.min(loss_test_all_diff[negidx]+0.1)),np.log10(np.max(loss_test_all_diff[negidx]+0.1)),51))\n",
    "        plt.savefig(os.path.join(plotsavepath,name+'loss'+s+'_diffHist_negative'+'.jpg'))\n",
    "        plt.close()\n",
    "    \n",
    "    if np.sum(posidx)>0 and np.sum(negidx)>0:\n",
    "        plotExprs(loss_test_all[posidx],loss_test_all[negidx]+0.1,name+'loss'+s,coordNp[posidx],coordNp[negidx],plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "    if np.sum(posidx)>0:\n",
    "        plotExprs(loss_test_all[posidx],None,name+'loss'+s+'_positive',coordNp[posidx],None,plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "    if np.sum(negidx)>0:\n",
    "        plotExprs(None,loss_test_all[negidx]+0.1,name+'loss'+s+'_negative',None,coordNp[negidx],plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "\n",
    "    if np.sum(posidx)>0:\n",
    "        if np.sum(largeridx)>0 and np.sum(smalleridx)>0:\n",
    "            plotExprsDiff(loss_test_all_diff[largeridx]+0.1,np.abs(loss_test_all_diff[smalleridx]),loss_test_all_diff[negidx]+0.1,name+'loss'+s+'_diff',coordNp[largeridx],coordNp[smalleridx],coordNp[negidx],plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "            plotExprsDiff(loss_test_all_diff[largeridx]+0.1,np.abs(loss_test_all_diff[smalleridx]),None,name+'loss'+s+'_diff_positive',coordNp[largeridx],coordNp[smalleridx],None,plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "            plotExprsDiff(np.abs(loss_test_all_diff[smalleridx]),loss_test_all_diff[largeridx]+0.1,loss_test_all_diff[negidx]+0.1,name+'loss'+s+'_diff2',coordNp[smalleridx],coordNp[largeridx],coordNp[negidx],plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "            plotExprsDiff(np.abs(loss_test_all_diff[smalleridx]),loss_test_all_diff[largeridx]+0.1,None,name+'loss'+s+'_diff2_positive',coordNp[smalleridx],coordNp[largeridx],None,plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "        elif np.sum(largeridx)>0:\n",
    "            plotExprsDiff(loss_test_all_diff[largeridx]+0.1,None,loss_test_all_diff[negidx]+0.1,name+'loss'+s+'_diff',coordNp[largeridx],coordNp[smalleridx],coordNp[negidx],plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "            plotExprsDiff(loss_test_all_diff[largeridx]+0.1,None,None,name+'loss'+s+'_diff_positive',coordNp[largeridx],None,None,plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "        elif np.sum(smalleridx)>0:\n",
    "            plotExprsDiff(None,np.abs(loss_test_all_diff[smalleridx]),loss_test_all_diff[negidx]+0.1,name+'loss'+s+'_diff',None,coordNp[smalleridx],coordNp[negidx],plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "            plotExprsDiff(None,np.abs(loss_test_all_diff[smalleridx]),None,name+'loss'+s+'_diff_positive',None,coordNp[smalleridx],None,plotsavepath,savenameAdd=savenameAdd,norm=matplotlib.colors.LogNorm())\n",
    "            \n",
    "            \n",
    "            \n",
    "    loss_test_posLoss_binary=np.sum(loss_test_all_binary[posidx])/np.sum(posidx)\n",
    "    loss_test_negLoss_binary=np.sum(loss_test_all_binary[negidx])/np.sum(negidx)\n",
    "    print(name+' results',\n",
    "          'loss positive binary: {:.4f}'.format(loss_test_posLoss_binary),\n",
    "         'loss negative binary: {:.4f}'.format(loss_test_negLoss_binary))\n",
    "    \n",
    "    plotExprs(loss_test_all_binary[posidx],loss_test_all_binary[negidx],name+'loss'+s+'_binary',coordNp[posidx],coordNp[negidx],plotsavepath,savenameAdd=savenameAdd)\n",
    "    if np.sum(posidx)>0:\n",
    "        plotExprs(loss_test_all_binary[posidx],None,name+'loss'+s+'_positive'+'_binary',coordNp[posidx],None,plotsavepath,savenameAdd=savenameAdd)\n",
    "    plotExprs(None,loss_test_all_binary[negidx],name+'loss'+s+'_negative'+'_binary',None,coordNp[negidx],plotsavepath,savenameAdd=savenameAdd)\n",
    "\n",
    "    \n",
    "testepoch=920\n",
    "# Create model\n",
    "if model_str=='alexnet':\n",
    "    model = modelsCNN.AlexNet(2)\n",
    "    lossCE=torch.nn.CrossEntropyLoss(torch.tensor([negweight,posweight]).cuda().float())\n",
    "if model_str=='alexnet_regrs':\n",
    "    model = modelsCNN.AlexNet(1,regrs=True)\n",
    "    lossCE=torch.nn.MSELoss(reduction='none')\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(modelsavepath,str(testepoch)+'.pt')))\n",
    "model.eval()\n",
    "plotepoch='epoch'+str(testepoch)\n",
    "plotepoch+=savenameAdd\n",
    "for s in sampleidx.keys():\n",
    "#     if s in ['disease13']:\n",
    "#         continue\n",
    "    print(s)\n",
    "    if s in ['disease13','disease8']:\n",
    "        trainInputAll, valInputAll, testInputAll, trainLabelsAll,valLabelsAll,testLabelsAll,trainCoordAll,valCoordAll,testCoordAll=loadImage.loadandsplitPlaque_overlap_regrs(plaqueMaskImg,plaqueSizeFactor,areaThresh,plaqueCentroids[s][['Y','X']].to_numpy().astype(int),plaqueCutoffRadius,sampleidx[s],datadir,diamThresh_mul,overlap,valNodes,testNodes,ifFlip=False,minCutoff=minThresh_mul,seed=seed,returnPos=True)\n",
    "    if s in ['control13','control8']:\n",
    "        trainInputAll, valInputAll, testInputAll, trainLabelsAll,valLabelsAll,testLabelsAll,trainCoordAll,valCoordAll,testCoordAll=loadImage.loadandsplit(sampleidx[s],datadir,diamThresh_mul,overlap,valNodes,testNodes,ifFlip=False,minCutoff=minThresh_mul,seed=seed,clf=True,returnPos=True)\n",
    "        \n",
    "    trainClusterAll=getClusterLabels(gaeclusterlabels[sampleNames==s],gaeCoord[sampleNames==s],minPt,trainCoordAll,diamThresh_mul,imageSizes[s])\n",
    "    valClusterAll=getClusterLabels(gaeclusterlabels[sampleNames==s],gaeCoord[sampleNames==s],minPt,valCoordAll,diamThresh_mul,imageSizes[s])\n",
    "    testClusterAll=getClusterLabels(gaeclusterlabels[sampleNames==s],gaeCoord[sampleNames==s],minPt,testCoordAll,diamThresh_mul,imageSizes[s])\n",
    "\n",
    "    for c in np.unique(trainClusterAll):\n",
    "        print(c)\n",
    "    \n",
    "        cidx=trainClusterAll==float(c)\n",
    "        trainInputnp=trainInputAll[cidx]\n",
    "        trainLabelsnp=trainLabelsAll[cidx]\n",
    "        trainCoordnp=trainCoordAll[cidx]\n",
    "        vidx=valClusterAll==float(c)\n",
    "        valInputnp=valInputAll[vidx]\n",
    "        valLabelsnp=valLabelsAll[vidx]\n",
    "        valCoordnp=valCoordAll[vidx]\n",
    "        tidx=testClusterAll==float(c)\n",
    "        testInputnp=testInputAll[tidx]\n",
    "        testLabelsnp=testLabelsAll[tidx]\n",
    "        testCoordnp=testCoordAll[tidx]\n",
    "        \n",
    "        plotLoss(testInputnp,testLabelsnp,testCoordnp,'test',os.path.join(plotsavepath,str(c)),plotepoch)\n",
    "        plotLoss(trainInputnp,trainLabelsnp,trainCoordnp,'train',os.path.join(plotsavepath,str(c)),plotepoch)\n",
    "        plotLoss(valInputnp,valLabelsnp,valCoordnp,'val',os.path.join(plotsavepath,str(c)),plotepoch)\n",
    "        plotLoss(np.concatenate((trainInputnp,valInputnp,testInputnp),axis=0),np.concatenate((trainLabelsnp,valLabelsnp,testLabelsnp)),np.concatenate((trainCoordnp,valCoordnp,testCoordnp),axis=0),'all',os.path.join(plotsavepath,str(c)),plotepoch)\n",
    "    \n",
    "    trainInputnp, valInputnp, testInputnp, trainLabelsnp,valLabelsnp,testLabelsnp,trainCoordnp,valCoordnp,testCoordnp,trainClusterAll,valClusterAll,testClusterAll=None,None,None,None,None,None,None,None,None,None,None,None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
