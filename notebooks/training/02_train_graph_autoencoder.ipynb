{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import scanpy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import gae.gae.optimizer as optimizer\n",
    "import gae.gae.model\n",
    "import gae.gae.preprocessing as preprocessing\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" #this should be set to the GPU device you would like to use on your machine\n",
    "use_cuda=True #set to true if GPU is used \n",
    "fastmode=False #Perform validation during training pass\n",
    "seed=3 #random seed\n",
    "useSavedMaskedEdges=True #some edges of the adjacency matrices are held-out for validation; set to True to save and use saved version of the edge masks\n",
    "maskedgeName='knn20_connectivity'\n",
    "epochs=10000 #number of training epochs\n",
    "saveFreq=30 #the model parameters will be saved during training at a frequency defined by this parameter\n",
    "lr=0.001 #initial learning rate\n",
    "lr_adv=0.001 #this is ignored if not using an adversarial loss in the latent space (i.e. it is ignored for the default setup of STACI. If a discriminator is trained to use the adversarial loss, this is the learning rate of the discriminator.)\n",
    "weight_decay=0 #regularization term\n",
    "\n",
    "hidden1=6000 #Number of units in hidden layer 1\n",
    "hidden2=6000 #Number of units in hidden layer 2\n",
    "# hidden3=2048 # dimensions of additional hidden layers in the encoder, if more layers are specified\n",
    "# hidden4=2048\n",
    "# hidden5=128\n",
    "fc_dim1=6000 #Number of units in the fully connected layer of the decoder\n",
    "# fc_dim2=128 # dimensions of additional hidden layers in the decoder, if more layers are specified\n",
    "# fc_dim3=128\n",
    "# fc_dim4=128\n",
    "adv_hidden=128 #ignored if not using an adversarial loss in the latent space. This is the hidden units of the discriminator.\n",
    "\n",
    "dropout=0.01 #neural network dropout term\n",
    "testNodes=0.1 #fraction of total cells used for testing\n",
    "valNodes=0.05 #fraction of total cells used for validation\n",
    "XreconWeight=20  #reconstruction weight of the gene expression\n",
    "advWeight=2 # weight of the adversarial loss, if used\n",
    "model_str='gcn_vae_xa_e2_d1_dca' #specify which model to use (see definition below): 'gcn_vae_xa_e2_d1_dca' is the default full STACI model, 'fc1_dca' is the version without using cell location\n",
    "adv=None  # different choices of the adversarial loss, if used (as defined below): 'clf_fc1_eq', 'clf_fc1_control_eq', 'clf_fc1_control', 'clf_fc1'\n",
    "ridgeL=0.01 #regularization weight of the gene dropout parameter\n",
    "shareGenePi=True #ignored in the default model; This is a parameter to specify how if the gene dropout term is shared for some variants of the ZINB distribution modeling as discussed in the original deep count autoencoder paper.\n",
    "\n",
    "num_features=2112 #number of input genes\n",
    "training_samples=['control13','disease13','control8','disease8'] #names of the input samples used for training\n",
    "targetBatch=None #if adversarial loss is used, one possibility is to make all batches look like one target batch. None, if not using this option.\n",
    "training_sample_X='logminmax' #specify the normalization method for the gene expression input. 'logminmax' is the default that log transforms and min-max scales the expression. 'corrected' uses the z-score normalized and ComBat corrected data from Hu et al. 'scaled' uses the same normalization as 'corrected'.\n",
    "switchFreq=10 #the number of epochs spent on training the model using one sample, before switching to the next sample\n",
    "standardizeX=False #if perform additional z-score normalization of genes. Default is False.\n",
    "name='newModel' #name of the model\n",
    "useA=True #set to True to include adjacency loss as in the full STACI model\n",
    "\n",
    "#provide the paths to save the training log, trained models, and plots, and the path to the directory where the data is stored\n",
    "logsavepath='log/train_gae_starmap/'+name\n",
    "modelsavepath='models/train_gae_starmap/'+name\n",
    "plotsavepath='plots/train_gae_starmap/'+name\n",
    "datadir='2021-01-13-mAD-test-dataset'\n",
    "\n",
    "#Load data\n",
    "sampleidx={'disease13':'AD_mouse9494',\n",
    "           'control13':'AD_mouse9498',\n",
    "           'disease8':'AD_mouse9723',\n",
    "           'control8':'AD_mouse9735'} #this is formated as {name of the sample as used in 'training_samples':name of the sample as stored in the metadata}\n",
    "savedir=os.path.join('starmap') #where pre-computed adjacency matrices are stored\n",
    "adj_dir=os.path.join(savedir,'a')\n",
    "\n",
    "#normalize the gene expression or load the normalized gene expression from Hu et al.\n",
    "#batch information should be stored in the metadata as 'sample'\n",
    "featureslist={}\n",
    "if training_sample_X in ['corrected','scaled']:\n",
    "    scaleddata=scanpy.read_h5ad(datadir+'/2020-12-27-starmap-mAD-scaled.h5ad') #change to the h5ad file name of the input data\n",
    "    \n",
    "    for s in sampleidx.keys():\n",
    "        featureslist[s+'X_'+'corrected']=torch.tensor(scaleddata.layers['corrected'][scaleddata.obs['sample']==sampleidx[s]])\n",
    "        featureslist[s+'X_'+'scaled']=torch.tensor(scaleddata.layers['scaled'][scaleddata.obs['sample']==sampleidx[s]])\n",
    "\n",
    "else:\n",
    "    scaleddata=scanpy.read_h5ad(datadir+'/2020-12-27-starmap-mAD-raw.h5ad') #change to the h5ad file name of the input data\n",
    "    \n",
    "    for s in sampleidx.keys():\n",
    "        scaleddata_train=scaleddata.X[scaleddata.obs['sample']==sampleidx[s]]\n",
    "\n",
    "        if training_sample_X=='logminmax':\n",
    "            featurelog_train=np.log2(scaleddata_train+1/2)\n",
    "            scaler = MinMaxScaler()\n",
    "            featurelog_train_minmax=np.transpose(scaler.fit_transform(np.transpose(featurelog_train)))\n",
    "            featureslist[s+'X_'+training_sample_X]=torch.tensor(featurelog_train_minmax)\n",
    "        elif training_sample_X=='logminmax10':\n",
    "            featurelog_train=np.log2(scaleddata_train+1/2)\n",
    "            scaler = MinMaxScaler(feature_range=(0,10))\n",
    "            featurelog_train_minmax=np.transpose(scaler.fit_transform(np.transpose(featurelog_train)))\n",
    "            featureslist[s+'X_'+training_sample_X]=torch.tensor(featurelog_train_minmax)\n",
    "\n",
    "#load pre-computed adjacency matrices; adjust the file name as needed\n",
    "adj_list={}\n",
    "adj_list['disease13']=sp.load_npz(os.path.join(adj_dir,maskedgeName+'_AD_mouse9494.npz'))\n",
    "adj_list['control13']=sp.load_npz(os.path.join(adj_dir,maskedgeName+'_AD_mouse9498.npz'))\n",
    "adj_list['disease8']=sp.load_npz(os.path.join(adj_dir,maskedgeName+'_AD_mouse9723.npz'))\n",
    "adj_list['control8']=sp.load_npz(os.path.join(adj_dir,maskedgeName+'_AD_mouse9735.npz'))\n",
    "\n",
    "adjnormlist={}\n",
    "pos_weightlist={}\n",
    "normlist={}\n",
    "for ai in adj_list.keys():\n",
    "    adjnormlist[ai]=preprocessing.preprocess_graph(adj_list[ai])\n",
    "    \n",
    "    pos_weightlist[ai] = torch.tensor(float(adj_list[ai].shape[0] * adj_list[ai].shape[0] - adj_list[ai].sum()) / adj_list[ai].sum()) #using full unmasked adj\n",
    "    normlist[ai] = adj_list[ai].shape[0] * adj_list[ai].shape[0] / float((adj_list[ai].shape[0] * adj_list[ai].shape[0] - adj_list[ai].sum()) * 2)\n",
    "    \n",
    "    adj_label=adj_list[ai] + sp.eye(adj_list[ai].shape[0])\n",
    "    adj_list[ai]=torch.tensor(adj_label.todense())\n",
    "    \n",
    "if adv: #different choices of how adversarial loss is computed\n",
    "    if 'control_eq' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0.5,0.5]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([0.5,0.5]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['control13']=torch.tensor([1,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "    elif 'control' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0,1]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([1,0]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['control13']=torch.tensor([1,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,1]).expand(adjnormlist['control8'].shape[0],-1)        \n",
    "    elif 'eq' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['disease13']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['disease8']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['disease13']=torch.tensor([1,0,0,0]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_d['control13']=torch.tensor([0,1,0,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['disease8']=torch.tensor([0,0,1,0]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,0,0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "    else:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['disease13']=torch.tensor([0,1,1,1]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control13']=torch.tensor([1,0,1,1]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['disease8']=torch.tensor([1,1,0,1]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([1,1,1,0]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['disease13']=torch.tensor([1,0,0,0]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_d['control13']=torch.tensor([0,1,0,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['disease8']=torch.tensor([0,0,1,0]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,0,0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "if 'dca' in model_str:\n",
    "    rawdata=scanpy.read_h5ad(datadir+'/2020-12-27-starmap-mAD-raw.h5ad')\n",
    "    features_raw_list={}\n",
    "    for s in sampleidx.keys():\n",
    "        features_raw_list[s+'X_'+'raw']=torch.tensor(rawdata.X[rawdata.obs['sample']==sampleidx[s]])\n",
    "\n",
    "if standardizeX:\n",
    "    features=torch.tensor(scale(features,axis=0, with_mean=True, with_std=True, copy=True))\n",
    "\n",
    "# Set cuda and seed\n",
    "np.random.seed(seed)\n",
    "if use_cuda and (not torch.cuda.is_available()):\n",
    "    print('cuda not available')\n",
    "    use_cuda=False\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(logsavepath):\n",
    "    os.mkdir(logsavepath)\n",
    "if not os.path.exists(modelsavepath):\n",
    "    os.mkdir(modelsavepath)\n",
    "if not os.path.exists(plotsavepath):\n",
    "    os.mkdir(plotsavepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all train/validation sets\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "mse=torch.nn.MSELoss()\n",
    "# Create model\n",
    "if model_str=='gcn_vae_xa':\n",
    "    model = gae.gae.model.GCNModelVAE_XA(num_features, hidden1, hidden2,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "elif model_str == 'gcn_vae_gcnX_inprA':\n",
    "    model = gae.gae.model.GCNModelVAE_gcnX_inprA(num_features, hidden1, hidden2,gcn_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str == 'gcn_vae_gcnX_inprA_w':\n",
    "    model = gae.gae.model.GCNModelVAE_gcnX_inprA_w(num_features, hidden1, hidden2,gcn_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e3':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e3(num_features, hidden1, hidden2,hidden3,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str == 'gcn_vae_xa_e1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e1(num_features, hidden1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str == 'gcn_vae_xa_e2_d1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1(num_features, hidden1,hidden2, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e2_d1_dca':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dca_fca':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA_fca(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaFork':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCAfork(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaElemPi':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCAelemPi(num_features, hidden1,hidden2,fc_dim1, dropout,shareGenePi)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaConstantDisp':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA_constantDisp(num_features, hidden1,hidden2,fc_dim1, dropout,shareGenePi)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str == 'gcn_vae_xa_e4_d1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e4_d1(num_features, hidden1,hidden2,hidden3,hidden4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='fc':\n",
    "    model = gae.gae.model.FCVAE(num_features, hidden1, hidden2,hidden3,hidden4,hidden5,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "elif model_str=='fcae':\n",
    "    model = gae.gae.model.FCAE(num_features, hidden1, hidden2,hidden3,hidden4,hidden5,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "\n",
    "elif model_str=='fcae1':\n",
    "    model = gae.gae.model.FCAE1(num_features, dropout,hidden1)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "elif model_str=='fcae2':\n",
    "    model = gae.gae.model.FCAE2(num_features, dropout,hidden1,hidden2)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "\n",
    "elif model_str=='fc1':\n",
    "    model = gae.gae.model.FCVAE1(num_features, hidden1,dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='fc1_fca':\n",
    "    model = gae.gae.model.FCVAE1_fca(num_features, hidden1,dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='fc1_dca':\n",
    "    model = gae.gae.model.FCVAE1_DCA(num_features, hidden1,fc_dim1, dropout)\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "if adv=='clf_fc1' or adv=='clf_fc1_eq' or adv=='clf_fc1_control' or adv=='clf_fc1_control_eq':\n",
    "    modelAdv=gae.gae.model.Clf_fc1(hidden2, dropout,adv_hidden,sampleLabellist_ae['control13'].size()[1])\n",
    "    loss_adv=optimizer.optimizer_CEclf\n",
    "    \n",
    "if adv=='clf_linear1' or adv=='clf_linear1_control':\n",
    "    modelAdv=gae.gae.model.Clf_linear1(hidden2, dropout,sampleLabellist_ae['control13'].size()[1])\n",
    "    loss_adv=optimizer.optimizer_CEclf\n",
    "    \n",
    "if 'NB' in name:\n",
    "    print('using NB loss for X')\n",
    "    loss_x=optimizer.optimizer_nb\n",
    "    \n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    if adv:\n",
    "        modelAdv.cuda()\n",
    "    \n",
    "\n",
    "optimizerVAEXA = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "if adv:\n",
    "    optimizerAdv=optim.Adam(modelAdv.parameters(), lr=lr_adv, weight_decay=weight_decay)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(modelsavepath,str(9360)+'.pt')))\n",
    "# epochs=20000\n",
    "if pretrainedAE:\n",
    "    print('loading '+pretrainedAE['name']+' epoch '+str(pretrainedAE['epoch']))\n",
    "    model.load_state_dict(torch.load(os.path.join('/mnt/xinyi/pamrats/models/train_gae_starmap/'+pretrainedAE['name'],str(pretrainedAE['epoch'])+'.pt')))\n",
    "model.load_state_dict(torch.load(os.path.join(modelsavepath,str(5910)+'.pt')))\n",
    "# model.cuda()\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizerVAEXA.zero_grad()\n",
    "    \n",
    "    adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "        \n",
    "    \n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        modelAdv.eval()\n",
    "        advOut=modelAdv(z)\n",
    "    \n",
    "    loss_kl_train=loss_kl(mu, logvar, train_nodes_idx)\n",
    "    \n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight,mse)\n",
    "    \n",
    "    loss_a_train=loss_a(adj_recon, adj_label, pos_weight, norm,train_nodes_idx)\n",
    "    \n",
    "    \n",
    "    loss=loss_kl_train+loss_x_train #for lossXreconOnly_wKL only\n",
    "    if useA:\n",
    "        loss=loss+loss_a_train\n",
    "#     loss = loss_kl_train+loss_a_train #for lossAreconOnly_wKL only\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        loss_adv_train=loss_adv(advOut,sampleLabel_ae,train_nodes_idx)\n",
    "        loss+=loss_adv_train*advWeight\n",
    "    loss.backward()\n",
    "    optimizerVAEXA.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run & no variation in z.\n",
    "        model.eval()\n",
    "        adj_recon,mu,logvar,z, features_recon = model(features, adj_norm)\n",
    "    \n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        advOut=modelAdv(z)\n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight,mse)\n",
    "    \n",
    "    \n",
    "    loss_a_val=loss_a(adj_recon, adj_label, pos_weight, norm,val_nodes_idx)\n",
    "    \n",
    "    \n",
    "    loss_val=loss_x_val\n",
    "    if useA:\n",
    "        loss_val=loss_val+loss_a_val\n",
    "#     loss_val=loss_a_val\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        loss_adv_val=loss_adv(advOut,sampleLabel_ae,val_nodes_idx)\n",
    "        loss_val+=loss_adv_val*advWeight\n",
    "    print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "          'loss_train: {:.4f}'.format(loss.item()),\n",
    "          'loss_kl_train: {:.4f}'.format(loss_kl_train.item()),\n",
    "          'loss_x_train: {:.4f}'.format(loss_x_train.item()),\n",
    "          'loss_a_train: {:.4f}'.format(loss_a_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'loss_x_val: {:.4f}'.format(loss_x_val.item()),\n",
    "          'loss_a_val: {:.4f}'.format(loss_a_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        print('loss_adv_train: {:.4f}'.format(loss_adv_train.item()),\n",
    "              'loss_adv_val: {:.4f}'.format(loss_adv_val.item())\n",
    "             )\n",
    "    if adv:\n",
    "        if (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "            return float(loss),float(loss_kl_train),float(loss_x_train),float(loss_a_train),float(loss_val),float(loss_x_val),float(loss_a_val),float(loss_adv_train),float(loss_adv_val)        \n",
    "        else:\n",
    "            return float(loss),float(loss_kl_train),float(loss_x_train),float(loss_a_train),float(loss_val),float(loss_x_val),float(loss_a_val),None,None   \n",
    "    else:\n",
    "        return loss.item(),loss_kl_train.item(),loss_x_train.item(),loss_a_train.item(),loss_val.item(),loss_x_val.item(),loss_a_val.item()        \n",
    "\n",
    "def train_discriminator(epoch):\n",
    "    t = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "        \n",
    "    modelAdv.train()\n",
    "    optimizerAdv.zero_grad()\n",
    "    advOut=modelAdv(z)\n",
    "    \n",
    "    loss_adv_train=loss_adv(advOut,sampleLabel_d,train_nodes_idx)\n",
    "    loss = loss_adv_train*advWeight\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizerAdv.step()\n",
    "\n",
    "    modelAdv.eval()\n",
    "    advOut=modelAdv(z)\n",
    "    loss_adv_val=loss_adv(advOut,sampleLabel_d,val_nodes_idx)\n",
    "    loss_val=loss_adv_val*advWeight\n",
    "    print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "          'loss_adv_train: {:.4f}'.format(loss_adv_train.item()),\n",
    "          'loss_adv_val: {:.4f}'.format(loss_adv_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    return float(loss_adv_train),float(loss_adv_val)\n",
    "    \n",
    "# print('cross-validation ',seti)\n",
    "train_loss_ep=[None]*epochs\n",
    "train_loss_kl_ep=[None]*epochs\n",
    "train_loss_x_ep=[None]*epochs\n",
    "train_loss_a_ep=[None]*epochs\n",
    "train_loss_adv_ep=[None]*epochs\n",
    "train_loss_advD_ep=[None]*epochs\n",
    "val_loss_ep=[None]*epochs\n",
    "val_loss_x_ep=[None]*epochs\n",
    "val_loss_a_ep=[None]*epochs\n",
    "val_loss_adv_ep=[None]*epochs\n",
    "val_loss_advD_ep=[None]*epochs\n",
    "t_ep=time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t=int(ep/switchFreq)%len(training_samples)\n",
    "    training_samples_t=training_samples[t]\n",
    "\n",
    "    adj_norm=adjnormlist[training_samples_t].cuda().float()\n",
    "    adj_label=adj_list[training_samples_t].cuda().float()\n",
    "    features=featureslist[training_samples_t+'X_'+training_sample_X].cuda().float()\n",
    "    pos_weight=pos_weightlist[training_samples_t]\n",
    "    norm=normlist[training_samples_t]\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())):\n",
    "        sampleLabel_ae=sampleLabellist_ae[training_samples_t].cuda().float()\n",
    "        sampleLabel_d=sampleLabellist_d[training_samples_t].cuda().float()\n",
    "    if 'dca' in model_str:\n",
    "        features_raw=features_raw_list[training_samples_t+'X_raw'].cuda()\n",
    "    num_nodes,_ = features.shape\n",
    "    \n",
    "    maskpath=os.path.join(savedir,'trainMask',training_samples_t+'_'+maskedgeName+'_seed'+str(seed)+'.pkl')\n",
    "    if useSavedMaskedEdges and os.path.exists(maskpath):\n",
    "        with open(maskpath, 'rb') as input:\n",
    "            maskedgeres = pickle.load(input)\n",
    "    else:\n",
    "        # construct training, validation, and test sets\n",
    "        maskedgeres= preprocessing.mask_nodes_edges(features.shape[0],testNodeSize=testNodes,valNodeSize=valNodes,seed=seed)\n",
    "        with open(maskpath, 'wb') as output:\n",
    "            pickle.dump(maskedgeres, output, pickle.HIGHEST_PROTOCOL)\n",
    "    train_nodes_idx,val_nodes_idx,test_nodes_idx = maskedgeres\n",
    "    if use_cuda:\n",
    "        train_nodes_idx=train_nodes_idx.cuda()\n",
    "        val_nodes_idx=val_nodes_idx.cuda()\n",
    "        test_nodes_idx=test_nodes_idx.cuda()\n",
    "    \n",
    "    if adv:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep],train_loss_adv_ep[ep],val_loss_adv_ep[ep]=train(ep)\n",
    "        if (training_samples_t in list(sampleLabellist_ae.keys())):\n",
    "            train_loss_advD_ep[ep],val_loss_advD_ep[ep]=train_discriminator(ep)\n",
    "    else:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep]=train(ep)\n",
    "\n",
    "        \n",
    "    if ep%saveFreq == 0:\n",
    "        torch.save(model.cpu().state_dict(), os.path.join(modelsavepath,str(ep)+'.pt'))\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()\n",
    "print(' total time: {:.4f}s'.format(time.time() - t_ep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(logsavepath,'train_loss'), 'wb') as output:\n",
    "    pickle.dump(train_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_kl'), 'wb') as output:\n",
    "    pickle.dump(train_loss_kl_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_x'), 'wb') as output:\n",
    "    pickle.dump(train_loss_x_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_a'), 'wb') as output:\n",
    "    pickle.dump(train_loss_a_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss'), 'wb') as output:\n",
    "    pickle.dump(val_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss_x'), 'wb') as output:\n",
    "    pickle.dump(val_loss_x_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss_a'), 'wb') as output:\n",
    "    pickle.dump(val_loss_a_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "if adv:\n",
    "    with open(os.path.join(logsavepath,'train_loss_adv'), 'wb') as output:\n",
    "        pickle.dump(train_loss_adv_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_adv'), 'wb') as output:\n",
    "        pickle.dump(val_loss_adv_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'train_loss_advD'), 'wb') as output:\n",
    "        pickle.dump(train_loss_advD_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_advD'), 'wb') as output:\n",
    "        pickle.dump(val_loss_advD_ep, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs),train_loss_x_ep)\n",
    "plt.plot(np.arange(epochs),val_loss_x_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_a_ep)\n",
    "plt.plot(np.arange(epochs),val_loss_a_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "plt.legend(['training x recon loss','validation x recon loss','training a recon loss','validation a recon loss','training kl loss'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath,'loss_seed3.jpg'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compute test loss\n",
    "testepoch=9420\n",
    "model.load_state_dict(torch.load(os.path.join(modelsavepath,str(testepoch)+'.pt')))\n",
    "model.eval()\n",
    "for s in sampleidx.keys():\n",
    "    print(s)\n",
    "    \n",
    "    adj_norm=adjnormlist[s].cuda().float()\n",
    "    adj_label=adj_list[s].cuda().float()\n",
    "    features=featureslist[s+'X_'+training_sample_X].cuda().float()\n",
    "    pos_weight=pos_weightlist[s]\n",
    "    norm=normlist[s]\n",
    "    \n",
    "    if 'dca' in model_str:\n",
    "        features_raw=features_raw_list[s+'X_raw'].cuda()\n",
    "    num_nodes,num_features = features.shape\n",
    "    maskpath=os.path.join(savedir,'trainMask',s+'_'+maskedgeName+'_seed'+str(seed)+'.pkl')\n",
    "    if useSavedMaskedEdges and os.path.exists(maskpath):\n",
    "#         print('opening saved')\n",
    "        with open(maskpath, 'rb') as input:\n",
    "            maskedgeres = pickle.load(input)\n",
    "    else:\n",
    "        # construct training, validation, and test sets\n",
    "        maskedgeres= preprocessing.mask_nodes_edges(features.shape[0],testNodeSize=testNodes,valNodeSize=valNodes)\n",
    "        with open(maskpath, 'wb') as output:\n",
    "            pickle.dump(maskedgeres, output, pickle.HIGHEST_PROTOCOL)\n",
    "    train_nodes_idx,val_nodes_idx,test_nodes_idx = maskedgeres\n",
    "    \n",
    "    if s in training_samples:\n",
    "        test_nodes_idx_s=test_nodes_idx\n",
    "    else:\n",
    "        test_nodes_idx_s=torch.tensor(np.arange(num_nodes))\n",
    "        \n",
    "\n",
    "    adj_recon,mu,logvar,z, features_recon = model(features, adj_norm)\n",
    "    if adv and (s in list(sampleLabellist_ae.keys())):\n",
    "        sampleLabel_ae=sampleLabellist_ae[s].cuda().float()\n",
    "        modelAdv.eval()\n",
    "        advOut=modelAdv(z)\n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_test=loss_x(features_recon, features,test_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_test=loss_x(features_recon, features,test_nodes_idx_s,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_test=loss_x(features_recon, features,test_nodes_idx_s,XreconWeight,mse)\n",
    "    loss_a_test=loss_a(adj_recon, adj_label, pos_weight, norm,test_nodes_idx_s)\n",
    "    loss_test = loss_x_test+loss_a_test\n",
    "    \n",
    "    if adv and (s in list(sampleLabellist_ae.keys())):\n",
    "        loss_adv_test=loss_adv(advOut,sampleLabel_ae,test_nodes_idx)\n",
    "        print('loss_adv_test: {:.4f}'.format(loss_adv_test.item()))\n",
    "        \n",
    "    print('test results',\n",
    "          'loss_test: {:.4f}'.format(loss_test.item()),\n",
    "          'loss_x_test: {:.4f}'.format(loss_x_test.item()),\n",
    "          'loss_a_test: {:.4f}'.format(loss_a_test.item()))\n",
    "#          'loss_adv_test: {:.4f}'.format(loss_adv_test.item()))\n",
    "    if protein:\n",
    "        test_nodes_idx_s_genes=torch.clone(test_nodes_idx_s)\n",
    "        test_nodes_idx_s_genes[2112:]=0\n",
    "        test_nodes_idx_s_proteins=torch.clone(test_nodes_idx_s)\n",
    "        test_nodes_idx_s_proteins[:2112]=0\n",
    "        if 'dca' in model_str:\n",
    "            loss_genes_test=loss_x(features_recon, features,test_nodes_idx_s_genes,XreconWeight,ridgeL,features_raw)\n",
    "            loss_proteins_test=loss_x(features_recon, features,test_nodes_idx_s_proteins,XreconWeight,ridgeL,features_raw)\n",
    "        print('loss_x_genes: {:.4f}'.format(loss_genes_test.item()),\n",
    "          'loss_x_proteins: {:.4f}'.format(loss_proteins_test.item()))\n",
    "    if 'dca' in model_str:\n",
    "        features_raw=features_raw.cpu()\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
