{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/xinyiz/pamrats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is adapted from https://github.com/tkipf/gae/blob/master/gae/train.py and https://github.com/tkipf/pygcn/blob/master/pygcn/train.py##\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import scanpy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "import image.loadImage as loadImage\n",
    "import gae.gae.optimizer as optimizer\n",
    "import image.modelsCNN as modelsCNN\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n",
      "no cells\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "use_cuda=True\n",
    "fastmode=False #Validate during training pass\n",
    "seed=3\n",
    "epochs=5000\n",
    "saveFreq=20\n",
    "lr=0.00001 #initial learning rate\n",
    "lr_adv=0.001\n",
    "weight_decay=0 #Weight for L2 loss on embedding matrix.\n",
    "\n",
    "batchsize=1\n",
    "kernel_size=4\n",
    "stride=2\n",
    "padding=1\n",
    "\n",
    "hidden1=128 #Number of channels in hidden layer 1\n",
    "hidden2=256 \n",
    "hidden3=512\n",
    "hidden4=1024\n",
    "hidden5=1024\n",
    "fc_dim1=1024*19*19\n",
    "fc_dim2=1024\n",
    "# fc_dim3=128\n",
    "# fc_dim4=128\n",
    "# gcn_dim1=2600\n",
    "adv_hidden=128\n",
    "\n",
    "dropout=0.01\n",
    "testNodes=0.1 #fraction of total nodes for testing\n",
    "valNodes=0.05 #fraction of total nodes for validation\n",
    "# clfweight=20\n",
    "advWeight=2\n",
    "# randFeatureSubset=None\n",
    "kl_weight=0.0000001\n",
    "model_str='cnn_vae'\n",
    "adv=None  #'clf_fc1_eq'  #'clf_fc1_control_eq' #'clf_fc1_control'  #'clf_fc1'\n",
    "\n",
    "pretrainedAE=None #{'name':'controlphy5XAbin_01_dca','epoch':9990}\n",
    "training_samples=['control13','disease13','disease8','control8']\n",
    "# training_samples=['control13','control8']\n",
    "targetBatch=None\n",
    "switchFreq=20\n",
    "diamThresh_mul=608\n",
    "minThresh_mul=6\n",
    "name='all_thresh15min6_01'\n",
    "logsavepath='/mnt/external_ssd/xinyi/log/train_cnn_starmap/'+name\n",
    "modelsavepath='/mnt/external_ssd/xinyi/models/train_cnn_starmap/'+name\n",
    "plotsavepath='/mnt/external_ssd/xinyi/plots/train_cnn_starmap/'+name\n",
    "\n",
    "#Load data\n",
    "sampleidx={'disease13':'AD_mouse9494','control13':'AD_mouse9498','disease8':'AD_mouse9723','control8':'AD_mouse9735'}\n",
    "datadir='/home/xinyiz/2021-01-13-mAD-test-dataset'\n",
    "\n",
    "imageslist={}\n",
    "for s in sampleidx.keys():\n",
    "    imageslist[s]=loadImage.loadandsplit(sampleidx[s],datadir,diamThresh_mul,valNodes,testNodes,ifFlip=True,minCutoff=minThresh_mul,seed=seed)\n",
    "    \n",
    "if adv:\n",
    "    if 'control_eq' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0.5,0.5]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([0.5,0.5]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['control13']=torch.tensor([1,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "    elif 'control' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0,1]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([1,0]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['control13']=torch.tensor([1,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,1]).expand(adjnormlist['control8'].shape[0],-1)        \n",
    "    elif 'eq' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['disease13']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['disease8']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['disease13']=torch.tensor([1,0,0,0]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_d['control13']=torch.tensor([0,1,0,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['disease8']=torch.tensor([0,0,1,0]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,0,0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "    else:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['disease13']=torch.tensor([0,1,1,1]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control13']=torch.tensor([1,0,1,1]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['disease8']=torch.tensor([1,1,0,1]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([1,1,1,0]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['disease13']=torch.tensor([1,0,0,0]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_d['control13']=torch.tensor([0,1,0,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['disease8']=torch.tensor([0,0,1,0]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,0,0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "        \n",
    "# Set cuda and seed\n",
    "np.random.seed(seed)\n",
    "if use_cuda and (not torch.cuda.is_available()):\n",
    "    print('cuda not available')\n",
    "    use_cuda=False\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "# Load data\n",
    "# if randFeatureSubset != None:\n",
    "#     idx=np.random.choice(features.shape[1],randFeatureSubset,replace=False)\n",
    "#     features=features[:,idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(logsavepath):\n",
    "    os.mkdir(logsavepath)\n",
    "if not os.path.exists(modelsavepath):\n",
    "    os.mkdir(modelsavepath)\n",
    "if not os.path.exists(plotsavepath):\n",
    "    os.mkdir(plotsavepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=torch.nn.MSELoss()\n",
    "# mse=torch.nn.MSELoss(reduction=None)\n",
    "# Create model\n",
    "if model_str=='cnn_vae':\n",
    "    model = modelsCNN.CNN_VAE(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5, fc_dim1,fc_dim2)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=mse\n",
    "\n",
    "if adv=='clf_fc1' or adv=='clf_fc1_eq' or adv=='clf_fc1_control' or adv=='clf_fc1_control_eq':\n",
    "    modelAdv=gae.gae.model.Clf_fc1(hidden2, dropout,adv_hidden,sampleLabellist_ae['control13'].size()[1])\n",
    "    loss_adv=optimizer.optimizer_CEclf\n",
    "    \n",
    "if adv=='clf_linear1' or adv=='clf_linear1_control':\n",
    "    modelAdv=gae.gae.model.Clf_linear1(hidden2, dropout,sampleLabellist_ae['control13'].size()[1])\n",
    "    loss_adv=optimizer.optimizer_CEclf\n",
    "        \n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    if adv:\n",
    "        modelAdv.cuda()\n",
    "    \n",
    "\n",
    "optimizerVAEXA = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "if adv:\n",
    "    optimizerAdv=optim.Adam(modelAdv.parameters(), lr=lr_adv, weight_decay=weight_decay)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0000 loss_train: 0.0776 loss_kl_train: 6492.7821 loss_x_train: 0.0769 loss_x_val: 0.0490 time: 276.8244s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0001 loss_train: 0.0463 loss_kl_train: 50202.3385 loss_x_train: 0.0413 loss_x_val: 0.0388 time: 276.7166s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0002 loss_train: 0.0397 loss_kl_train: 51478.4944 loss_x_train: 0.0346 loss_x_val: 0.0453 time: 276.6890s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0003 loss_train: 0.0649 loss_kl_train: 397120.0411 loss_x_train: 0.0252 loss_x_val: 0.0346 time: 276.6838s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0004 loss_train: 593.3840 loss_kl_train: 5933621357.8244 loss_x_train: 0.0219 loss_x_val: 0.0190 time: 276.7222s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0005 loss_train: 0.2550 loss_kl_train: 2350053.9139 loss_x_train: 0.0200 loss_x_val: 0.0235 time: 276.7859s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0006 loss_train: 106.3725 loss_kl_train: 1063565150.8785 loss_x_train: 0.0160 loss_x_val: 0.0219 time: 276.7229s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0007 loss_train: 7778089.2005 loss_kl_train: 77780889313391.1250 loss_x_train: 0.0113 loss_x_val: 0.0224 time: 276.8966s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0008 loss_train: 8.6643 loss_kl_train: 86555307.9469 loss_x_train: 0.0088 loss_x_val: 0.0202 time: 276.9399s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0009 loss_train: 2.9002 loss_kl_train: 28930182.3296 loss_x_train: 0.0072 loss_x_val: 0.0217 time: 276.9949s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0010 loss_train: 0.5192 loss_kl_train: 5127581.3415 loss_x_train: 0.0064 loss_x_val: 0.0212 time: 276.9502s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0011 loss_train: 1442.5977 loss_kl_train: 14425919638.1614 loss_x_train: 0.0056 loss_x_val: 0.0189 time: 276.9898s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0012 loss_train: 0.2130 loss_kl_train: 2079578.9276 loss_x_train: 0.0050 loss_x_val: 0.0197 time: 276.9434s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0013 loss_train: 1342195.3434 loss_kl_train: 13421953352300.2871 loss_x_train: 0.0048 loss_x_val: 0.0176 time: 276.9395s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0014 loss_train: 26.4759 loss_kl_train: 264707672.0274 loss_x_train: 0.0052 loss_x_val: 0.0221 time: 276.9008s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0015 loss_train: 156.0876 loss_kl_train: 1560824121.2881 loss_x_train: 0.0052 loss_x_val: 0.0170 time: 276.9743s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0016 loss_train: 175488.6426 loss_kl_train: 1754886336296.0828 loss_x_train: 0.0048 loss_x_val: 0.0166 time: 276.9450s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0017 loss_train: 112758316569.1109 loss_kl_train: 1127583121432815744.0000 loss_x_train: 0.0065 loss_x_val: 0.0223 time: 276.9956s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0018 loss_train: 6.5450 loss_kl_train: 65409667.3796 loss_x_train: 0.0040 loss_x_val: 0.0107 time: 276.9222s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0019 loss_train: 2.3211 loss_kl_train: 23174936.4764 loss_x_train: 0.0036 loss_x_val: 0.0128 time: 277.0181s\n",
      "disease13 Epoch: 0020 loss_train: 1390.3604 loss_kl_train: 13903524463.7218 loss_x_train: 0.0080 loss_x_val: 0.0130 time: 330.7638s\n",
      "disease13 Epoch: 0021 loss_train: 40926526657949984.0000 loss_kl_train: 409265269271784914419712.0000 loss_x_train: 0.0067 loss_x_val: 0.0174 time: 330.7286s\n",
      "disease13 Epoch: 0022 loss_train: 7326.3361 loss_kl_train: 73263301841.0625 loss_x_train: 0.0060 loss_x_val: 0.0140 time: 330.6963s\n",
      "disease13 Epoch: 0023 loss_train: 69033.1313 loss_kl_train: 690331257874.4313 loss_x_train: 0.0047 loss_x_val: 0.0209 time: 330.7134s\n",
      "disease13 Epoch: 0024 loss_train: 29678.5258 loss_kl_train: 296785209766.2104 loss_x_train: 0.0044 loss_x_val: 0.0151 time: 330.7211s\n",
      "disease13 Epoch: 0025 loss_train: 116493.2974 loss_kl_train: 1164932915160.3350 loss_x_train: 0.0041 loss_x_val: 0.0156 time: 330.7520s\n",
      "disease13 Epoch: 0026 loss_train: 0.6990 loss_kl_train: 6951868.5064 loss_x_train: 0.0038 loss_x_val: 0.0166 time: 330.6907s\n",
      "disease13 Epoch: 0027 loss_train: 40890859.3829 loss_kl_train: 408908606542972.0000 loss_x_train: 0.0036 loss_x_val: 0.0269 time: 330.7429s\n",
      "disease13 Epoch: 0028 loss_train: 80.5079 loss_kl_train: 805047084.2205 loss_x_train: 0.0032 loss_x_val: 0.0157 time: 330.7847s\n",
      "disease13 Epoch: 0029 loss_train: 0.1649 loss_kl_train: 1616733.0749 loss_x_train: 0.0032 loss_x_val: 0.0157 time: 330.7886s\n",
      "disease13 Epoch: 0030 loss_train: 231.2533 loss_kl_train: 2312498948.7848 loss_x_train: 0.0034 loss_x_val: 0.0167 time: 330.7718s\n",
      "disease13 Epoch: 0031 loss_train: 13816861.9855 loss_kl_train: 138168618286697.1094 loss_x_train: 0.0026 loss_x_val: 0.0173 time: 330.7285s\n",
      "disease13 Epoch: 0032 loss_train: 0.1727 loss_kl_train: 1698431.0333 loss_x_train: 0.0029 loss_x_val: 0.0177 time: 330.7527s\n",
      "disease13 Epoch: 0033 loss_train: 208158813880.6741 loss_kl_train: 2081588047074275072.0000 loss_x_train: 0.0028 loss_x_val: 0.0192 time: 330.7545s\n",
      "disease13 Epoch: 0034 loss_train: 0.0186 loss_kl_train: 160209.0571 loss_x_train: 0.0026 loss_x_val: 0.0182 time: 330.7170s\n",
      "disease13 Epoch: 0035 loss_train: 9.2679 loss_kl_train: 92652678.5011 loss_x_train: 0.0026 loss_x_val: 0.0200 time: 330.7766s\n",
      "disease13 Epoch: 0036 loss_train: 0.0481 loss_kl_train: 458199.0244 loss_x_train: 0.0023 loss_x_val: 0.0211 time: 330.7516s\n",
      "disease13 Epoch: 0037 loss_train: 0.0227 loss_kl_train: 202347.4729 loss_x_train: 0.0025 loss_x_val: 0.0192 time: 330.7020s\n",
      "disease13 Epoch: 0038 loss_train: 0.0283 loss_kl_train: 259041.5675 loss_x_train: 0.0024 loss_x_val: 0.0193 time: 330.7558s\n",
      "disease13 Epoch: 0039 loss_train: 0.1297 loss_kl_train: 1274706.1044 loss_x_train: 0.0023 loss_x_val: 0.0265 time: 330.7954s\n",
      "disease8 Epoch: 0040 loss_train: 0.2275 loss_kl_train: 2198221.7373 loss_x_train: 0.0077 loss_x_val: 0.0243 time: 286.1574s\n",
      "disease8 Epoch: 0041 loss_train: 63538.7378 loss_kl_train: 635387319305.6167 loss_x_train: 0.0052 loss_x_val: 0.0205 time: 286.1296s\n",
      "disease8 Epoch: 0042 loss_train: 3.1028 loss_kl_train: 30982878.0229 loss_x_train: 0.0045 loss_x_val: 0.0681 time: 286.1641s\n",
      "disease8 Epoch: 0043 loss_train: 208436412578.8767 loss_kl_train: 2084364032074351360.0000 loss_x_train: 0.0041 loss_x_val: 0.0262 time: 286.0940s\n",
      "disease8 Epoch: 0044 loss_train: 317.4696 loss_kl_train: 3174653419.4181 loss_x_train: 0.0042 loss_x_val: 0.0209 time: 286.0926s\n",
      "disease8 Epoch: 0045 loss_train: 6781013362281136.0000 loss_kl_train: 67810130362975788204032.0000 loss_x_train: 0.0034 loss_x_val: 0.0175 time: 286.0921s\n",
      "disease8 Epoch: 0046 loss_train: 1.1011 loss_kl_train: 10976002.2444 loss_x_train: 0.0035 loss_x_val: 0.0160 time: 285.9965s\n",
      "disease8 Epoch: 0047 loss_train: 571.9192 loss_kl_train: 5719159650.4103 loss_x_train: 0.0033 loss_x_val: 0.0145 time: 286.1029s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease8 Epoch: 0048 loss_train: 4.7047 loss_kl_train: 47015568.7123 loss_x_train: 0.0031 loss_x_val: 0.0148 time: 286.1253s\n",
      "disease8 Epoch: 0049 loss_train: 17.0550 loss_kl_train: 170521795.6637 loss_x_train: 0.0028 loss_x_val: 0.0203 time: 286.1350s\n",
      "disease8 Epoch: 0050 loss_train: 0.0730 loss_kl_train: 703929.1640 loss_x_train: 0.0026 loss_x_val: 0.0238 time: 286.0322s\n",
      "disease8 Epoch: 0051 loss_train: 106.1444 loss_kl_train: 1061415072.1456 loss_x_train: 0.0029 loss_x_val: 0.0129 time: 286.1368s\n",
      "disease8 Epoch: 0052 loss_train: 0.0120 loss_kl_train: 94908.5200 loss_x_train: 0.0025 loss_x_val: 0.0157 time: 286.0422s\n",
      "disease8 Epoch: 0053 loss_train: 0.7994 loss_kl_train: 7970206.3527 loss_x_train: 0.0024 loss_x_val: 0.0179 time: 286.1778s\n",
      "disease8 Epoch: 0054 loss_train: 905219925906.1068 loss_kl_train: 9052198877509129216.0000 loss_x_train: 0.0027 loss_x_val: 0.0275 time: 286.1369s\n",
      "disease8 Epoch: 0055 loss_train: 0.2359 loss_kl_train: 2332536.8664 loss_x_train: 0.0026 loss_x_val: 0.0201 time: 286.0626s\n",
      "disease8 Epoch: 0056 loss_train: 23118578380.8726 loss_kl_train: 231185771819300352.0000 loss_x_train: 0.0023 loss_x_val: 0.0151 time: 286.0857s\n",
      "disease8 Epoch: 0057 loss_train: 10694.3054 loss_kl_train: 106943029570.7558 loss_x_train: 0.0022 loss_x_val: 0.0186 time: 286.0655s\n",
      "disease8 Epoch: 0058 loss_train: 2.9505 loss_kl_train: 29483285.9232 loss_x_train: 0.0022 loss_x_val: 0.0186 time: 286.1049s\n",
      "disease8 Epoch: 0059 loss_train: 0.0278 loss_kl_train: 254555.6489 loss_x_train: 0.0023 loss_x_val: 0.0156 time: 286.0621s\n",
      "control8 Epoch: 0060 loss_train: 195.6617 loss_kl_train: 1956547164.8312 loss_x_train: 0.0070 loss_x_val: 0.0104 time: 286.0998s\n",
      "control8 Epoch: 0061 loss_train: 264247.0717 loss_kl_train: 2642470690805.8652 loss_x_train: 0.0056 loss_x_val: 0.0198 time: 286.0531s\n",
      "control8 Epoch: 0062 loss_train: 71133386750.7520 loss_kl_train: 711333857291989632.0000 loss_x_train: 0.0044 loss_x_val: 0.0145 time: 286.1238s\n",
      "control8 Epoch: 0063 loss_train: 163.3689 loss_kl_train: 1633647886.9098 loss_x_train: 0.0041 loss_x_val: 0.0198 time: 286.0623s\n",
      "control8 Epoch: 0064 loss_train: 510014456.0050 loss_kl_train: 5100144445322401.0000 loss_x_train: 0.0038 loss_x_val: 0.0151 time: 286.1261s\n",
      "control8 Epoch: 0065 loss_train: 432172042.1159 loss_kl_train: 4321720478936637.0000 loss_x_train: 0.0032 loss_x_val: 0.0193 time: 286.1126s\n",
      "control8 Epoch: 0066 loss_train: 477964.1187 loss_kl_train: 4779641155243.6846 loss_x_train: 0.0034 loss_x_val: 0.0198 time: 286.0787s\n",
      "control8 Epoch: 0067 loss_train: 29893.0279 loss_kl_train: 298930244001.5490 loss_x_train: 0.0034 loss_x_val: 0.0220 time: 286.0765s\n",
      "control8 Epoch: 0068 loss_train: 18.2125 loss_kl_train: 182096981.3760 loss_x_train: 0.0028 loss_x_val: 0.0099 time: 286.0662s\n",
      "control8 Epoch: 0069 loss_train: 58.5854 loss_kl_train: 585828048.5672 loss_x_train: 0.0026 loss_x_val: 0.0291 time: 286.1048s\n",
      "control8 Epoch: 0070 loss_train: 0.0363 loss_kl_train: 334845.4682 loss_x_train: 0.0028 loss_x_val: 0.0148 time: 286.0800s\n",
      "control8 Epoch: 0071 loss_train: 87975826588.9278 loss_kl_train: 879758293698746112.0000 loss_x_train: 0.0027 loss_x_val: 0.0132 time: 286.0486s\n",
      "control8 Epoch: 0072 loss_train: 0.0079 loss_kl_train: 55690.1196 loss_x_train: 0.0023 loss_x_val: 0.0129 time: 286.0201s\n",
      "control8 Epoch: 0073 loss_train: 4.1285 loss_kl_train: 41260450.0436 loss_x_train: 0.0025 loss_x_val: 0.0171 time: 286.0908s\n",
      "control8 Epoch: 0074 loss_train: 136722553020.2689 loss_kl_train: 1367225528035976192.0000 loss_x_train: 0.0022 loss_x_val: 0.0169 time: 286.0448s\n",
      "control8 Epoch: 0075 loss_train: 2.4853 loss_kl_train: 24828540.2635 loss_x_train: 0.0024 loss_x_val: 0.0102 time: 286.1592s\n",
      "control8 Epoch: 0076 loss_train: 0.7565 loss_kl_train: 7539400.3233 loss_x_train: 0.0026 loss_x_val: 0.0145 time: 286.1660s\n",
      "control8 Epoch: 0077 loss_train: 0.0347 loss_kl_train: 323310.4086 loss_x_train: 0.0023 loss_x_val: 0.0128 time: 286.1436s\n",
      "control8 Epoch: 0078 loss_train: 18.3198 loss_kl_train: 183173971.5899 loss_x_train: 0.0024 loss_x_val: 0.0128 time: 286.0358s\n",
      "control8 Epoch: 0079 loss_train: 11.4966 loss_kl_train: 114937781.9821 loss_x_train: 0.0028 loss_x_val: 0.0142 time: 286.1396s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0080 loss_train: 10.6234 loss_kl_train: 106173960.0545 loss_x_train: 0.0060 loss_x_val: 0.0136 time: 277.0233s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0081 loss_train: 477812735.1553 loss_kl_train: 4778127186901525.0000 loss_x_train: 0.0045 loss_x_val: 0.0223 time: 276.9992s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0082 loss_train: 60985799.3596 loss_kl_train: 609858002748229.3750 loss_x_train: 0.0037 loss_x_val: 0.0120 time: 277.0470s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0083 loss_train: 288.2660 loss_kl_train: 2882623350.5509 loss_x_train: 0.0037 loss_x_val: 0.0225 time: 277.0523s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0084 loss_train: 3777611421249.3887 loss_kl_train: 37776112758343180288.0000 loss_x_train: 0.0042 loss_x_val: 0.0186 time: 276.9609s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0085 loss_train: 1304.8364 loss_kl_train: 13048326959.5970 loss_x_train: 0.0037 loss_x_val: 0.0225 time: 277.0448s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0086 loss_train: 37179.8100 loss_kl_train: 371798067098.7097 loss_x_train: 0.0030 loss_x_val: 0.0731 time: 276.9110s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0087 loss_train: 311296.1472 loss_kl_train: 3112961352722.2520 loss_x_train: 0.0030 loss_x_val: 0.0211 time: 276.9644s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0088 loss_train: 908208.3191 loss_kl_train: 9082083007742.5527 loss_x_train: 0.0030 loss_x_val: 0.0172 time: 276.9454s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0089 loss_train: 0.1420 loss_kl_train: 1395602.8725 loss_x_train: 0.0025 loss_x_val: 0.0396 time: 276.9653s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0090 loss_train: 115995810.0586 loss_kl_train: 1159958099443710.5000 loss_x_train: 0.0025 loss_x_val: 0.0189 time: 276.9723s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0091 loss_train: 0.0585 loss_kl_train: 557960.3953 loss_x_train: 0.0027 loss_x_val: 0.0342 time: 276.9986s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0092 loss_train: 300.3900 loss_kl_train: 3003870122.8316 loss_x_train: 0.0030 loss_x_val: 0.0564 time: 276.9373s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0093 loss_train: 65449.0842 loss_kl_train: 654490814263.2649 loss_x_train: 0.0024 loss_x_val: 0.0338 time: 276.9676s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0094 loss_train: 2207.5105 loss_kl_train: 22075077227.6019 loss_x_train: 0.0026 loss_x_val: 0.0195 time: 276.9841s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0095 loss_train: 19.4866 loss_kl_train: 194844343.7277 loss_x_train: 0.0022 loss_x_val: 0.0252 time: 276.9987s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0096 loss_train: 46852.9697 loss_kl_train: 468529663839.1857 loss_x_train: 0.0022 loss_x_val: 0.0311 time: 277.0256s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0097 loss_train: 4.7853 loss_kl_train: 47831117.3851 loss_x_train: 0.0022 loss_x_val: 0.0279 time: 277.0101s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0098 loss_train: 3.1558 loss_kl_train: 31535659.9101 loss_x_train: 0.0022 loss_x_val: 0.0218 time: 277.0253s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0099 loss_train: 4.0726 loss_kl_train: 40704212.8550 loss_x_train: 0.0022 loss_x_val: 0.0226 time: 276.9874s\n",
      "disease13 Epoch: 0100 loss_train: 351.6854 loss_kl_train: 3516797713.9099 loss_x_train: 0.0056 loss_x_val: 0.0372 time: 330.7437s\n",
      "disease13 Epoch: 0101 loss_train: 3294931664.2865 loss_kl_train: 32949317374482424.0000 loss_x_train: 0.0032 loss_x_val: 0.0252 time: 330.6865s\n",
      "disease13 Epoch: 0102 loss_train: 20796239.7035 loss_kl_train: 207962399943870.9688 loss_x_train: 0.0030 loss_x_val: 0.0270 time: 330.7552s\n",
      "disease13 Epoch: 0103 loss_train: 1487.2308 loss_kl_train: 14872279158.9033 loss_x_train: 0.0028 loss_x_val: 0.0477 time: 330.8455s\n",
      "disease13 Epoch: 0104 loss_train: 39.5928 loss_kl_train: 395897554.4092 loss_x_train: 0.0031 loss_x_val: 0.0283 time: 330.7560s\n",
      "disease13 Epoch: 0105 loss_train: 14477100426.0478 loss_kl_train: 144771006343536000.0000 loss_x_train: 0.0034 loss_x_val: 0.0280 time: 330.7465s\n",
      "disease13 Epoch: 0106 loss_train: 0.4753 loss_kl_train: 4723226.9341 loss_x_train: 0.0030 loss_x_val: 0.0185 time: 330.7296s\n",
      "disease13 Epoch: 0107 loss_train: 2370.2835 loss_kl_train: 23702811409.9494 loss_x_train: 0.0024 loss_x_val: 0.0263 time: 330.6988s\n",
      "disease13 Epoch: 0108 loss_train: 5155.3410 loss_kl_train: 51553387745.1015 loss_x_train: 0.0022 loss_x_val: 0.0233 time: 330.8033s\n",
      "disease13 Epoch: 0109 loss_train: 8310550044.5918 loss_kl_train: 83105495414174400.0000 loss_x_train: 0.0022 loss_x_val: 0.0270 time: 330.6837s\n",
      "disease13 Epoch: 0110 loss_train: 375.4995 loss_kl_train: 3754976452.8879 loss_x_train: 0.0019 loss_x_val: 0.0198 time: 330.7817s\n",
      "disease13 Epoch: 0111 loss_train: 76.8797 loss_kl_train: 768770617.8357 loss_x_train: 0.0026 loss_x_val: 0.0240 time: 330.7324s\n",
      "disease13 Epoch: 0112 loss_train: 36533158005109.1875 loss_kl_train: 365331568063767969792.0000 loss_x_train: 0.0024 loss_x_val: 0.0198 time: 330.7637s\n",
      "disease13 Epoch: 0113 loss_train: 81288.8236 loss_kl_train: 812888221069.2692 loss_x_train: 0.0023 loss_x_val: 0.0235 time: 330.7870s\n",
      "disease13 Epoch: 0114 loss_train: 359240.7476 loss_kl_train: 3592407515079.1602 loss_x_train: 0.0020 loss_x_val: 0.0268 time: 330.7540s\n",
      "disease13 Epoch: 0115 loss_train: 2.4889 loss_kl_train: 24867798.1405 loss_x_train: 0.0021 loss_x_val: 0.0186 time: 330.7806s\n",
      "disease13 Epoch: 0116 loss_train: 105341130272727.5781 loss_kl_train: 1053411268383373131776.0000 loss_x_train: 0.0018 loss_x_val: 0.0271 time: 330.7653s\n",
      "disease13 Epoch: 0117 loss_train: 0.0391 loss_kl_train: 373281.5543 loss_x_train: 0.0017 loss_x_val: 0.0265 time: 330.7878s\n",
      "disease13 Epoch: 0118 loss_train: 3947.8803 loss_kl_train: 39478784842.4905 loss_x_train: 0.0017 loss_x_val: 0.0276 time: 330.7453s\n",
      "disease13 Epoch: 0119 loss_train: 4977.8193 loss_kl_train: 49778173069.4840 loss_x_train: 0.0019 loss_x_val: 0.0230 time: 330.7770s\n",
      "disease8 Epoch: 0120 loss_train: 4.5007 loss_kl_train: 44958967.3680 loss_x_train: 0.0048 loss_x_val: 0.0182 time: 286.0659s\n",
      "disease8 Epoch: 0121 loss_train: 3255803874230.0518 loss_kl_train: 32558037210367234048.0000 loss_x_train: 0.0028 loss_x_val: 0.0161 time: 286.0724s\n",
      "disease8 Epoch: 0122 loss_train: 37.3260 loss_kl_train: 373236424.1584 loss_x_train: 0.0024 loss_x_val: 0.0168 time: 286.1258s\n",
      "disease8 Epoch: 0123 loss_train: 14609.9789 loss_kl_train: 146099759982.3041 loss_x_train: 0.0025 loss_x_val: 0.0163 time: 286.1296s\n",
      "disease8 Epoch: 0124 loss_train: 102.1828 loss_kl_train: 1021806715.7317 loss_x_train: 0.0022 loss_x_val: 0.0275 time: 286.1245s\n",
      "disease8 Epoch: 0125 loss_train: 0.0588 loss_kl_train: 566254.9507 loss_x_train: 0.0022 loss_x_val: 0.0163 time: 286.0945s\n",
      "disease8 Epoch: 0126 loss_train: 0.0173 loss_kl_train: 153511.2156 loss_x_train: 0.0020 loss_x_val: 0.0161 time: 286.1607s\n",
      "disease8 Epoch: 0127 loss_train: 0.0189 loss_kl_train: 169065.0381 loss_x_train: 0.0020 loss_x_val: 0.0159 time: 286.1424s\n",
      "disease8 Epoch: 0128 loss_train: 14992191276121048743936.0000 loss_kl_train: 149921914406674726599423688704.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.1374s\n",
      "disease8 Epoch: 0129 loss_train: 14984301900735648366592.0000 loss_kl_train: 149843014572069735217657544704.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.1121s\n",
      "disease8 Epoch: 0130 loss_train: 14980758328874570350592.0000 loss_kl_train: 149807578384625601477640978432.0000 loss_x_train: 0.0019 loss_x_val: 0.0275 time: 286.0798s\n",
      "disease8 Epoch: 0131 loss_train: 14978474260724786921472.0000 loss_kl_train: 149784736175898739844327669760.0000 loss_x_train: 0.0020 loss_x_val: 0.0275 time: 286.1248s\n",
      "disease8 Epoch: 0132 loss_train: 14976302505007989653504.0000 loss_kl_train: 149763018772905934495401639936.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.0887s\n",
      "disease8 Epoch: 0133 loss_train: 14974930688863708905472.0000 loss_kl_train: 149749309602178814673703403520.0000 loss_x_train: 0.0019 loss_x_val: 0.0275 time: 286.1531s\n",
      "disease8 Epoch: 0134 loss_train: 14973446560286704140288.0000 loss_kl_train: 149734466011996033641760161792.0000 loss_x_train: 0.0017 loss_x_val: 0.0275 time: 286.0624s\n",
      "disease8 Epoch: 0135 loss_train: 14972189348665877331968.0000 loss_kl_train: 149721891260727196266027548672.0000 loss_x_train: 0.0017 loss_x_val: 0.0275 time: 286.1379s\n",
      "disease8 Epoch: 0136 loss_train: 14970930990999813488640.0000 loss_kl_train: 149709306895734150320902635520.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.1388s\n",
      "disease8 Epoch: 0137 loss_train: 14969674925424219521024.0000 loss_kl_train: 149696751371913747676140666880.0000 loss_x_train: 0.0017 loss_x_val: 0.0275 time: 286.1201s\n",
      "disease8 Epoch: 0138 loss_train: 14968761527373351878656.0000 loss_kl_train: 149687618333912668858386219008.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.1443s\n",
      "disease8 Epoch: 0139 loss_train: 14967618920275844792320.0000 loss_kl_train: 149676187615825020676244242432.0000 loss_x_train: 0.0019 loss_x_val: 0.0275 time: 286.0965s\n",
      "control8 Epoch: 0140 loss_train: 421.9735 loss_kl_train: 4219685214.1883 loss_x_train: 0.0050 loss_x_val: 0.0150 time: 286.1263s\n",
      "control8 Epoch: 0141 loss_train: 87683267470381776.0000 loss_kl_train: 876832643397980811952128.0000 loss_x_train: 0.0031 loss_x_val: 0.0197 time: 286.0998s\n",
      "control8 Epoch: 0142 loss_train: 225.0448 loss_kl_train: 2250425107.1563 loss_x_train: 0.0023 loss_x_val: 0.0204 time: 286.1424s\n",
      "control8 Epoch: 0143 loss_train: 470190.0803 loss_kl_train: 4701900714513.9883 loss_x_train: 0.0026 loss_x_val: 0.0124 time: 286.1155s\n",
      "control8 Epoch: 0144 loss_train: 6.7763 loss_kl_train: 67739165.7640 loss_x_train: 0.0024 loss_x_val: 0.0187 time: 286.0915s\n",
      "control8 Epoch: 0145 loss_train: 0.0608 loss_kl_train: 586680.6600 loss_x_train: 0.0021 loss_x_val: 0.0180 time: 286.1038s\n",
      "control8 Epoch: 0146 loss_train: 26.1945 loss_kl_train: 261919777.7481 loss_x_train: 0.0026 loss_x_val: 0.0188 time: 286.0414s\n",
      "control8 Epoch: 0147 loss_train: 88.7711 loss_kl_train: 887689694.4441 loss_x_train: 0.0021 loss_x_val: 0.0167 time: 286.1189s\n",
      "control8 Epoch: 0148 loss_train: 0.0764 loss_kl_train: 744087.0324 loss_x_train: 0.0020 loss_x_val: 0.0244 time: 286.1283s\n",
      "control8 Epoch: 0149 loss_train: 27363.1221 loss_kl_train: 273631193257.3455 loss_x_train: 0.0020 loss_x_val: 0.0171 time: 286.1288s\n",
      "control8 Epoch: 0150 loss_train: 14.8962 loss_kl_train: 148941654.0678 loss_x_train: 0.0020 loss_x_val: 0.0184 time: 286.0357s\n",
      "control8 Epoch: 0151 loss_train: 508495.4616 loss_kl_train: 5084954595052.2490 loss_x_train: 0.0019 loss_x_val: 0.0178 time: 286.1346s\n",
      "control8 Epoch: 0152 loss_train: 4075.1949 loss_kl_train: 40751928710.4825 loss_x_train: 0.0022 loss_x_val: 0.0173 time: 286.1074s\n",
      "control8 Epoch: 0153 loss_train: 0.1276 loss_kl_train: 1257563.9722 loss_x_train: 0.0018 loss_x_val: 0.0130 time: 286.1265s\n",
      "control8 Epoch: 0154 loss_train: 0.0625 loss_kl_train: 607652.5931 loss_x_train: 0.0018 loss_x_val: 0.0195 time: 286.1128s\n",
      "control8 Epoch: 0155 loss_train: 20.2824 loss_kl_train: 202805769.0518 loss_x_train: 0.0018 loss_x_val: 0.0212 time: 286.1147s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control8 Epoch: 0156 loss_train: 4.3806 loss_kl_train: 43787346.8482 loss_x_train: 0.0019 loss_x_val: 0.0202 time: 286.0713s\n",
      "control8 Epoch: 0157 loss_train: 34039.1944 loss_kl_train: 340391925978.9864 loss_x_train: 0.0016 loss_x_val: 0.0220 time: 286.1086s\n",
      "control8 Epoch: 0158 loss_train: 0.1813 loss_kl_train: 1795357.4540 loss_x_train: 0.0018 loss_x_val: 0.0172 time: 286.1189s\n",
      "control8 Epoch: 0159 loss_train: 1.0461 loss_kl_train: 10445282.5666 loss_x_train: 0.0016 loss_x_val: 0.0197 time: 286.1233s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0160 loss_train: 5689999201999909.0000 loss_kl_train: 56899990605760722632704.0000 loss_x_train: 0.0047 loss_x_val: 0.0559 time: 276.9860s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0161 loss_train: 6162909.6782 loss_kl_train: 61629094348206.5938 loss_x_train: 0.0031 loss_x_val: 0.0222 time: 276.9479s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0162 loss_train: 67258141.9177 loss_kl_train: 672581398734740.7500 loss_x_train: 0.0025 loss_x_val: 0.0202 time: 276.9074s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0163 loss_train: 208731648660.5975 loss_kl_train: 2087316414579266048.0000 loss_x_train: 0.0024 loss_x_val: 0.0226 time: 276.9663s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0164 loss_train: 0.4860 loss_kl_train: 4836825.1727 loss_x_train: 0.0023 loss_x_val: 0.0312 time: 276.9980s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0165 loss_train: 7.4065 loss_kl_train: 74043755.5801 loss_x_train: 0.0021 loss_x_val: 0.0166 time: 276.9739s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0166 loss_train: 10441.1831 loss_kl_train: 104411812825.5220 loss_x_train: 0.0022 loss_x_val: 0.0183 time: 276.9613s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0167 loss_train: 480723356.2875 loss_kl_train: 4807233459237331.0000 loss_x_train: 0.0022 loss_x_val: 0.0224 time: 277.0393s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0168 loss_train: 50614815214.2332 loss_kl_train: 506148144367598208.0000 loss_x_train: 0.0020 loss_x_val: 0.0185 time: 277.0518s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0169 loss_train: 2.5791 loss_kl_train: 25770425.7158 loss_x_train: 0.0021 loss_x_val: 0.0170 time: 276.9765s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0170 loss_train: 4093613251.6051 loss_kl_train: 40936131898064424.0000 loss_x_train: 0.0020 loss_x_val: 0.0226 time: 276.9784s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0171 loss_train: 1322.6562 loss_kl_train: 13226540643.8198 loss_x_train: 0.0021 loss_x_val: 0.0237 time: 276.9460s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0172 loss_train: 57.2367 loss_kl_train: 572351078.9824 loss_x_train: 0.0016 loss_x_val: 0.0230 time: 276.9705s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0173 loss_train: 0.0801 loss_kl_train: 784517.2798 loss_x_train: 0.0017 loss_x_val: 0.0185 time: 276.9825s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0174 loss_train: 4434745.8325 loss_kl_train: 44347456431430.0781 loss_x_train: 0.0022 loss_x_val: 0.0336 time: 276.9575s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0175 loss_train: 1910.2739 loss_kl_train: 19102714382.0406 loss_x_train: 0.0023 loss_x_val: 0.0174 time: 276.9095s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0176 loss_train: 4841783.0598 loss_kl_train: 48417829174444.5547 loss_x_train: 0.0018 loss_x_val: 0.0298 time: 277.0134s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0177 loss_train: 0.1074 loss_kl_train: 1056915.2650 loss_x_train: 0.0017 loss_x_val: 0.0278 time: 276.9861s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0178 loss_train: 205.1761 loss_kl_train: 2051741473.6417 loss_x_train: 0.0020 loss_x_val: 0.0194 time: 276.9847s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0179 loss_train: 4112.1376 loss_kl_train: 41121358956.2026 loss_x_train: 0.0018 loss_x_val: 0.0251 time: 277.0142s\n",
      "disease13 Epoch: 0180 loss_train: 663550.3474 loss_kl_train: 6635503151691.3047 loss_x_train: 0.0038 loss_x_val: 0.0238 time: 330.7572s\n",
      "disease13 Epoch: 0181 loss_train: 91198599.5460 loss_kl_train: 911986006734257.8750 loss_x_train: 0.0022 loss_x_val: 0.0199 time: 330.6617s\n",
      "disease13 Epoch: 0182 loss_train: 0.2580 loss_kl_train: 2560397.5461 loss_x_train: 0.0020 loss_x_val: 0.0321 time: 330.7417s\n",
      "disease13 Epoch: 0183 loss_train: 53.1585 loss_kl_train: 531562376.9943 loss_x_train: 0.0023 loss_x_val: 0.0239 time: 330.6641s\n",
      "disease13 Epoch: 0184 loss_train: 15.8912 loss_kl_train: 158893700.7461 loss_x_train: 0.0019 loss_x_val: 0.0235 time: 330.7350s\n",
      "disease13 Epoch: 0185 loss_train: 4924506.7625 loss_kl_train: 49245065379506.3203 loss_x_train: 0.0018 loss_x_val: 0.0222 time: 330.6937s\n",
      "disease13 Epoch: 0186 loss_train: 0.1245 loss_kl_train: 1226351.2248 loss_x_train: 0.0018 loss_x_val: 0.0289 time: 330.7299s\n",
      "disease13 Epoch: 0187 loss_train: 0.4534 loss_kl_train: 4513763.2818 loss_x_train: 0.0020 loss_x_val: 0.0224 time: 330.7250s\n",
      "disease13 Epoch: 0188 loss_train: 1.3290 loss_kl_train: 13273577.3348 loss_x_train: 0.0017 loss_x_val: 0.0216 time: 330.7665s\n",
      "disease13 Epoch: 0189 loss_train: 0.0497 loss_kl_train: 480746.5513 loss_x_train: 0.0016 loss_x_val: 0.0218 time: 330.7258s\n",
      "disease13 Epoch: 0190 loss_train: 0.0670 loss_kl_train: 653357.1318 loss_x_train: 0.0017 loss_x_val: 0.0291 time: 330.7110s\n",
      "disease13 Epoch: 0191 loss_train: 117597.2640 loss_kl_train: 1175972542175.2905 loss_x_train: 0.0020 loss_x_val: 0.0342 time: 330.7750s\n",
      "disease13 Epoch: 0192 loss_train: 11065764863246196.0000 loss_kl_train: 110657647897674302619648.0000 loss_x_train: 0.0019 loss_x_val: 0.0265 time: 330.7010s\n",
      "disease13 Epoch: 0193 loss_train: 0.0883 loss_kl_train: 868843.6964 loss_x_train: 0.0014 loss_x_val: 0.0335 time: 330.8063s\n",
      "disease13 Epoch: 0194 loss_train: 0.6049 loss_kl_train: 6031841.0169 loss_x_train: 0.0018 loss_x_val: 0.0323 time: 330.7160s\n",
      "disease13 Epoch: 0195 loss_train: 7.8469 loss_kl_train: 78454970.5471 loss_x_train: 0.0014 loss_x_val: 0.0237 time: 330.7380s\n",
      "disease13 Epoch: 0196 loss_train: 0.2331 loss_kl_train: 2315314.3187 loss_x_train: 0.0016 loss_x_val: 0.0232 time: 330.7705s\n",
      "disease13 Epoch: 0197 loss_train: 511.3310 loss_kl_train: 5113293761.7219 loss_x_train: 0.0016 loss_x_val: 0.0402 time: 330.7414s\n",
      "disease13 Epoch: 0198 loss_train: 110508304.9829 loss_kl_train: 1105083046378374.2500 loss_x_train: 0.0016 loss_x_val: 0.0230 time: 330.7484s\n",
      "disease13 Epoch: 0199 loss_train: 1.8158 loss_kl_train: 18138997.6307 loss_x_train: 0.0019 loss_x_val: 0.0275 time: 330.7081s\n",
      "disease8 Epoch: 0200 loss_train: 14966593209792123109376.0000 loss_kl_train: 149665929772091204988055846912.0000 loss_x_train: 0.0040 loss_x_val: 0.0275 time: 286.1493s\n",
      "disease8 Epoch: 0201 loss_train: 14965105643079419822080.0000 loss_kl_train: 149651057340735780655749660672.0000 loss_x_train: 0.0023 loss_x_val: 0.0275 time: 286.0677s\n",
      "disease8 Epoch: 0202 loss_train: 14964194538602792747008.0000 loss_kl_train: 149641943545020861267849510912.0000 loss_x_train: 0.0017 loss_x_val: 0.0275 time: 286.1055s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease8 Epoch: 0203 loss_train: 14963051930021512871936.0000 loss_kl_train: 149630522425819696956216180736.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.0548s\n",
      "disease8 Epoch: 0204 loss_train: 14962366594972053929984.0000 loss_kl_train: 149623667840456805548436750336.0000 loss_x_train: 0.0019 loss_x_val: 0.0275 time: 286.1424s\n",
      "disease8 Epoch: 0205 loss_train: 14961681259922594988032.0000 loss_kl_train: 149616813255093896548471275520.0000 loss_x_train: 0.0017 loss_x_val: 0.0275 time: 286.0958s\n",
      "disease8 Epoch: 0206 loss_train: 14960997070918370983936.0000 loss_kl_train: 149609968283455196117898100736.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.0865s\n",
      "disease8 Epoch: 0207 loss_train: 14960311735868912041984.0000 loss_kl_train: 149603113698092287117932625920.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1358s\n",
      "disease8 Epoch: 0208 loss_train: 14959514088386601156608.0000 loss_kl_train: 149595134306996623655347159040.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.1523s\n",
      "disease8 Epoch: 0209 loss_train: 14959060257090016641024.0000 loss_kl_train: 149590596655326417034879696896.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.0871s\n",
      "disease8 Epoch: 0210 loss_train: 14958488377902879473664.0000 loss_kl_train: 149584876463262807967158763520.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.1042s\n",
      "disease8 Epoch: 0211 loss_train: 14957688438330100809728.0000 loss_kl_train: 149576887458442918342994952192.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.1235s\n",
      "disease8 Epoch: 0212 loss_train: 14957232312327283408896.0000 loss_kl_train: 149572320939442387730210750464.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1062s\n",
      "disease8 Epoch: 0213 loss_train: 14956546977277826564096.0000 loss_kl_train: 149565466354079478730245275648.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.0196s\n",
      "disease8 Epoch: 0214 loss_train: 14955976246751689441280.0000 loss_kl_train: 149559765415621985085448781824.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.0872s\n",
      "disease8 Epoch: 0215 loss_train: 14955518974703639199744.0000 loss_kl_train: 149555189282897228311086235648.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.1027s\n",
      "disease8 Epoch: 0216 loss_train: 14954950536267967758336.0000 loss_kl_train: 149549507571888151805074341888.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1389s\n",
      "disease8 Epoch: 0217 loss_train: 14954493264220303392768.0000 loss_kl_train: 149544931439167282903827611648.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1187s\n",
      "disease8 Epoch: 0218 loss_train: 14953809075215693512704.0000 loss_kl_train: 149538086467524712192324665344.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1133s\n",
      "disease8 Epoch: 0219 loss_train: 14953466635378946473984.0000 loss_kl_train: 149534666258585097464507793408.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.0246s\n",
      "control8 Epoch: 0220 loss_train: 2859116.2287 loss_kl_train: 28591162305572.6875 loss_x_train: 0.0039 loss_x_val: 0.0177 time: 286.1070s\n",
      "control8 Epoch: 0221 loss_train: 10925.6244 loss_kl_train: 109256221654.6193 loss_x_train: 0.0020 loss_x_val: 0.0151 time: 286.1052s\n",
      "control8 Epoch: 0222 loss_train: 149.0543 loss_kl_train: 1490524107.7130 loss_x_train: 0.0019 loss_x_val: 0.0146 time: 286.1334s\n",
      "control8 Epoch: 0223 loss_train: 2.5322 loss_kl_train: 25302740.4050 loss_x_train: 0.0019 loss_x_val: 0.0146 time: 286.1252s\n",
      "control8 Epoch: 0224 loss_train: 64.5832 loss_kl_train: 645812650.3118 loss_x_train: 0.0020 loss_x_val: 0.0174 time: 286.0408s\n",
      "control8 Epoch: 0225 loss_train: 0.1832 loss_kl_train: 1814187.3640 loss_x_train: 0.0018 loss_x_val: 0.0157 time: 286.1075s\n",
      "control8 Epoch: 0226 loss_train: 4.8292 loss_kl_train: 48272912.6450 loss_x_train: 0.0019 loss_x_val: 0.0156 time: 286.0871s\n",
      "control8 Epoch: 0227 loss_train: 0.0262 loss_kl_train: 245625.4045 loss_x_train: 0.0016 loss_x_val: 0.0173 time: 286.1038s\n",
      "control8 Epoch: 0228 loss_train: 1.5492 loss_kl_train: 15473976.4096 loss_x_train: 0.0018 loss_x_val: 0.0150 time: 286.0753s\n",
      "control8 Epoch: 0229 loss_train: 3.1258 loss_kl_train: 31239843.1557 loss_x_train: 0.0018 loss_x_val: 0.0168 time: 286.1145s\n",
      "control8 Epoch: 0230 loss_train: 5.4187 loss_kl_train: 54170904.6367 loss_x_train: 0.0016 loss_x_val: 0.0185 time: 286.1172s\n",
      "control8 Epoch: 0231 loss_train: 9785.8875 loss_kl_train: 97858862477.6224 loss_x_train: 0.0015 loss_x_val: 0.0171 time: 286.1106s\n",
      "control8 Epoch: 0232 loss_train: 595564395.2349 loss_kl_train: 5955644102286208.0000 loss_x_train: 0.0016 loss_x_val: 0.0199 time: 286.1410s\n",
      "control8 Epoch: 0233 loss_train: 1619.6290 loss_kl_train: 16196273037.4421 loss_x_train: 0.0016 loss_x_val: 0.0158 time: 286.1537s\n",
      "control8 Epoch: 0234 loss_train: 0.1125 loss_kl_train: 1111686.1429 loss_x_train: 0.0013 loss_x_val: 0.0169 time: 286.1281s\n",
      "control8 Epoch: 0235 loss_train: 5.4204 loss_kl_train: 54185025.6751 loss_x_train: 0.0019 loss_x_val: 0.0177 time: 286.1834s\n",
      "control8 Epoch: 0236 loss_train: 18.5682 loss_kl_train: 185667803.3707 loss_x_train: 0.0015 loss_x_val: 0.0170 time: 286.1157s\n",
      "control8 Epoch: 0237 loss_train: 465702167674023872.0000 loss_kl_train: 4657021734490174266540032.0000 loss_x_train: 0.0016 loss_x_val: 0.0198 time: 286.1373s\n",
      "control8 Epoch: 0238 loss_train: 1.5545 loss_kl_train: 15527728.1145 loss_x_train: 0.0017 loss_x_val: 0.0164 time: 286.1249s\n",
      "control8 Epoch: 0239 loss_train: 0.8213 loss_kl_train: 8198618.2469 loss_x_train: 0.0015 loss_x_val: 0.0168 time: 286.1317s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0240 loss_train: 19488.4554 loss_kl_train: 194884508730.2410 loss_x_train: 0.0042 loss_x_val: 0.0227 time: 277.0007s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0241 loss_train: 14.7381 loss_kl_train: 147360295.0051 loss_x_train: 0.0020 loss_x_val: 0.0193 time: 276.9347s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0242 loss_train: 600.1475 loss_kl_train: 6001456941.0104 loss_x_train: 0.0018 loss_x_val: 0.0203 time: 276.9527s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0243 loss_train: 527870.5968 loss_kl_train: 5278705728363.3955 loss_x_train: 0.0018 loss_x_val: 0.0197 time: 276.9718s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0244 loss_train: 172812477.5849 loss_kl_train: 1728124808492139.5000 loss_x_train: 0.0020 loss_x_val: 0.0225 time: 276.9561s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0245 loss_train: 25.7858 loss_kl_train: 257840609.8913 loss_x_train: 0.0018 loss_x_val: 0.0201 time: 276.9951s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0246 loss_train: 0.0313 loss_kl_train: 296235.7934 loss_x_train: 0.0016 loss_x_val: 0.0233 time: 276.9716s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0247 loss_train: 0.1304 loss_kl_train: 1286978.1321 loss_x_train: 0.0017 loss_x_val: 0.0205 time: 276.9892s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0248 loss_train: 73.6578 loss_kl_train: 736560600.2968 loss_x_train: 0.0017 loss_x_val: 0.0192 time: 277.0205s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0249 loss_train: 64905329705902.1094 loss_kl_train: 649053303695036514304.0000 loss_x_train: 0.0015 loss_x_val: 0.0226 time: 276.9881s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0250 loss_train: 0.3722 loss_kl_train: 3706104.5675 loss_x_train: 0.0016 loss_x_val: 0.0427 time: 277.0314s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0251 loss_train: 11244.3091 loss_kl_train: 112443074875.4285 loss_x_train: 0.0019 loss_x_val: 0.0200 time: 277.0165s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0252 loss_train: 8.1442 loss_kl_train: 81426463.3426 loss_x_train: 0.0015 loss_x_val: 0.0197 time: 276.9955s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0253 loss_train: 74.9258 loss_kl_train: 749238932.2577 loss_x_train: 0.0019 loss_x_val: 0.0202 time: 276.9679s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0254 loss_train: 39.2342 loss_kl_train: 392326827.1158 loss_x_train: 0.0016 loss_x_val: 0.0205 time: 276.9545s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0255 loss_train: 1796003583.5032 loss_kl_train: 17960035900453510.0000 loss_x_train: 0.0017 loss_x_val: 0.0225 time: 276.9809s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0256 loss_train: 1.2949 loss_kl_train: 12932160.8311 loss_x_train: 0.0017 loss_x_val: 0.0208 time: 277.0064s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0257 loss_train: 41956.6290 loss_kl_train: 419566251928.4236 loss_x_train: 0.0016 loss_x_val: 0.0195 time: 277.0014s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0258 loss_train: 889.9719 loss_kl_train: 8899704378.8337 loss_x_train: 0.0015 loss_x_val: 0.0197 time: 276.9944s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0259 loss_train: 4298.5581 loss_kl_train: 42985565829.6283 loss_x_train: 0.0015 loss_x_val: 0.0206 time: 276.9786s\n",
      "disease13 Epoch: 0260 loss_train: 904.0546 loss_kl_train: 9040510531.3134 loss_x_train: 0.0036 loss_x_val: 0.0260 time: 330.7942s\n",
      "disease13 Epoch: 0261 loss_train: 8.4913 loss_kl_train: 84891163.9537 loss_x_train: 0.0021 loss_x_val: 0.0250 time: 330.6832s\n",
      "disease13 Epoch: 0262 loss_train: 261.3647 loss_kl_train: 2613629828.5420 loss_x_train: 0.0017 loss_x_val: 0.0251 time: 330.7704s\n",
      "disease13 Epoch: 0263 loss_train: 1127614010.5188 loss_kl_train: 11276139659745038.0000 loss_x_train: 0.0018 loss_x_val: 0.0224 time: 330.7848s\n",
      "disease13 Epoch: 0264 loss_train: 0.6635 loss_kl_train: 6615637.7716 loss_x_train: 0.0019 loss_x_val: 0.0232 time: 330.7413s\n",
      "disease13 Epoch: 0265 loss_train: 10.7441 loss_kl_train: 107421791.8190 loss_x_train: 0.0019 loss_x_val: 0.0226 time: 330.7256s\n",
      "disease13 Epoch: 0266 loss_train: 8129.3776 loss_kl_train: 81293759711.3342 loss_x_train: 0.0016 loss_x_val: 0.0252 time: 330.7295s\n",
      "disease13 Epoch: 0267 loss_train: 9.5901 loss_kl_train: 95883130.5677 loss_x_train: 0.0018 loss_x_val: 0.0229 time: 330.6612s\n",
      "disease13 Epoch: 0268 loss_train: 135.1111 loss_kl_train: 1351092948.6659 loss_x_train: 0.0018 loss_x_val: 0.0232 time: 330.7569s\n",
      "disease13 Epoch: 0269 loss_train: 178.4313 loss_kl_train: 1784295543.4558 loss_x_train: 0.0018 loss_x_val: 0.0245 time: 330.6860s\n",
      "disease13 Epoch: 0270 loss_train: 3101229.5840 loss_kl_train: 31012295046157.6836 loss_x_train: 0.0018 loss_x_val: 0.0258 time: 330.6988s\n",
      "disease13 Epoch: 0271 loss_train: 736453.9788 loss_kl_train: 7364539421398.1074 loss_x_train: 0.0017 loss_x_val: 0.0270 time: 330.7354s\n",
      "disease13 Epoch: 0272 loss_train: 15836.8677 loss_kl_train: 158368666445.0163 loss_x_train: 0.0013 loss_x_val: 0.0270 time: 330.7190s\n",
      "disease13 Epoch: 0273 loss_train: 0.4104 loss_kl_train: 4090174.2881 loss_x_train: 0.0014 loss_x_val: 0.0313 time: 330.7212s\n",
      "disease13 Epoch: 0274 loss_train: 2.8913 loss_kl_train: 28896070.2662 loss_x_train: 0.0017 loss_x_val: 0.0255 time: 330.7365s\n",
      "disease13 Epoch: 0275 loss_train: 19336.3173 loss_kl_train: 193363153951.3398 loss_x_train: 0.0015 loss_x_val: 0.0246 time: 330.7399s\n",
      "disease13 Epoch: 0276 loss_train: 21437.8820 loss_kl_train: 214378810179.2580 loss_x_train: 0.0015 loss_x_val: 0.0267 time: 330.7443s\n",
      "disease13 Epoch: 0277 loss_train: 1167760.6251 loss_kl_train: 11677606162855.4570 loss_x_train: 0.0015 loss_x_val: 0.0254 time: 330.6876s\n",
      "disease13 Epoch: 0278 loss_train: 1158.2322 loss_kl_train: 11582305162.8300 loss_x_train: 0.0018 loss_x_val: 0.0241 time: 330.7136s\n",
      "disease13 Epoch: 0279 loss_train: 0.0796 loss_kl_train: 781882.2265 loss_x_train: 0.0015 loss_x_val: 0.0256 time: 330.7103s\n",
      "disease8 Epoch: 0280 loss_train: 14953011427733380530176.0000 loss_kl_train: 149530107076429048729739198464.0000 loss_x_train: 0.0038 loss_x_val: 0.0275 time: 286.0550s\n",
      "disease8 Epoch: 0281 loss_train: 14952324946638688747520.0000 loss_kl_train: 149523252491066139729773723648.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.1070s\n",
      "disease8 Epoch: 0282 loss_train: 14951643049724930424832.0000 loss_kl_train: 149516426746875874030171193344.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.1264s\n",
      "disease8 Epoch: 0283 loss_train: 14951185777676880183296.0000 loss_kl_train: 149511850614151134847994691584.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1400s\n",
      "disease8 Epoch: 0284 loss_train: 14950843110152246132736.0000 loss_kl_train: 149508428128332708222475436032.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.0988s\n",
      "disease8 Epoch: 0285 loss_train: 14950158921147925659648.0000 loss_kl_train: 149501592770417301567620251648.0000 loss_x_train: 0.0017 loss_x_val: 0.0275 time: 286.1346s\n",
      "disease8 Epoch: 0286 loss_train: 14949930858146519056384.0000 loss_kl_train: 149499304704054923180438978560.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1397s\n",
      "disease8 Epoch: 0287 loss_train: 14949358990666556768256.0000 loss_kl_train: 149493584629063090113655865344.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.1504s\n",
      "disease8 Epoch: 0288 loss_train: 14948905147662797373440.0000 loss_kl_train: 149489046860321107492250583040.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.1163s\n",
      "disease8 Epoch: 0289 loss_train: 14948446729569514291200.0000 loss_kl_train: 149484470727596350717888036864.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1377s\n",
      "disease8 Epoch: 0290 loss_train: 14948106354135251550208.0000 loss_kl_train: 149481067469225444029665116160.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.1233s\n",
      "disease8 Epoch: 0291 loss_train: 14947534477563881586688.0000 loss_kl_train: 149475347303319498061711933440.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.0281s\n",
      "disease8 Epoch: 0292 loss_train: 14947305268517240045568.0000 loss_kl_train: 149473049623232928697324404736.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1110s\n",
      "disease8 Epoch: 0293 loss_train: 14946852580650123264000.0000 loss_kl_train: 149468521559129232362109403136.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.1138s\n",
      "disease8 Epoch: 0294 loss_train: 14946622225558248882176.0000 loss_kl_train: 149466223879042645405535830016.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.0931s\n",
      "disease8 Epoch: 0295 loss_train: 14945938036554022780928.0000 loss_kl_train: 149459378907403962567148699648.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.1380s\n",
      "disease8 Epoch: 0296 loss_train: 14945481910551205380096.0000 loss_kl_train: 149454822002127640523756797952.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.1437s\n",
      "disease8 Epoch: 0297 loss_train: 14945252701504565936128.0000 loss_kl_train: 149452524322041053567183224832.0000 loss_x_train: 0.0011 loss_x_val: 0.0275 time: 286.1369s\n",
      "disease8 Epoch: 0298 loss_train: 14945140389071709798400.0000 loss_kl_train: 149451399516308299104563232768.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.1374s\n",
      "disease8 Epoch: 0299 loss_train: 14944569658545572675584.0000 loss_kl_train: 149445698577850787867580694528.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.0367s\n",
      "control8 Epoch: 0300 loss_train: 101633724.1587 loss_kl_train: 1016337214612983.7500 loss_x_train: 0.0035 loss_x_val: 0.0177 time: 286.1142s\n",
      "control8 Epoch: 0301 loss_train: 2.7897 loss_kl_train: 27878605.6741 loss_x_train: 0.0018 loss_x_val: 0.0177 time: 286.0847s\n",
      "control8 Epoch: 0302 loss_train: 3510197800.2878 loss_kl_train: 35101978671678616.0000 loss_x_train: 0.0020 loss_x_val: 0.0168 time: 286.0870s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control8 Epoch: 0303 loss_train: 0.6274 loss_kl_train: 6257865.4317 loss_x_train: 0.0016 loss_x_val: 0.0172 time: 286.1183s\n",
      "control8 Epoch: 0304 loss_train: 0.7651 loss_kl_train: 7637421.3494 loss_x_train: 0.0014 loss_x_val: 0.0170 time: 286.1284s\n",
      "control8 Epoch: 0305 loss_train: 3495680922872624185344.0000 loss_kl_train: 34956808700041821847122083840.0000 loss_x_train: 0.0018 loss_x_val: 0.0198 time: 286.1329s\n",
      "control8 Epoch: 0306 loss_train: 3495627631769279791104.0000 loss_kl_train: 34956275138348074722035499008.0000 loss_x_train: 0.0015 loss_x_val: 0.0199 time: 286.1083s\n",
      "control8 Epoch: 0307 loss_train: 3495627631769281888256.0000 loss_kl_train: 34956275138348092314221543424.0000 loss_x_train: 0.0019 loss_x_val: 0.0198 time: 286.0971s\n",
      "control8 Epoch: 0308 loss_train: 3495525454165733539840.0000 loss_kl_train: 34955255387785490843941470208.0000 loss_x_train: 0.0018 loss_x_val: 0.0198 time: 286.1122s\n",
      "control8 Epoch: 0309 loss_train: 3495600986217607856128.0000 loss_kl_train: 34956010760932255500863537152.0000 loss_x_train: 0.0018 loss_x_val: 0.0198 time: 286.1310s\n",
      "control8 Epoch: 0310 loss_train: 3495574340665935921152.0000 loss_kl_train: 34955743980085384137343500288.0000 loss_x_train: 0.0015 loss_x_val: 0.0198 time: 286.1056s\n",
      "control8 Epoch: 0311 loss_train: 3495547408602955251712.0000 loss_kl_train: 34955474795807456233428877312.0000 loss_x_train: 0.0018 loss_x_val: 0.0198 time: 286.1840s\n",
      "control8 Epoch: 0312 loss_train: 3495520476540542910464.0000 loss_kl_train: 34955205611535210605606600704.0000 loss_x_train: 0.0016 loss_x_val: 0.0198 time: 286.1282s\n",
      "control8 Epoch: 0313 loss_train: 3495547408602955251712.0000 loss_kl_train: 34955474795807456233428877312.0000 loss_x_train: 0.0018 loss_x_val: 0.0198 time: 286.1136s\n",
      "control8 Epoch: 0314 loss_train: 3495441112907575721984.0000 loss_kl_train: 34954410075851018523650293760.0000 loss_x_train: 0.0015 loss_x_val: 0.0198 time: 285.8417s\n",
      "control8 Epoch: 0315 loss_train: 3495494117499611381760.0000 loss_kl_train: 34954941234113709108342292480.0000 loss_x_train: 0.0017 loss_x_val: 0.0198 time: 286.0905s\n",
      "control8 Epoch: 0316 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0198 time: 286.0290s\n",
      "control8 Epoch: 0317 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0198 time: 286.0774s\n",
      "control8 Epoch: 0318 loss_train: inf loss_kl_train: inf loss_x_train: 0.0015 loss_x_val: 0.0198 time: 286.1436s\n",
      "control8 Epoch: 0319 loss_train: inf loss_kl_train: inf loss_x_train: 0.0014 loss_x_val: 0.0198 time: 286.1024s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0320 loss_train: 422401589.4745 loss_kl_train: 4224015902384132.0000 loss_x_train: 0.0039 loss_x_val: 0.0226 time: 277.0059s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0321 loss_train: 834.2146 loss_kl_train: 8342130263.2984 loss_x_train: 0.0015 loss_x_val: 0.0225 time: 276.9382s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0322 loss_train: 13.4114 loss_kl_train: 134098250.7406 loss_x_train: 0.0015 loss_x_val: 0.0225 time: 277.0225s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0323 loss_train: 797.2286 loss_kl_train: 7972267572.2073 loss_x_train: 0.0018 loss_x_val: 0.0225 time: 276.9911s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0324 loss_train: 50285.6933 loss_kl_train: 502856914037.9611 loss_x_train: 0.0018 loss_x_val: 0.0226 time: 277.0130s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0325 loss_train: 5405.8476 loss_kl_train: 54058459592.2681 loss_x_train: 0.0015 loss_x_val: 0.0226 time: 277.0589s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0326 loss_train: 3.8467 loss_kl_train: 38452786.5675 loss_x_train: 0.0015 loss_x_val: 0.0226 time: 277.0464s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0327 loss_train: 0.5662 loss_kl_train: 5647360.3072 loss_x_train: 0.0015 loss_x_val: 0.0226 time: 277.0660s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0328 loss_train: 22.2391 loss_kl_train: 222376740.5428 loss_x_train: 0.0014 loss_x_val: 0.0226 time: 277.0306s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0329 loss_train: 13192686934.7495 loss_kl_train: 131926864621061216.0000 loss_x_train: 0.0013 loss_x_val: 0.0226 time: 277.0058s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0330 loss_train: 0.0278 loss_kl_train: 264872.1589 loss_x_train: 0.0014 loss_x_val: 0.0226 time: 276.9815s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0331 loss_train: 0.0269 loss_kl_train: 255490.8465 loss_x_train: 0.0013 loss_x_val: 0.0226 time: 276.9277s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0332 loss_train: 1.7406 loss_kl_train: 17390150.8526 loss_x_train: 0.0016 loss_x_val: 0.0226 time: 276.9962s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0333 loss_train: 198725.1365 loss_kl_train: 1987251356004.4233 loss_x_train: 0.0016 loss_x_val: 0.0226 time: 277.0080s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0334 loss_train: 44207.0302 loss_kl_train: 442070276498.4352 loss_x_train: 0.0018 loss_x_val: 0.0225 time: 277.0140s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0335 loss_train: 480272.7084 loss_kl_train: 4802727104961.6582 loss_x_train: 0.0018 loss_x_val: 0.0226 time: 276.9172s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0336 loss_train: 210.9177 loss_kl_train: 2109161364.0422 loss_x_train: 0.0015 loss_x_val: 0.0226 time: 276.9916s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0337 loss_train: 3.5025 loss_kl_train: 35008264.1608 loss_x_train: 0.0017 loss_x_val: 0.0226 time: 276.9956s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0338 loss_train: 324.3463 loss_kl_train: 3243449100.5496 loss_x_train: 0.0014 loss_x_val: 0.0226 time: 276.9823s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0339 loss_train: 28.1780 loss_kl_train: 281764593.8837 loss_x_train: 0.0016 loss_x_val: 0.0226 time: 276.9446s\n",
      "disease13 Epoch: 0340 loss_train: 122205.5033 loss_kl_train: 1222054979336.3943 loss_x_train: 0.0029 loss_x_val: 0.0270 time: 330.7877s\n",
      "disease13 Epoch: 0341 loss_train: 27.4972 loss_kl_train: 274956019.3318 loss_x_train: 0.0016 loss_x_val: 0.0270 time: 330.7059s\n",
      "disease13 Epoch: 0342 loss_train: 2001771294963408.7500 loss_kl_train: 20017713131485554278400.0000 loss_x_train: 0.0018 loss_x_val: 0.0270 time: 330.7965s\n",
      "disease13 Epoch: 0343 loss_train: 9422.7669 loss_kl_train: 94227649382.2981 loss_x_train: 0.0015 loss_x_val: 0.0270 time: 330.7445s\n",
      "disease13 Epoch: 0344 loss_train: 1.0366 loss_kl_train: 10349574.3692 loss_x_train: 0.0016 loss_x_val: 0.0270 time: 330.7502s\n",
      "disease13 Epoch: 0345 loss_train: 3031519240850459.5000 loss_kl_train: 30315191650331291287552.0000 loss_x_train: 0.0014 loss_x_val: 0.0270 time: 330.8028s\n",
      "disease13 Epoch: 0346 loss_train: 5.3251 loss_kl_train: 53235195.6743 loss_x_train: 0.0016 loss_x_val: 0.0270 time: 330.7467s\n",
      "disease13 Epoch: 0347 loss_train: 188801224562.5469 loss_kl_train: 1888012163317559552.0000 loss_x_train: 0.0015 loss_x_val: 0.0270 time: 330.6838s\n",
      "disease13 Epoch: 0348 loss_train: 0.0655 loss_kl_train: 641386.1315 loss_x_train: 0.0014 loss_x_val: 0.0270 time: 330.7222s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease13 Epoch: 0349 loss_train: 16.1906 loss_kl_train: 161891756.1035 loss_x_train: 0.0014 loss_x_val: 0.0270 time: 330.7137s\n",
      "disease13 Epoch: 0350 loss_train: 36.1662 loss_kl_train: 361647085.1432 loss_x_train: 0.0015 loss_x_val: 0.0270 time: 330.7107s\n",
      "disease13 Epoch: 0351 loss_train: 66.8494 loss_kl_train: 668479170.3598 loss_x_train: 0.0015 loss_x_val: 0.0270 time: 330.7116s\n",
      "disease13 Epoch: 0352 loss_train: 232.8065 loss_kl_train: 2328050918.4778 loss_x_train: 0.0014 loss_x_val: 0.0270 time: 330.7026s\n",
      "disease13 Epoch: 0353 loss_train: 234350.4722 loss_kl_train: 2343504625166.2725 loss_x_train: 0.0014 loss_x_val: 0.0270 time: 330.7431s\n",
      "disease13 Epoch: 0354 loss_train: 870273.7224 loss_kl_train: 8702737176200.8906 loss_x_train: 0.0015 loss_x_val: 0.0270 time: 330.6738s\n",
      "disease13 Epoch: 0355 loss_train: 0.0650 loss_kl_train: 637074.3537 loss_x_train: 0.0013 loss_x_val: 0.0270 time: 330.7398s\n",
      "disease13 Epoch: 0356 loss_train: 7349.7452 loss_kl_train: 73497438371.7808 loss_x_train: 0.0016 loss_x_val: 0.0270 time: 330.6806s\n",
      "disease13 Epoch: 0357 loss_train: 23.0029 loss_kl_train: 230014237.2036 loss_x_train: 0.0015 loss_x_val: 0.0270 time: 330.7516s\n",
      "disease13 Epoch: 0358 loss_train: 626.6840 loss_kl_train: 6266825827.7476 loss_x_train: 0.0014 loss_x_val: 0.0270 time: 330.7545s\n",
      "disease13 Epoch: 0359 loss_train: 42.4667 loss_kl_train: 424652422.9922 loss_x_train: 0.0015 loss_x_val: 0.0270 time: 330.7039s\n",
      "disease8 Epoch: 0360 loss_train: 14944342741589398913024.0000 loss_kl_train: 149443420125212618049791721472.0000 loss_x_train: 0.0034 loss_x_val: 0.0275 time: 286.0957s\n",
      "disease8 Epoch: 0361 loss_train: 14944342741797348311040.0000 loss_kl_train: 149443420127292102401171914752.0000 loss_x_train: 0.0013 loss_x_val: 0.0275 time: 286.1120s\n",
      "disease8 Epoch: 0362 loss_train: 14944342741589409398784.0000 loss_kl_train: 149443420125212741195094032384.0000 loss_x_train: 0.0013 loss_x_val: 0.0275 time: 286.1178s\n",
      "disease8 Epoch: 0363 loss_train: 14944342741589430370304.0000 loss_kl_train: 149443420125212934709140520960.0000 loss_x_train: 0.0013 loss_x_val: 0.0275 time: 286.1323s\n",
      "disease8 Epoch: 0364 loss_train: 17056507831038016225280.0000 loss_kl_train: 170565070243651964726201548800.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1229s\n",
      "disease8 Epoch: 0365 loss_train: 17056507831038016225280.0000 loss_kl_train: 170565070243651964726201548800.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.0984s\n",
      "disease8 Epoch: 0366 loss_train: 17056491929660737716224.0000 loss_kl_train: 170564911617205830662204948480.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.0771s\n",
      "disease8 Epoch: 0367 loss_train: 17056491929660404269056.0000 loss_kl_train: 170564911617202488146856509440.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.0977s\n",
      "disease8 Epoch: 0368 loss_train: 17056475598515832094720.0000 loss_kl_train: 170564748183890872098443231232.0000 loss_x_train: 0.0014 loss_x_val: 0.0274 time: 286.1551s\n",
      "disease8 Epoch: 0369 loss_train: 17056475598515832094720.0000 loss_kl_train: 170564748183890872098443231232.0000 loss_x_train: 0.0013 loss_x_val: 0.0275 time: 286.1295s\n",
      "disease8 Epoch: 0370 loss_train: 17056459267371257823232.0000 loss_kl_train: 170564584750579291234402041856.0000 loss_x_train: 0.0013 loss_x_val: 0.0275 time: 286.1487s\n",
      "disease8 Epoch: 0371 loss_train: 17056459267371257823232.0000 loss_kl_train: 170564584750579291234402041856.0000 loss_x_train: 0.0014 loss_x_val: 0.0275 time: 286.1429s\n",
      "disease8 Epoch: 0372 loss_train: 17056459274745909608448.0000 loss_kl_train: 170564584824325805501338091520.0000 loss_x_train: 0.0016 loss_x_val: 0.0274 time: 286.1489s\n",
      "disease8 Epoch: 0373 loss_train: 17056427034849073692672.0000 loss_kl_train: 170564262690818198606643724288.0000 loss_x_train: 0.0013 loss_x_val: 0.0274 time: 286.1559s\n",
      "disease8 Epoch: 0374 loss_train: 17056427034849073692672.0000 loss_kl_train: 170564262690818198606643724288.0000 loss_x_train: 0.0015 loss_x_val: 0.0275 time: 286.1481s\n",
      "disease8 Epoch: 0375 loss_train: 17056427034849073692672.0000 loss_kl_train: 170564262690818198606643724288.0000 loss_x_train: 0.0013 loss_x_val: 0.0274 time: 286.0693s\n",
      "disease8 Epoch: 0376 loss_train: 17056427034849073692672.0000 loss_kl_train: 170564262690818198606643724288.0000 loss_x_train: 0.0013 loss_x_val: 0.0274 time: 285.9753s\n",
      "disease8 Epoch: 0377 loss_train: 17056427034849073692672.0000 loss_kl_train: 170564262690818198606643724288.0000 loss_x_train: 0.0015 loss_x_val: 0.0274 time: 286.0781s\n",
      "disease8 Epoch: 0378 loss_train: 17056427114258724028416.0000 loss_kl_train: 170564263484914734399676743680.0000 loss_x_train: 0.0012 loss_x_val: 0.0274 time: 286.0651s\n",
      "disease8 Epoch: 0379 loss_train: 17056395088838197772288.0000 loss_kl_train: 170563943034488158121233481728.0000 loss_x_train: 0.0015 loss_x_val: 0.0274 time: 286.0993s\n",
      "control8 Epoch: 0380 loss_train: inf loss_kl_train: inf loss_x_train: 0.0036 loss_x_val: 0.0198 time: 286.0732s\n",
      "control8 Epoch: 0381 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0198 time: 286.0446s\n",
      "control8 Epoch: 0382 loss_train: inf loss_kl_train: inf loss_x_train: 0.0015 loss_x_val: 0.0198 time: 286.0880s\n",
      "control8 Epoch: 0383 loss_train: inf loss_kl_train: inf loss_x_train: 0.0020 loss_x_val: 0.0198 time: 286.1103s\n",
      "control8 Epoch: 0384 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0198 time: 286.0907s\n",
      "control8 Epoch: 0385 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0198 time: 286.1097s\n",
      "control8 Epoch: 0386 loss_train: inf loss_kl_train: inf loss_x_train: 0.0014 loss_x_val: 0.0198 time: 286.1203s\n",
      "control8 Epoch: 0387 loss_train: inf loss_kl_train: inf loss_x_train: 0.0015 loss_x_val: 0.0198 time: 286.0779s\n",
      "control8 Epoch: 0388 loss_train: inf loss_kl_train: inf loss_x_train: 0.0018 loss_x_val: 0.0197 time: 286.0908s\n",
      "control8 Epoch: 0389 loss_train: inf loss_kl_train: inf loss_x_train: 0.0015 loss_x_val: 0.0198 time: 286.0835s\n",
      "control8 Epoch: 0390 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0198 time: 286.1060s\n",
      "control8 Epoch: 0391 loss_train: inf loss_kl_train: inf loss_x_train: 0.0018 loss_x_val: 0.0196 time: 286.0969s\n",
      "control8 Epoch: 0392 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0196 time: 286.1318s\n",
      "control8 Epoch: 0393 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0198 time: 286.1252s\n",
      "control8 Epoch: 0394 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0198 time: 286.1015s\n",
      "control8 Epoch: 0395 loss_train: inf loss_kl_train: inf loss_x_train: 0.0012 loss_x_val: 0.0198 time: 286.1272s\n",
      "control8 Epoch: 0396 loss_train: inf loss_kl_train: inf loss_x_train: 0.0015 loss_x_val: 0.0198 time: 286.0941s\n",
      "control8 Epoch: 0397 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0198 time: 286.0910s\n",
      "control8 Epoch: 0398 loss_train: inf loss_kl_train: inf loss_x_train: 0.0015 loss_x_val: 0.0198 time: 286.1407s\n",
      "control8 Epoch: 0399 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0199 time: 286.1583s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0400 loss_train: 453563901.5562 loss_kl_train: 4535638993227157.0000 loss_x_train: 0.0040 loss_x_val: 0.0225 time: 276.9904s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0401 loss_train: 37122.0526 loss_kl_train: 371220510375.2941 loss_x_train: 0.0014 loss_x_val: 0.0225 time: 276.9229s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0402 loss_train: 37069.7739 loss_kl_train: 370697740137.2865 loss_x_train: 0.0012 loss_x_val: 0.0225 time: 276.9769s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0403 loss_train: 36985.9052 loss_kl_train: 369859047557.0632 loss_x_train: 0.0014 loss_x_val: 0.0225 time: 276.9441s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0404 loss_train: 36896.2356 loss_kl_train: 368962338863.1551 loss_x_train: 0.0013 loss_x_val: 0.0225 time: 276.9883s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0405 loss_train: 37182.5495 loss_kl_train: 371825478180.3752 loss_x_train: 0.0013 loss_x_val: 0.0225 time: 277.0088s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0406 loss_train: 5837676577460790272.0000 loss_kl_train: 58376764778082438563758080.0000 loss_x_train: 0.0013 loss_x_val: 0.0225 time: 276.9904s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0407 loss_train: 2626433912158906368.0000 loss_kl_train: 26264339674301934405156864.0000 loss_x_train: 0.0015 loss_x_val: 0.0226 time: 276.9950s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0408 loss_train: 1638396771137656832.0000 loss_kl_train: 16383966879422079148490752.0000 loss_x_train: 0.0013 loss_x_val: 0.0225 time: 276.9187s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0409 loss_train: 1086323484757882752.0000 loss_kl_train: 10863234403840174300594176.0000 loss_x_train: 0.0017 loss_x_val: 0.0225 time: 276.9949s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0410 loss_train: 737166058180228992.0000 loss_kl_train: 7371660139931657620684800.0000 loss_x_train: 0.0014 loss_x_val: 0.0225 time: 276.8666s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0411 loss_train: 505410897924828992.0000 loss_kl_train: 5054109091972371152109568.0000 loss_x_train: 0.0014 loss_x_val: 0.0226 time: 276.9654s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0412 loss_train: 348197178751885056.0000 loss_kl_train: 3481971814360079235284992.0000 loss_x_train: 0.0018 loss_x_val: 0.0225 time: 276.9523s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0413 loss_train: 240451100624452064.0000 loss_kl_train: 2404510893310055431536640.0000 loss_x_train: 0.0015 loss_x_val: 0.0225 time: 277.0018s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0414 loss_train: 166239529441845440.0000 loss_kl_train: 1662395273202061270843392.0000 loss_x_train: 0.0016 loss_x_val: 0.0225 time: 276.9974s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0415 loss_train: 114995794373020704.0000 loss_kl_train: 1149957904491323691368448.0000 loss_x_train: 0.0016 loss_x_val: 0.0225 time: 277.0020s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0416 loss_train: 79570523124670224.0000 loss_kl_train: 795705191923434253189120.0000 loss_x_train: 0.0018 loss_x_val: 0.0225 time: 277.0256s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0417 loss_train: 55067710077684328.0000 loss_kl_train: 550677090392323972399104.0000 loss_x_train: 0.0021 loss_x_val: 0.0225 time: 276.9158s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0418 loss_train: 38112148038923336.0000 loss_kl_train: 381121474843540537212928.0000 loss_x_train: 0.0020 loss_x_val: 0.0225 time: 277.0176s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0419 loss_train: 26378674708704072.0000 loss_kl_train: 263786754671837708288000.0000 loss_x_train: 0.0024 loss_x_val: 0.0225 time: 276.9827s\n",
      "disease13 Epoch: 0420 loss_train: 28254724442895.2070 loss_kl_train: 282547248287987466240.0000 loss_x_train: 0.0034 loss_x_val: 0.0270 time: 330.7728s\n",
      "disease13 Epoch: 0421 loss_train: 13326558983371.7031 loss_kl_train: 133265590042821918720.0000 loss_x_train: 0.0025 loss_x_val: 0.0270 time: 330.7153s\n",
      "disease13 Epoch: 0422 loss_train: 2550368668998955.0000 loss_kl_train: 25503686467631341109248.0000 loss_x_train: 0.0023 loss_x_val: 0.0270 time: 330.7496s\n",
      "disease13 Epoch: 0423 loss_train: 4328505796489.4224 loss_kl_train: 43285059020744777728.0000 loss_x_train: 0.0026 loss_x_val: 0.0270 time: 330.7082s\n",
      "disease13 Epoch: 0424 loss_train: 2552556641680.1108 loss_kl_train: 25525566176828022784.0000 loss_x_train: 0.0025 loss_x_val: 0.0270 time: 330.7563s\n",
      "disease13 Epoch: 0425 loss_train: 1514364859173.7666 loss_kl_train: 15143648532179935232.0000 loss_x_train: 0.0023 loss_x_val: 0.0270 time: 330.7055s\n",
      "disease13 Epoch: 0426 loss_train: 899919891282.3834 loss_kl_train: 8999199052664948736.0000 loss_x_train: 0.0024 loss_x_val: 0.0270 time: 330.7656s\n",
      "disease13 Epoch: 0427 loss_train: 533772038120.5425 loss_kl_train: 5337720407088882688.0000 loss_x_train: 0.0022 loss_x_val: 0.0270 time: 330.7630s\n",
      "disease13 Epoch: 0428 loss_train: 315177668718.1833 loss_kl_train: 3151776695875177984.0000 loss_x_train: 0.0023 loss_x_val: 0.0270 time: 330.7381s\n",
      "disease13 Epoch: 0429 loss_train: 185033701186.1570 loss_kl_train: 1850336944833387776.0000 loss_x_train: 0.0022 loss_x_val: 0.0270 time: 330.7506s\n",
      "disease13 Epoch: 0430 loss_train: 108021044326.0268 loss_kl_train: 1080210443478078976.0000 loss_x_train: 0.0020 loss_x_val: 0.0270 time: 330.7288s\n",
      "disease13 Epoch: 0431 loss_train: 62800146882.6765 loss_kl_train: 628001459807235840.0000 loss_x_train: 0.0022 loss_x_val: 0.0270 time: 330.7545s\n",
      "disease13 Epoch: 0432 loss_train: 36429454284.0827 loss_kl_train: 364294534485120576.0000 loss_x_train: 0.0018 loss_x_val: 0.0270 time: 330.6993s\n",
      "disease13 Epoch: 0433 loss_train: 21123416094.1699 loss_kl_train: 211234160777903680.0000 loss_x_train: 0.0023 loss_x_val: 0.0270 time: 330.6255s\n",
      "disease13 Epoch: 0434 loss_train: 12258550819.7378 loss_kl_train: 122585506268078256.0000 loss_x_train: 0.0021 loss_x_val: 0.0270 time: 330.7653s\n",
      "disease13 Epoch: 0435 loss_train: 7123184659.3184 loss_kl_train: 71231845948177672.0000 loss_x_train: 0.0019 loss_x_val: 0.0269 time: 330.7624s\n",
      "disease13 Epoch: 0436 loss_train: 4139834984.6045 loss_kl_train: 41398350577867632.0000 loss_x_train: 0.0021 loss_x_val: 0.0270 time: 330.7434s\n",
      "disease13 Epoch: 0437 loss_train: 2402497076.4342 loss_kl_train: 24024970310293252.0000 loss_x_train: 0.0019 loss_x_val: 0.0270 time: 330.7550s\n",
      "disease13 Epoch: 0438 loss_train: 1390499381.9900 loss_kl_train: 13904993656717576.0000 loss_x_train: 0.0019 loss_x_val: 0.0269 time: 330.7623s\n",
      "disease13 Epoch: 0439 loss_train: 801475392.4660 loss_kl_train: 8014753861515810.0000 loss_x_train: 0.0018 loss_x_val: 0.0270 time: 330.7394s\n",
      "disease8 Epoch: 0440 loss_train: 17056395089931447304192.0000 loss_kl_train: 170563943045420646216675360768.0000 loss_x_train: 0.0036 loss_x_val: 0.0275 time: 286.0927s\n",
      "disease8 Epoch: 0441 loss_train: 17056395088838199869440.0000 loss_kl_train: 170563943034488158121233481728.0000 loss_x_train: 0.0018 loss_x_val: 0.0275 time: 286.0458s\n",
      "disease8 Epoch: 0442 loss_train: 17056378757693627695104.0000 loss_kl_train: 170563779601176577257192292352.0000 loss_x_train: 0.0016 loss_x_val: 0.0275 time: 286.1090s\n",
      "disease8 Epoch: 0443 loss_train: 17056378757693627695104.0000 loss_kl_train: 170563779601176577257192292352.0000 loss_x_train: 0.0019 loss_x_val: 0.0274 time: 286.1270s\n",
      "disease8 Epoch: 0444 loss_train: 17056362426549053423616.0000 loss_kl_train: 170563616167864961208779014144.0000 loss_x_train: 0.0016 loss_x_val: 0.0273 time: 286.1344s\n",
      "disease8 Epoch: 0445 loss_train: 17056362426549053423616.0000 loss_kl_train: 170563616167864961208779014144.0000 loss_x_train: 0.0016 loss_x_val: 0.0274 time: 286.0775s\n",
      "disease8 Epoch: 0446 loss_train: 17056362426549053423616.0000 loss_kl_train: 170563616167864961208779014144.0000 loss_x_train: 0.0018 loss_x_val: 0.0274 time: 286.0792s\n",
      "disease8 Epoch: 0447 loss_train: 17056362426549053423616.0000 loss_kl_train: 170563616167864961208779014144.0000 loss_x_train: 0.0017 loss_x_val: 0.0274 time: 286.0937s\n",
      "disease8 Epoch: 0448 loss_train: 17056362426549053423616.0000 loss_kl_train: 170563616167864961208779014144.0000 loss_x_train: 0.0019 loss_x_val: 0.0274 time: 286.0355s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease8 Epoch: 0449 loss_train: 17056330337282523398144.0000 loss_kl_train: 170563296511534920723368771584.0000 loss_x_train: 0.0018 loss_x_val: 0.0273 time: 286.0960s\n",
      "disease8 Epoch: 0450 loss_train: 17056330337282523398144.0000 loss_kl_train: 170563296511534920723368771584.0000 loss_x_train: 0.0015 loss_x_val: 0.0274 time: 286.1192s\n",
      "disease8 Epoch: 0451 loss_train: 17056314292649259433984.0000 loss_kl_train: 170563135481654392001675657216.0000 loss_x_train: 0.0017 loss_x_val: 0.0273 time: 286.1453s\n",
      "disease8 Epoch: 0452 loss_train: 17056314292649259433984.0000 loss_kl_train: 170563135481654392001675657216.0000 loss_x_train: 0.0016 loss_x_val: 0.0273 time: 286.1609s\n",
      "disease8 Epoch: 0453 loss_train: 17056298391271649574912.0000 loss_kl_train: 170562976855204880237958529024.0000 loss_x_train: 0.0017 loss_x_val: 0.0273 time: 286.0556s\n",
      "disease8 Epoch: 0454 loss_train: 17056298391271649574912.0000 loss_kl_train: 170562976855204880237958529024.0000 loss_x_train: 0.0017 loss_x_val: 0.0272 time: 286.0857s\n",
      "disease8 Epoch: 0455 loss_train: 17056298391271649574912.0000 loss_kl_train: 170562976855204880237958529024.0000 loss_x_train: 0.0015 loss_x_val: 0.0273 time: 286.0440s\n",
      "disease8 Epoch: 0456 loss_train: 17056298391271649574912.0000 loss_kl_train: 170562976855204880237958529024.0000 loss_x_train: 0.0017 loss_x_val: 0.0273 time: 286.0772s\n",
      "disease8 Epoch: 0457 loss_train: 17056298391271649574912.0000 loss_kl_train: 170562976855204880237958529024.0000 loss_x_train: 0.0017 loss_x_val: 0.0273 time: 286.0789s\n",
      "disease8 Epoch: 0458 loss_train: 17056298391271649574912.0000 loss_kl_train: 170562976855204880237958529024.0000 loss_x_train: 0.0014 loss_x_val: 0.0272 time: 286.0297s\n",
      "disease8 Epoch: 0459 loss_train: 17056282346638383513600.0000 loss_kl_train: 170562815825324351516265414656.0000 loss_x_train: 0.0016 loss_x_val: 0.0273 time: 286.0751s\n",
      "control8 Epoch: 0460 loss_train: inf loss_kl_train: inf loss_x_train: 0.0037 loss_x_val: 0.0196 time: 285.9855s\n",
      "control8 Epoch: 0461 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0198 time: 286.0065s\n",
      "control8 Epoch: 0462 loss_train: inf loss_kl_train: inf loss_x_train: 0.0018 loss_x_val: 0.0197 time: 286.0726s\n",
      "control8 Epoch: 0463 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0197 time: 286.0848s\n",
      "control8 Epoch: 0464 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0197 time: 286.1270s\n",
      "control8 Epoch: 0465 loss_train: inf loss_kl_train: inf loss_x_train: 0.0019 loss_x_val: 0.0197 time: 286.1108s\n",
      "control8 Epoch: 0466 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0197 time: 286.1246s\n",
      "control8 Epoch: 0467 loss_train: inf loss_kl_train: inf loss_x_train: 0.0018 loss_x_val: 0.0198 time: 286.1096s\n",
      "control8 Epoch: 0468 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0197 time: 286.0660s\n",
      "control8 Epoch: 0469 loss_train: inf loss_kl_train: inf loss_x_train: 0.0018 loss_x_val: 0.0197 time: 286.0561s\n",
      "control8 Epoch: 0470 loss_train: inf loss_kl_train: inf loss_x_train: 0.0019 loss_x_val: 0.0196 time: 286.0790s\n",
      "control8 Epoch: 0471 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0197 time: 286.0886s\n",
      "control8 Epoch: 0472 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0198 time: 286.1093s\n",
      "control8 Epoch: 0473 loss_train: inf loss_kl_train: inf loss_x_train: 0.0018 loss_x_val: 0.0197 time: 286.1334s\n",
      "control8 Epoch: 0474 loss_train: inf loss_kl_train: inf loss_x_train: 0.0018 loss_x_val: 0.0197 time: 286.1073s\n",
      "control8 Epoch: 0475 loss_train: inf loss_kl_train: inf loss_x_train: 0.0017 loss_x_val: 0.0196 time: 286.1217s\n",
      "control8 Epoch: 0476 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0196 time: 286.1059s\n",
      "control8 Epoch: 0477 loss_train: inf loss_kl_train: inf loss_x_train: 0.0015 loss_x_val: 0.0197 time: 286.1260s\n",
      "control8 Epoch: 0478 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0197 time: 286.1201s\n",
      "control8 Epoch: 0479 loss_train: inf loss_kl_train: inf loss_x_train: 0.0016 loss_x_val: 0.0196 time: 286.1600s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0480 loss_train: 21181637709176796.0000 loss_kl_train: 211816367620452935270400.0000 loss_x_train: 0.0044 loss_x_val: 0.0224 time: 276.9669s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0481 loss_train: 9529880441493822.0000 loss_kl_train: 95298805985880685150208.0000 loss_x_train: 0.0024 loss_x_val: 0.0224 time: 276.9900s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0482 loss_train: 5944749499169484.0000 loss_kl_train: 59447492595183935029248.0000 loss_x_train: 0.0026 loss_x_val: 0.0224 time: 276.9668s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0483 loss_train: 3941654364030316.0000 loss_kl_train: 39416542070227645497344.0000 loss_x_train: 0.0026 loss_x_val: 0.0224 time: 276.9286s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0484 loss_train: 2674749727083999.5000 loss_kl_train: 26747497378885633310720.0000 loss_x_train: 0.0029 loss_x_val: 0.0224 time: 276.9490s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0485 loss_train: 1833858102978836.7500 loss_kl_train: 18338580248272631234560.0000 loss_x_train: 0.0032 loss_x_val: 0.0223 time: 276.9769s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0486 loss_train: 1263414950957569.2500 loss_kl_train: 12634148780727764779008.0000 loss_x_train: 0.0035 loss_x_val: 0.0225 time: 276.9486s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0487 loss_train: 872477796520072.0000 loss_kl_train: 8724777777854947524608.0000 loss_x_train: 0.0039 loss_x_val: 0.0224 time: 276.9570s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0488 loss_train: 603262079286430.1250 loss_kl_train: 6032620454322967150592.0000 loss_x_train: 0.0040 loss_x_val: 0.0223 time: 276.9593s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0489 loss_train: 418070113008659.1250 loss_kl_train: 4180701045216697647104.0000 loss_x_train: 0.0046 loss_x_val: 0.0223 time: 276.9599s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0490 loss_train: 293942196743632.6875 loss_kl_train: 2939421933819993784320.0000 loss_x_train: 0.0050 loss_x_val: 0.0224 time: 277.0169s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0491 loss_train: 211023052056360.1562 loss_kl_train: 2110230421949641654272.0000 loss_x_train: 0.0049 loss_x_val: 0.0223 time: 276.8947s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0492 loss_train: 151846046237448.6562 loss_kl_train: 1518460407144705949696.0000 loss_x_train: 0.0047 loss_x_val: 0.0223 time: 276.9937s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0493 loss_train: 108988779000087.1094 loss_kl_train: 1089887739974222217216.0000 loss_x_train: 0.0052 loss_x_val: 0.0223 time: 276.9855s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0494 loss_train: 78117960555688.9531 loss_kl_train: 781179629951540396032.0000 loss_x_train: 0.0054 loss_x_val: 0.0224 time: 276.9812s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0495 loss_train: 55913683519712.9766 loss_kl_train: 559136818614119759872.0000 loss_x_train: 0.0054 loss_x_val: 0.0222 time: 276.9954s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all zeros 865\n",
      "control13 Epoch: 0496 loss_train: 40012997900557.7266 loss_kl_train: 400129956056824545280.0000 loss_x_train: 0.0052 loss_x_val: 0.0223 time: 276.9704s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0497 loss_train: 28632724988479.4023 loss_kl_train: 286327237727421005824.0000 loss_x_train: 0.0053 loss_x_val: 0.0221 time: 276.9429s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0498 loss_train: 20481112388496.3477 loss_kl_train: 204811130160489103360.0000 loss_x_train: 0.0052 loss_x_val: 0.0222 time: 276.9507s\n",
      "all zeros 81\n",
      "all zeros 86\n",
      "all zeros 351\n",
      "all zeros 480\n",
      "all zeros 502\n",
      "all zeros 590\n",
      "all zeros 865\n",
      "control13 Epoch: 0499 loss_train: 14643666384173.0215 loss_kl_train: 146436666774278750208.0000 loss_x_train: 0.0052 loss_x_val: 0.0223 time: 276.9464s\n",
      "disease13 Epoch: 0500 loss_train: 7369640209211885568.0000 loss_kl_train: 73696402263007789745963008.0000 loss_x_train: 0.0054 loss_x_val: 0.0269 time: 330.7297s\n",
      "disease13 Epoch: 0501 loss_train: 6080782741407387648.0000 loss_kl_train: 60807824645205675374280704.0000 loss_x_train: 0.0044 loss_x_val: 0.0269 time: 330.6765s\n",
      "disease13 Epoch: 0502 loss_train: 5719385216416068608.0000 loss_kl_train: 57193851640051543612325888.0000 loss_x_train: 0.0045 loss_x_val: 0.0269 time: 330.7212s\n",
      "disease13 Epoch: 0503 loss_train: 5381679921435732992.0000 loss_kl_train: 53816800543690757232394240.0000 loss_x_train: 0.0047 loss_x_val: 0.0269 time: 330.7653s\n",
      "disease13 Epoch: 0504 loss_train: 5063861148337784832.0000 loss_kl_train: 50638610211510993991761920.0000 loss_x_train: 0.0047 loss_x_val: 0.0269 time: 330.7438s\n",
      "disease13 Epoch: 0505 loss_train: 4765961730858435584.0000 loss_kl_train: 47659616345432204756647936.0000 loss_x_train: 0.0047 loss_x_val: 0.0269 time: 330.7776s\n",
      "disease13 Epoch: 0506 loss_train: 4487620812756813312.0000 loss_kl_train: 44876205716724503575265280.0000 loss_x_train: 0.0049 loss_x_val: 0.0268 time: 330.7499s\n",
      "disease13 Epoch: 0507 loss_train: 4228392715679969792.0000 loss_kl_train: 42283928679873994272276480.0000 loss_x_train: 0.0047 loss_x_val: 0.0269 time: 330.7505s\n",
      "disease13 Epoch: 0508 loss_train: 3987341256399455744.0000 loss_kl_train: 39873411421481421028458496.0000 loss_x_train: 0.0048 loss_x_val: 0.0269 time: 330.7472s\n",
      "disease13 Epoch: 0509 loss_train: 3762800685227566592.0000 loss_kl_train: 37628007072070603607375872.0000 loss_x_train: 0.0047 loss_x_val: 0.0269 time: 330.7155s\n",
      "disease13 Epoch: 0510 loss_train: 3553309349103388672.0000 loss_kl_train: 35533092069411307147755520.0000 loss_x_train: 0.0048 loss_x_val: 0.0268 time: 330.6528s\n",
      "disease13 Epoch: 0511 loss_train: 3357331213348923904.0000 loss_kl_train: 33573311650281252584423424.0000 loss_x_train: 0.0049 loss_x_val: 0.0268 time: 330.7629s\n",
      "disease13 Epoch: 0512 loss_train: 3173541168891312128.0000 loss_kl_train: 31735410483172160995065856.0000 loss_x_train: 0.0050 loss_x_val: 0.0269 time: 330.7657s\n",
      "disease13 Epoch: 0513 loss_train: 3000896545666028032.0000 loss_kl_train: 30008965436769308350873600.0000 loss_x_train: 0.0049 loss_x_val: 0.0269 time: 330.6798s\n",
      "disease13 Epoch: 0514 loss_train: 2838651335366464000.0000 loss_kl_train: 28386512176750458130399232.0000 loss_x_train: 0.0052 loss_x_val: 0.0269 time: 330.7151s\n",
      "disease13 Epoch: 0515 loss_train: 2686131340977096704.0000 loss_kl_train: 26861313017576252732604416.0000 loss_x_train: 0.0051 loss_x_val: 0.0269 time: 330.7866s\n",
      "disease13 Epoch: 0516 loss_train: 2542697646971296256.0000 loss_kl_train: 25426975699965896820785152.0000 loss_x_train: 0.0051 loss_x_val: 0.0269 time: 330.7349s\n",
      "disease13 Epoch: 0517 loss_train: 2407737702037594112.0000 loss_kl_train: 24077376537678306107981824.0000 loss_x_train: 0.0049 loss_x_val: 0.0269 time: 330.7083s\n",
      "disease13 Epoch: 0518 loss_train: 2280748104721047040.0000 loss_kl_train: 22807481808218217796075520.0000 loss_x_train: 0.0051 loss_x_val: 0.0269 time: 330.7587s\n",
      "disease13 Epoch: 0519 loss_train: 2161185170877493504.0000 loss_kl_train: 21611852362087404088066048.0000 loss_x_train: 0.0053 loss_x_val: 0.0269 time: 330.6656s\n",
      "disease8 Epoch: 0520 loss_train: 260337044638734965800960.0000 loss_kl_train: 2603370395745338636531954352128.0000 loss_x_train: 0.0071 loss_x_val: 0.0271 time: 285.9456s\n",
      "disease8 Epoch: 0521 loss_train: 260337044612742662586368.0000 loss_kl_train: 2603370395485415635838079860736.0000 loss_x_train: 0.0062 loss_x_val: 0.0270 time: 286.0056s\n",
      "disease8 Epoch: 0522 loss_train: 260337044612742662586368.0000 loss_kl_train: 2603370395485415635838079860736.0000 loss_x_train: 0.0060 loss_x_val: 0.0271 time: 286.0599s\n",
      "disease8 Epoch: 0523 loss_train: 260337044612742662586368.0000 loss_kl_train: 2603370395485415635838079860736.0000 loss_x_train: 0.0061 loss_x_val: 0.0270 time: 286.1118s\n",
      "disease8 Epoch: 0524 loss_train: 260337012666731790860288.0000 loss_kl_train: 2603370075829085665721413795840.0000 loss_x_train: 0.0060 loss_x_val: 0.0270 time: 286.1216s\n",
      "disease8 Epoch: 0525 loss_train: 260337012666731790860288.0000 loss_kl_train: 2603370075829085665721413795840.0000 loss_x_train: 0.0059 loss_x_val: 0.0272 time: 286.1348s\n",
      "disease8 Epoch: 0526 loss_train: 260337012666731790860288.0000 loss_kl_train: 2603370075829085665721413795840.0000 loss_x_train: 0.0062 loss_x_val: 0.0270 time: 286.1713s\n",
      "disease8 Epoch: 0527 loss_train: 260336996622098531090432.0000 loss_kl_train: 2603369914799205488843441569792.0000 loss_x_train: 0.0058 loss_x_val: 0.0266 time: 286.1196s\n",
      "disease8 Epoch: 0528 loss_train: 260336980434209589952512.0000 loss_kl_train: 2603369753769324749015515922432.0000 loss_x_train: 0.0059 loss_x_val: 0.0269 time: 286.1320s\n",
      "disease8 Epoch: 0529 loss_train: 260336980434209589952512.0000 loss_kl_train: 2603369753769324749015515922432.0000 loss_x_train: 0.0060 loss_x_val: 0.0268 time: 286.1322s\n",
      "disease8 Epoch: 0530 loss_train: 260336980434209589952512.0000 loss_kl_train: 2603369753769324749015515922432.0000 loss_x_train: 0.0057 loss_x_val: 0.0268 time: 286.1106s\n",
      "disease8 Epoch: 0531 loss_train: 260336964532831977996288.0000 loss_kl_train: 2603369595142874955776822083584.0000 loss_x_train: 0.0057 loss_x_val: 0.0266 time: 286.1000s\n",
      "disease8 Epoch: 0532 loss_train: 260336947771920445603840.0000 loss_kl_train: 2603369426902701376181201010688.0000 loss_x_train: 0.0059 loss_x_val: 0.0268 time: 286.1443s\n",
      "disease8 Epoch: 0533 loss_train: 260336947771920445603840.0000 loss_kl_train: 2603369426902701376181201010688.0000 loss_x_train: 0.0056 loss_x_val: 0.0266 time: 286.1296s\n",
      "disease8 Epoch: 0534 loss_train: 260336947771920445603840.0000 loss_kl_train: 2603369426902701376181201010688.0000 loss_x_train: 0.0056 loss_x_val: 0.0265 time: 286.0370s\n",
      "disease8 Epoch: 0535 loss_train: 260336947771920445603840.0000 loss_kl_train: 2603369426902701376181201010688.0000 loss_x_train: 0.0057 loss_x_val: 0.0265 time: 286.0759s\n",
      "disease8 Epoch: 0536 loss_train: 260336947771920445603840.0000 loss_kl_train: 2603369426902701376181201010688.0000 loss_x_train: 0.0056 loss_x_val: 0.0268 time: 286.1063s\n",
      "disease8 Epoch: 0537 loss_train: 260336947771920512712704.0000 loss_kl_train: 2603369426902702502081107853312.0000 loss_x_train: 0.0057 loss_x_val: 0.0266 time: 286.0403s\n",
      "disease8 Epoch: 0538 loss_train: 260336931870542833647616.0000 loss_kl_train: 2603369268276252145892460593152.0000 loss_x_train: 0.0056 loss_x_val: 0.0262 time: 286.0757s\n",
      "disease8 Epoch: 0539 loss_train: 260336931870542833647616.0000 loss_kl_train: 2603369268276252145892460593152.0000 loss_x_train: 0.0055 loss_x_val: 0.0267 time: 286.1323s\n",
      "control8 Epoch: 0540 loss_train: inf loss_kl_train: inf loss_x_train: 0.0077 loss_x_val: 0.0190 time: 286.1119s\n",
      "control8 Epoch: 0541 loss_train: inf loss_kl_train: inf loss_x_train: 0.0062 loss_x_val: 0.0193 time: 286.0809s\n",
      "control8 Epoch: 0542 loss_train: inf loss_kl_train: inf loss_x_train: 0.0064 loss_x_val: 0.0194 time: 286.1091s\n",
      "control8 Epoch: 0543 loss_train: inf loss_kl_train: inf loss_x_train: 0.0062 loss_x_val: 0.0191 time: 286.0890s\n",
      "control8 Epoch: 0544 loss_train: inf loss_kl_train: inf loss_x_train: 0.0062 loss_x_val: 0.0193 time: 286.0539s\n",
      "control8 Epoch: 0545 loss_train: inf loss_kl_train: inf loss_x_train: 0.0061 loss_x_val: 0.0193 time: 286.0699s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control8 Epoch: 0546 loss_train: inf loss_kl_train: inf loss_x_train: 0.0060 loss_x_val: 0.0192 time: 286.0718s\n",
      "control8 Epoch: 0547 loss_train: inf loss_kl_train: inf loss_x_train: 0.0060 loss_x_val: 0.0193 time: 286.0871s\n",
      "control8 Epoch: 0548 loss_train: inf loss_kl_train: inf loss_x_train: 0.0058 loss_x_val: 0.0192 time: 286.0819s\n",
      "control8 Epoch: 0549 loss_train: inf loss_kl_train: inf loss_x_train: 0.0059 loss_x_val: 0.0191 time: 286.1214s\n",
      "control8 Epoch: 0550 loss_train: inf loss_kl_train: inf loss_x_train: 0.0058 loss_x_val: 0.0192 time: 286.0918s\n",
      "control8 Epoch: 0551 loss_train: inf loss_kl_train: inf loss_x_train: 0.0059 loss_x_val: 0.0189 time: 286.0728s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-70c45ff02cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mtrain_loss_advD_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_advD_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtrain_loss_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_kl_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_x_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_x_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-70c45ff02cb3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtrainInput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainInput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizerVAEXA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(modelsavepath,str(9360)+'.pt')))\n",
    "# epochs=20000\n",
    "if pretrainedAE:\n",
    "    print('loading '+pretrainedAE['name']+' epoch '+str(pretrainedAE['epoch']))\n",
    "    model.load_state_dict(torch.load(os.path.join('/mnt/xinyi/pamrats/models/train_gae_starmap/'+pretrainedAE['name'],str(pretrainedAE['epoch'])+'.pt')))\n",
    "    \n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    loss_kl_train_all=0\n",
    "    loss_x_train_all=0\n",
    "    loss_all=0\n",
    "    for batch_idx, trainInput in enumerate(trainInputloader):\n",
    "#         print(batch_idx)\n",
    "        if torch.sum(trainInput)==0:\n",
    "            print('all zeros '+str(batch_idx))\n",
    "\n",
    "        if use_cuda:\n",
    "            trainInput=trainInput.cuda().float()\n",
    "        optimizerVAEXA.zero_grad()\n",
    "\n",
    "        recon, z, mu, logvar = model(trainInput)\n",
    "\n",
    "        if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "            modelAdv.eval()\n",
    "            advOut=modelAdv(z)\n",
    "\n",
    "        loss_kl_train=loss_kl(mu, logvar)\n",
    "        loss_x_train=loss_x(recon, trainInput)\n",
    "        loss=loss_kl_train*kl_weight+loss_x_train \n",
    "        if torch.isnan(torch.sum(loss_kl_train)):\n",
    "            print('kl '+str(batch_idx))\n",
    "        if torch.isnan(torch.sum(mu)):\n",
    "            print('mu '+str(batch_idx))\n",
    "        if torch.isnan(torch.sum(logvar)):\n",
    "            print('logvar '+str(batch_idx))\n",
    "        if torch.isnan(torch.sum(z)):\n",
    "            print('z '+str(batch_idx))\n",
    "        if torch.isnan(torch.sum(loss_x_train)):\n",
    "            print('x loss '+str(batch_idx))\n",
    "        if torch.isnan(torch.sum(trainInput)):\n",
    "            print('x input '+str(batch_idx))\n",
    "        if torch.isnan(torch.sum(recon)):\n",
    "            print('x recon '+str(batch_idx))\n",
    "        \n",
    "        \n",
    "        loss_kl_train_all+=loss_kl_train.item()\n",
    "        loss_x_train_all+=loss_x_train.item()\n",
    "        loss_all+=loss.item()\n",
    "        \n",
    "        if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "            loss_adv_train=loss_adv(advOut,sampleLabel_ae,train_nodes_idx)\n",
    "            loss+=loss_adv_train*advWeight\n",
    "        loss.backward()\n",
    "        optimizerVAEXA.step()\n",
    "\n",
    "    loss_kl_train_all=loss_kl_train_all/len(trainInputloader.dataset)\n",
    "    loss_x_train_all=loss_x_train_all/len(trainInputloader.dataset)\n",
    "    loss_all=loss_all/len(trainInputloader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    loss_x_val_all=0\n",
    "    for batch_idx, valInput in enumerate(valInputloader):\n",
    "        if use_cuda:\n",
    "            valInput=valInput.cuda().float()\n",
    "        recon,z, mu, logvar = model(valInput)\n",
    "\n",
    "        if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "            advOut=modelAdv(z)\n",
    "\n",
    "        loss_x_val_all+=loss_x(recon, valInput).item()\n",
    "\n",
    "        if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "            loss_adv_val=loss_adv(advOut,sampleLabel_ae,val_nodes_idx)\n",
    "            loss_val+=loss_adv_val*advWeight\n",
    "    loss_x_val_all=loss_x_val_all/len(valInputloader.dataset)\n",
    "    \n",
    "    print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "          'loss_train: {:.4f}'.format(loss_all),\n",
    "          'loss_kl_train: {:.4f}'.format(loss_kl_train_all),\n",
    "          'loss_x_train: {:.4f}'.format(loss_x_train_all),\n",
    "          'loss_x_val: {:.4f}'.format(loss_x_val_all),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        print('loss_adv_train: {:.4f}'.format(loss_adv_train.item()),\n",
    "              'loss_adv_val: {:.4f}'.format(loss_adv_val.item())\n",
    "             )\n",
    "#     return loss.item(),loss_x_train.item(),loss_val.item(),loss_x_val.item()\n",
    "#     return loss.item(),loss_kl_train.item(),loss_x_train.item(),loss_val.item(),loss_x_val.item()\n",
    "    if adv:\n",
    "        if (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "            return float(loss),float(loss_kl_train),float(loss_x_train),float(loss_a_train),float(loss_val),float(loss_x_val),float(loss_a_val),float(loss_adv_train),float(loss_adv_val)        \n",
    "        else:\n",
    "            return float(loss),float(loss_kl_train),float(loss_x_train),float(loss_a_train),float(loss_val),float(loss_x_val),float(loss_a_val),None,None   \n",
    "    else:\n",
    "        return loss_all,loss_kl_train_all,loss_x_train_all,loss_x_val_all      \n",
    "\n",
    "def train_discriminator(epoch):\n",
    "    t = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    if adj_decodeName==None:\n",
    "        adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "    #     features_recon, z, mu, logvar=model(features.float())\n",
    "    else:\n",
    "        adj_recon,mu,logvar,z,features_recon = model(features, adj_norm,adj_decode)\n",
    "        \n",
    "    \n",
    "    if clf:\n",
    "        modelClf.eval()\n",
    "        clfOut=modelClf(z)\n",
    "        \n",
    "    modelAdv.train()\n",
    "    optimizerAdv.zero_grad()\n",
    "    advOut=modelAdv(z)\n",
    "    \n",
    "    loss_adv_train=loss_adv(advOut,sampleLabel_d,train_nodes_idx)\n",
    "    loss = loss_adv_train*advWeight\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizerAdv.step()\n",
    "\n",
    "    modelAdv.eval()\n",
    "    advOut=modelAdv(z)\n",
    "    loss_adv_val=loss_adv(advOut,sampleLabel_d,val_nodes_idx)\n",
    "    loss_val=loss_adv_val*advWeight\n",
    "    print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "          'loss_adv_train: {:.4f}'.format(loss_adv_train.item()),\n",
    "          'loss_adv_val: {:.4f}'.format(loss_adv_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    return float(loss_adv_train),float(loss_adv_val)\n",
    "    \n",
    "train_loss_ep=[None]*epochs\n",
    "train_loss_kl_ep=[None]*epochs\n",
    "train_loss_x_ep=[None]*epochs\n",
    "train_loss_adv_ep=[None]*epochs\n",
    "train_loss_advD_ep=[None]*epochs\n",
    "val_loss_x_ep=[None]*epochs\n",
    "val_loss_adv_ep=[None]*epochs\n",
    "val_loss_advD_ep=[None]*epochs\n",
    "t_ep=time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "# for ep in range(10000,20000):\n",
    "    t=int(ep/switchFreq)%len(training_samples)\n",
    "    training_samples_t=training_samples[t]\n",
    "    \n",
    "    trainInputnp, valInputnp, _=imageslist[training_samples_t]\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())):\n",
    "#         sampleLabel_ae=sampleLabellist_ae[training_samples_t]\n",
    "#         sampleLabel_d=sampleLabellist_d[training_samples_t]\n",
    "        sampleLabel_ae=sampleLabellist_ae[training_samples_t].cuda().float()\n",
    "        sampleLabel_d=sampleLabellist_d[training_samples_t].cuda().float()\n",
    "    \n",
    "    trainInputloader=DataLoader(trainInputnp, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    valInputloader=DataLoader(valInputnp, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    \n",
    "    \n",
    "    if adv:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep],train_loss_adv_ep[ep],val_loss_adv_ep[ep]=train(ep)\n",
    "        if (training_samples_t in list(sampleLabellist_ae.keys())):\n",
    "            train_loss_advD_ep[ep],val_loss_advD_ep[ep]=train_discriminator(ep)\n",
    "    else:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],val_loss_x_ep[ep]=train(ep)\n",
    "\n",
    "        \n",
    "    if ep%saveFreq == 0:\n",
    "        torch.save(model.cpu().state_dict(), os.path.join(modelsavepath,str(ep)+'.pt'))\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()\n",
    "print(' total time: {:.4f}s'.format(time.time() - t_ep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(logsavepath,'train_loss'), 'rb') as input:\n",
    "#     train_loss_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_kl'), 'rb') as input:\n",
    "#     train_loss_kl_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_x'), 'rb') as input:\n",
    "#     train_loss_x_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_a'), 'rb') as input:\n",
    "#     train_loss_a_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss'), 'rb') as input:\n",
    "#     val_loss_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss_x'), 'rb') as input:\n",
    "#     val_loss_x_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss_a'), 'rb') as input:\n",
    "#     val_loss_a_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_adv'), 'rb') as input:\n",
    "#     train_loss_adv_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss_adv'), 'rb') as input:\n",
    "#     val_loss_adv_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_advD'), 'rb') as input:\n",
    "#     train_loss_advD_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss_advD'), 'rb') as input:\n",
    "#     val_loss_advD_ep[:9360]=pickle.load(input)\n",
    "    \n",
    "with open(os.path.join(logsavepath,'train_loss'), 'wb') as output:\n",
    "    pickle.dump(train_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_kl'), 'wb') as output:\n",
    "    pickle.dump(train_loss_kl_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_x'), 'wb') as output:\n",
    "    pickle.dump(train_loss_x_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss_x'), 'wb') as output:\n",
    "    pickle.dump(val_loss_x_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "if adv:\n",
    "    with open(os.path.join(logsavepath,'train_loss_adv'), 'wb') as output:\n",
    "        pickle.dump(train_loss_adv_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_adv'), 'wb') as output:\n",
    "        pickle.dump(val_loss_adv_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'train_loss_advD'), 'wb') as output:\n",
    "        pickle.dump(train_loss_advD_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_advD'), 'wb') as output:\n",
    "        pickle.dump(val_loss_advD_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(os.path.join(logsavepath,'ct_unique'), 'wb') as output:\n",
    "#     pickle.dump(ct_unique, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(np.arange(epochs),train_loss_ep)\n",
    "# plt.plot(np.arange(epochs),val_loss_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_x_ep)\n",
    "plt.plot(np.arange(epochs),val_loss_x_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "# plt.plot(np.arange(epochs),np.array(train_loss_adv_ep)*advWeight)\n",
    "# plt.plot(np.arange(epochs),np.array(val_loss_adv_ep)*advWeight)\n",
    "# plt.plot(np.arange(epochs),np.array(train_loss_advD_ep)*advWeight)\n",
    "# plt.plot(np.arange(epochs),np.array(val_loss_advD_ep)*advWeight)\n",
    "# plt.ylim((0,0.01))\n",
    "# plt.xlim((0,3000))\n",
    "# plt.legend(['training loss','validation loss','training x recon loss','validation x recon loss','training kl loss'],loc='upper right')\n",
    "# plt.legend(['training x recon loss','validation x recon loss','training a recon loss','validation a recon loss','training kl loss','training classifier loss','validation classifier loss'],loc='upper right')\n",
    "plt.legend(['training x recon loss','validation x recon loss','training kl loss','training discriminator ae','validation discriminator ae','training discriminator d','validation discriminator d'],loc='upper right')\n",
    "# plt.legend(['training loss','validation loss','training x recon loss','validation x recon loss','training a recon loss','validation a recon loss','training kl loss','training discriminator ae','validation discriminator ae','training discriminator d','validation discriminator d'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath,'loss_seed3.jpg'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name='c13k20XA_07_dca'\n",
    "# logsavepath='/mnt/xinyi/pamrats/log/train_gae_starmap/'+name\n",
    "# with open(os.path.join(logsavepath,'val_loss_a'), 'rb') as output:\n",
    "#     val_loss_a_ep=pickle.load(output)\n",
    "np.argmin(val_loss_x_ep[:])\n",
    "# np.where(np.logical_not(np.isfinite(val_loss_ep[:])))\n",
    "val_loss_a_ep[8700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testepoch=9980\n",
    "model.load_state_dict(torch.load(os.path.join(modelsavepath,str(testepoch)+'.pt')))\n",
    "model.eval()\n",
    "for s in sampleidx.keys():\n",
    "    print(s)\n",
    "    _, _, testInputnp=imageslist[s]\n",
    "    \n",
    "    testInputloader=DataLoader(testInputnp, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    \n",
    "    loss_x_test_all=0\n",
    "    for batch_idx, testInput in enumerate(testInputloader):\n",
    "        if use_cuda:\n",
    "            testInput=testInput.cuda().float()\n",
    "        recon,z, mu, logvar = model(testInput)\n",
    "        \n",
    "        if adv and (s in list(sampleLabellist_ae.keys())):\n",
    "            sampleLabel_ae=sampleLabellist_ae[s].cuda().float()\n",
    "            modelAdv.eval()\n",
    "            advOut=modelAdv(z)\n",
    "\n",
    "        loss_x_test_all+=loss_x(recon, testInput).item()\n",
    "    loss_x_test_all=loss_x_test_all/len(testInputloader.dataset)\n",
    "\n",
    "    if adv and (s in list(sampleLabellist_ae.keys())):\n",
    "        loss_adv_test=loss_adv(advOut,sampleLabel_ae,test_nodes_idx)\n",
    "        print('loss_adv_test: {:.4f}'.format(loss_adv_test.item()))\n",
    "        \n",
    "    print('test results',\n",
    "          'loss_x_test: {:.4f}'.format(loss_x_test_all))\n",
    "#          'loss_adv_test: {:.4f}'.format(loss_adv_test.item()))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=features\n",
    "y_true_raw=features_raw\n",
    "preds=features_recon\n",
    "mask=nodesmask=torch.tensor(np.arange(num_nodes))\n",
    "reconWeight=20\n",
    "eps = 1e-10\n",
    "\n",
    "output,pi,theta,y_pred=preds\n",
    "nb_case=optimizer.optimizer_nb(preds,y_true,mask,reconWeight,eps = 1e-10,ifmean=False)- torch.log(pi+eps)\n",
    "\n",
    "zero_nb = torch.pow(theta/(theta+y_pred+eps), theta)\n",
    "zero_case = -torch.log(pi + ((1.0-pi)*zero_nb)+eps)\n",
    "result = torch.where(torch.lt(y_true_raw.cuda(), 1), zero_case, nb_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:8,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:8,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroidx=(features_raw==0)\n",
    "nonzeroidx=(features_raw!=0)\n",
    "torch.sum(result[zeroidx])/torch.sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(result[zeroidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.pi.weight[:8,:8]\n",
    "model_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
