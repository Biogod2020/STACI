{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/xinyiz/pamrats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is adapted from https://github.com/tkipf/gae/blob/master/gae/train.py and https://github.com/tkipf/pygcn/blob/master/pygcn/train.py##\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import scanpy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "import gae.gae.optimizer as optimizer\n",
    "import gae.gae.model\n",
    "import gae.gae.preprocessing as preprocessing\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "use_cuda=True\n",
    "fastmode=False #Validate during training pass\n",
    "seed=3\n",
    "useSavedMaskedEdges=True\n",
    "maskedgeName='knn20_connectivity'\n",
    "epochs=10000\n",
    "saveFreq=10\n",
    "lr=0.001 #initial learning rate\n",
    "lr_adv=0.001\n",
    "weight_decay=0 #Weight for L2 loss on embedding matrix.\n",
    "\n",
    "hidden1=6000 #Number of units in hidden layer 1\n",
    "hidden2=6000 #Number of units in hidden layer 2\n",
    "# hidden3=2048\n",
    "# hidden4=2048\n",
    "# hidden5=128\n",
    "fc_dim1=6000\n",
    "# fc_dim2=128\n",
    "# fc_dim3=128\n",
    "# fc_dim4=128\n",
    "# gcn_dim1=2600\n",
    "# clf_hidden=256\n",
    "adv_hidden=128\n",
    "\n",
    "dropout=0.01\n",
    "testNodes=0.1 #fraction of total nodes for testing\n",
    "valNodes=0.05 #fraction of total nodes for validation\n",
    "XreconWeight=20\n",
    "# clfweight=20\n",
    "advWeight=2\n",
    "# randFeatureSubset=None\n",
    "model_str='gcn_vae_xa_e2_d1_dca'\n",
    "clf=None\n",
    "adv=None  #'clf_fc1_eq'  #'clf_fc1_control_eq' #'clf_fc1_control'  #'clf_fc1'\n",
    "protein=None #'scaled_binary'\n",
    "proteinWeights=0.05\n",
    "adj_decodeName=None #gala or None\n",
    "ridgeL=0.01\n",
    "shareGenePi=True\n",
    "\n",
    "pretrainedAE=None #{'name':'controlphy5XAbin_01_dca','epoch':9990}\n",
    "num_features=2112\n",
    "# training_samples=['control13','disease13','disease8']###rename and retrain C8\n",
    "# training_samples=['control13','disease13','disease8','control8']\n",
    "training_samples=['disease13','control13','disease8','control8']\n",
    "# training_samples=['control13','control8']\n",
    "targetBatch=None\n",
    "training_sample_X='logminmax'\n",
    "switchFreq=10\n",
    "standardizeX=False\n",
    "# name='allk20XA_01_dca_noD8' ###rename and retrain D8 C8\n",
    "name='allk20XA_02_dca_over'\n",
    "logsavepath='/mnt/xinyi/pamrats/log/train_gae_starmap/'+name\n",
    "modelsavepath='/mnt/xinyi/pamrats/models/train_gae_starmap/'+name\n",
    "plotsavepath='/mnt/xinyi/pamrats/plots/train_gae_starmap/'+name\n",
    "\n",
    "#Load data\n",
    "sampleidx={'disease13':'AD_mouse9494','control13':'AD_mouse9498','disease8':'AD_mouse9723','control8':'AD_mouse9735'}\n",
    "savedir=os.path.join('/mnt/xinyi/','starmap')\n",
    "adj_dir=os.path.join(savedir,'a')\n",
    "\n",
    "featureslist={}\n",
    "if training_sample_X in ['corrected','scaled']:\n",
    "    scaleddata=scanpy.read_h5ad('/mnt/xinyi/2021-01-13-mAD-test-dataset/2020-12-27-starmap-mAD-scaled.h5ad')\n",
    "    \n",
    "    for s in sampleidx.keys():\n",
    "        featureslist[s+'X_'+'corrected']=torch.tensor(scaleddata.layers['corrected'][scaleddata.obs['sample']==sampleidx[s]])\n",
    "        featureslist[s+'X_'+'scaled']=torch.tensor(scaleddata.layers['scaled'][scaleddata.obs['sample']==sampleidx[s]])\n",
    "\n",
    "else:\n",
    "    scaleddata=scanpy.read_h5ad('/mnt/xinyi/2021-01-13-mAD-test-dataset/2020-12-27-starmap-mAD-raw.h5ad')\n",
    "    \n",
    "    for s in sampleidx.keys():\n",
    "        scaleddata_train=scaleddata.X[scaleddata.obs['sample']==sampleidx[s]]\n",
    "\n",
    "        if training_sample_X=='logminmax':\n",
    "            featurelog_train=np.log2(scaleddata_train+1/2)\n",
    "            scaler = MinMaxScaler()\n",
    "            featurelog_train_minmax=np.transpose(scaler.fit_transform(np.transpose(featurelog_train)))\n",
    "            featureslist[s+'X_'+training_sample_X]=torch.tensor(featurelog_train_minmax)\n",
    "        elif training_sample_X=='logminmax10':\n",
    "            featurelog_train=np.log2(scaleddata_train+1/2)\n",
    "            scaler = MinMaxScaler(feature_range=(0,10))\n",
    "            featurelog_train_minmax=np.transpose(scaler.fit_transform(np.transpose(featurelog_train)))\n",
    "            featureslist[s+'X_'+training_sample_X]=torch.tensor(featurelog_train_minmax)\n",
    "\n",
    "if protein:\n",
    "    proteinsavepath=os.path.join('/mnt/xinyi/','starmap','protein')\n",
    "    for s in sampleidx.keys():\n",
    "        pmtx=sp.load_npz(os.path.join(proteinsavepath,sampleidx[s]+'_'+protein+'.npz'))\n",
    "        pmtx=preprocessing.sparse_mx_to_torch_sparse_tensor(pmtx)\n",
    "        pmtx=pmtx.to_dense()\n",
    "        scalefactor=torch.sum(featureslist[s+'X_'+training_sample_X])/torch.sum(pmtx)*proteinWeights\n",
    "        featureslist[s+'X_'+training_sample_X]=torch.cat((featureslist[s+'X_'+training_sample_X],pmtx*scalefactor),dim=1)\n",
    "            \n",
    "if clf:\n",
    "    ctlist={}\n",
    "    ctlist['disease13']=scaleddata.obs.loc[scaleddata.obs['sample']=='AD_mouse9494','top_level'].to_numpy()\n",
    "    ctlist['control13']=scaleddata.obs.loc[scaleddata.obs['sample']=='AD_mouse9498','top_level'].to_numpy()\n",
    "    ctlist['disease8']=scaleddata.obs.loc[scaleddata.obs['sample']=='AD_mouse9723','top_level'].to_numpy()\n",
    "    ctlist['control8']=scaleddata.obs.loc[scaleddata.obs['sample']=='AD_mouse9735','top_level'].to_numpy()\n",
    "    ct_unique=np.unique(scaleddata.obs.loc[scaleddata.obs['sample']=='AD_mouse9735','top_level'])\n",
    "    for k in ctlist.keys():\n",
    "        for i in range(ct_unique.size):\n",
    "            ctlist[k][ctlist[k]==ct_unique[i]]=i\n",
    "    \n",
    "adj_list={}\n",
    "adj_list['disease13']=sp.load_npz(os.path.join(adj_dir,maskedgeName+'_AD_mouse9494.npz'))\n",
    "adj_list['control13']=sp.load_npz(os.path.join(adj_dir,maskedgeName+'_AD_mouse9498.npz'))\n",
    "adj_list['disease8']=sp.load_npz(os.path.join(adj_dir,maskedgeName+'_AD_mouse9723.npz'))\n",
    "adj_list['control8']=sp.load_npz(os.path.join(adj_dir,maskedgeName+'_AD_mouse9735.npz'))\n",
    "\n",
    "adjnormlist={}\n",
    "pos_weightlist={}\n",
    "normlist={}\n",
    "for ai in adj_list.keys():\n",
    "    adjnormlist[ai]=preprocessing.preprocess_graph(adj_list[ai])\n",
    "    \n",
    "    pos_weightlist[ai] = torch.tensor(float(adj_list[ai].shape[0] * adj_list[ai].shape[0] - adj_list[ai].sum()) / adj_list[ai].sum()) #using full unmasked adj\n",
    "    normlist[ai] = adj_list[ai].shape[0] * adj_list[ai].shape[0] / float((adj_list[ai].shape[0] * adj_list[ai].shape[0] - adj_list[ai].sum()) * 2)\n",
    "    \n",
    "    adj_label=adj_list[ai] + sp.eye(adj_list[ai].shape[0])\n",
    "    adj_list[ai]=torch.tensor(adj_label.todense())\n",
    "    \n",
    "if adv:\n",
    "    if 'control_eq' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0.5,0.5]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([0.5,0.5]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['control13']=torch.tensor([1,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "    elif 'control' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0,1]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([1,0]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['control13']=torch.tensor([1,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,1]).expand(adjnormlist['control8'].shape[0],-1)        \n",
    "    elif 'eq' in adv:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['disease13']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control13']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['disease8']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([0.5,0.5,0.5,0.5]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['disease13']=torch.tensor([1,0,0,0]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_d['control13']=torch.tensor([0,1,0,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['disease8']=torch.tensor([0,0,1,0]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,0,0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "    else:\n",
    "        sampleLabellist_ae={}\n",
    "        sampleLabellist_ae['disease13']=torch.tensor([0,1,1,1]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_ae['control13']=torch.tensor([1,0,1,1]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_ae['disease8']=torch.tensor([1,1,0,1]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_ae['control8']=torch.tensor([1,1,1,0]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "\n",
    "        sampleLabellist_d={}\n",
    "        sampleLabellist_d['disease13']=torch.tensor([1,0,0,0]).expand(adjnormlist['disease13'].shape[0],-1)\n",
    "        sampleLabellist_d['control13']=torch.tensor([0,1,0,0]).expand(adjnormlist['control13'].shape[0],-1)\n",
    "        sampleLabellist_d['disease8']=torch.tensor([0,0,1,0]).expand(adjnormlist['disease8'].shape[0],-1)\n",
    "        sampleLabellist_d['control8']=torch.tensor([0,0,0,1]).expand(adjnormlist['control8'].shape[0],-1)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "# adj_decode=None\n",
    "# if adj_decodeName == 'gala':\n",
    "#     adj_decode=preprocessing.preprocess_graph_sharp(adj_train)\n",
    "\n",
    "\n",
    "\n",
    "if 'dca' in model_str:\n",
    "    rawdata=scanpy.read_h5ad('/mnt/xinyi/2021-01-13-mAD-test-dataset/2020-12-27-starmap-mAD-raw.h5ad')\n",
    "    features_raw_list={}\n",
    "    for s in sampleidx.keys():\n",
    "#         features_raw_list[s+'X_'+'raw']=torch.tensor(rawdata.X[rawdata.obs['sample']==sampleidx[s]]).cuda()\n",
    "        features_raw_list[s+'X_'+'raw']=torch.tensor(rawdata.X[rawdata.obs['sample']==sampleidx[s]])\n",
    "        if protein:\n",
    "            proteinsavepath=os.path.join('/mnt/xinyi/','starmap','protein')\n",
    "            pmtx=sp.load_npz(os.path.join(proteinsavepath,sampleidx[s]+'_'+protein+'.npz'))\n",
    "            pmtx=preprocessing.sparse_mx_to_torch_sparse_tensor(pmtx)\n",
    "            pmtx=pmtx.to_dense()\n",
    "            scalefactor=torch.sum(features_raw_list[s+'X_raw'])/torch.sum(pmtx)*proteinWeights\n",
    "            features_raw_list[s+'X_raw']=torch.cat((features_raw_list[s+'X_raw'],pmtx*scalefactor),dim=1)\n",
    "\n",
    "if clf:\n",
    "    ct_train=ctlist[training_samples].astype(int)\n",
    "# features=torch.tensor(np.identity(7000)).float()\n",
    "# features=features[0:10,:]\n",
    "if standardizeX:\n",
    "    features=torch.tensor(scale(features,axis=0, with_mean=True, with_std=True, copy=True))\n",
    "\n",
    "# Set cuda and seed\n",
    "np.random.seed(seed)\n",
    "if use_cuda and (not torch.cuda.is_available()):\n",
    "    print('cuda not available')\n",
    "    use_cuda=False\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "# Load data\n",
    "# if randFeatureSubset != None:\n",
    "#     idx=np.random.choice(features.shape[1],randFeatureSubset,replace=False)\n",
    "#     features=features[:,idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(logsavepath):\n",
    "    os.mkdir(logsavepath)\n",
    "if not os.path.exists(modelsavepath):\n",
    "    os.mkdir(modelsavepath)\n",
    "if not os.path.exists(plotsavepath):\n",
    "    os.mkdir(plotsavepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all train/validation sets\n",
    "\n",
    "mse=torch.nn.MSELoss()\n",
    "# mse=torch.nn.MSELoss(reduction=None)\n",
    "# Create model\n",
    "if model_str=='gcn_vae_xa':\n",
    "    model = gae.gae.model.GCNModelVAE_XA(num_features, hidden1, hidden2,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "elif model_str == 'gcn_vae_gcnX_inprA':\n",
    "    model = gae.gae.model.GCNModelVAE_gcnX_inprA(num_features, hidden1, hidden2,gcn_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str == 'gcn_vae_gcnX_inprA_w':\n",
    "    model = gae.gae.model.GCNModelVAE_gcnX_inprA_w(num_features, hidden1, hidden2,gcn_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e3':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e3(num_features, hidden1, hidden2,hidden3,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str == 'gcn_vae_xa_e1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e1(num_features, hidden1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str == 'gcn_vae_xa_e2_d1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1(num_features, hidden1,hidden2, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e2_d1_dca':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dca_fca':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA_fca(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaFork':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCAfork(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaElemPi':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCAelemPi(num_features, hidden1,hidden2,fc_dim1, dropout,shareGenePi)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaConstantDisp':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA_constantDisp(num_features, hidden1,hidden2,fc_dim1, dropout,shareGenePi)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str == 'gcn_vae_xa_e4_d1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e4_d1(num_features, hidden1,hidden2,hidden3,hidden4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='fc':\n",
    "    model = gae.gae.model.FCVAE(num_features, hidden1, hidden2,hidden3,hidden4,hidden5,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "elif model_str=='fcae':\n",
    "    model = gae.gae.model.FCAE(num_features, hidden1, hidden2,hidden3,hidden4,hidden5,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "\n",
    "elif model_str=='fcae1':\n",
    "    model = gae.gae.model.FCAE1(num_features, dropout,hidden1)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "elif model_str=='fcae2':\n",
    "    model = gae.gae.model.FCAE2(num_features, dropout,hidden1,hidden2)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "\n",
    "elif model_str=='fc1':\n",
    "    model = gae.gae.model.FCVAE1(num_features, hidden1,dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='fc1_fca':\n",
    "    model = gae.gae.model.FCVAE1_fca(num_features, hidden1,dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='fc1_dca':\n",
    "    model = gae.gae.model.FCVAE1_DCA(num_features, hidden1,fc_dim1, dropout)\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "if clf=='clf_fc1':\n",
    "    modelClf=gae.gae.model.Clf_fc1(hidden2, dropout,clf_hidden,ct_unique.size)\n",
    "    loss_clf=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "if adv=='clf_fc1' or adv=='clf_fc1_eq' or adv=='clf_fc1_control' or adv=='clf_fc1_control_eq':\n",
    "    modelAdv=gae.gae.model.Clf_fc1(hidden2, dropout,adv_hidden,sampleLabellist_ae['control13'].size()[1])\n",
    "    loss_adv=optimizer.optimizer_CEclf\n",
    "    \n",
    "if adv=='clf_linear1' or adv=='clf_linear1_control':\n",
    "    modelAdv=gae.gae.model.Clf_linear1(hidden2, dropout,sampleLabellist_ae['control13'].size()[1])\n",
    "    loss_adv=optimizer.optimizer_CEclf\n",
    "    \n",
    "if 'NB' in name:\n",
    "    print('using NB loss for X')\n",
    "    loss_x=optimizer.optimizer_nb\n",
    "    \n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    if clf:\n",
    "        modelClf.cuda()\n",
    "        ct_train=torch.tensor(ct_train).cuda()\n",
    "    if adv:\n",
    "        modelAdv.cuda()\n",
    "#         for sk in sampleLabellist_ae.keys():\n",
    "#             sampleLabellist_ae[sk]=sampleLabellist_ae[sk].cuda().float()\n",
    "#             sampleLabellist_d[sk]=sampleLabellist_d[sk].cuda().float()\n",
    "#     for fk in featureslist.keys():\n",
    "#         featureslist[fk] = featureslist[fk].cuda().float()\n",
    "#     for ak in adj_list.keys():\n",
    "#         adjnormlist[ak] =adjnormlist[ak].cuda()\n",
    "#         adj_list[ak] = adj_list[ak].cuda().float()\n",
    "#     if adj_decode is not None:\n",
    "#         adj_decode=adj_decode.cuda()\n",
    "    \n",
    "\n",
    "optimizerVAEXA = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "if clf:\n",
    "    optimizerClf=optim.Adam(modelClf.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "if adv:\n",
    "    optimizerAdv=optim.Adam(modelAdv.parameters(), lr=lr_adv, weight_decay=weight_decay)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease13 Epoch: 0000 loss_train: 40.1984 loss_kl_train: 0.0010 loss_x_train: 8.0066 loss_a_train: 32.1908 loss_val: 19.1585 loss_x_val: 7.6767 loss_a_val: 11.4818 time: 3.5552s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 11.91 GiB total capacity; 10.63 GiB already allocated; 58.56 MiB free; 658.88 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c58aa572f8f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mtrain_loss_advD_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_advD_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mtrain_loss_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_kl_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_x_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_a_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_x_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_a_ep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c58aa572f8f7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mloss_a_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_nodes_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0madj_recon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pamrats/gae/gae/optimizer.py\u001b[0m in \u001b[0;36moptimizer_CE\u001b[0;34m(preds, labels, pos_weight, norm, nodemask)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnodemask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnodemask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnodemask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnodemask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 11.91 GiB total capacity; 10.63 GiB already allocated; 58.56 MiB free; 658.88 MiB cached)"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(modelsavepath,str(9360)+'.pt')))\n",
    "# epochs=20000\n",
    "if pretrainedAE:\n",
    "    print('loading '+pretrainedAE['name']+' epoch '+str(pretrainedAE['epoch']))\n",
    "    model.load_state_dict(torch.load(os.path.join('/mnt/xinyi/pamrats/models/train_gae_starmap/'+pretrainedAE['name'],str(pretrainedAE['epoch'])+'.pt')))\n",
    "    \n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizerVAEXA.zero_grad()\n",
    "    \n",
    "    if adj_decodeName==None:\n",
    "        adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "    #     features_recon, z, mu, logvar=model(features.float())\n",
    "    else:\n",
    "        adj_recon,mu,logvar,z,features_recon = model(features, adj_norm,adj_decode)\n",
    "        \n",
    "    \n",
    "    if clf:\n",
    "        modelClf.train()\n",
    "        optimizerClf.zero_grad()\n",
    "        clfOut=modelClf(z)\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        modelAdv.eval()\n",
    "        advOut=modelAdv(z)\n",
    "    \n",
    "    z=None\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    loss_kl_train=loss_kl(mu, logvar, train_nodes_idx)\n",
    "    \n",
    "    mu=None\n",
    "    logvar=None\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight,mse)\n",
    "    \n",
    "    features_recon=None\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    loss_a_train=loss_a(adj_recon, adj_label, pos_weight, norm,train_nodes_idx)\n",
    "    \n",
    "    adj_recon=None\n",
    "#     train_nodes_idx=None\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    loss = loss_kl_train+loss_x_train+loss_a_train\n",
    "#     loss=loss_x_train #for lossXreconOnly only\n",
    "#     loss=loss_kl_train+loss_x_train #for lossXreconOnly_wKL only\n",
    "#     loss = loss_kl_train+loss_a_train #for lossAreconOnly_wKL only\n",
    "    if clf:\n",
    "        loss_clf_train=loss_clf(clfOut[train_nodes_idx],ct_train[train_nodes_idx])\n",
    "        loss=loss+clfweight*loss_clf_train\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        loss_adv_train=loss_adv(advOut,sampleLabel_ae,train_nodes_idx)\n",
    "        loss+=loss_adv_train*advWeight\n",
    "    loss.backward()\n",
    "    optimizerVAEXA.step()\n",
    "    if clf:\n",
    "        optimizerClf.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run & no variation in z.\n",
    "        model.eval()\n",
    "        if adj_decodeName==None:\n",
    "            adj_recon,mu,logvar,z, features_recon = model(features, adj_norm)\n",
    "    #         features_recon, z, mu, logvar=model(features.float())\n",
    "        else:\n",
    "            adj_recon,mu,logvar,z, features_recon = model(features, adj_norm,adj_decode)\n",
    "    \n",
    "    if clf:\n",
    "        modelClf.eval()\n",
    "        clfOut=modelClf(z)\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        advOut=modelAdv(z)\n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight,mse)\n",
    "    \n",
    "    features_recon=None\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    loss_a_val=loss_a(adj_recon, adj_label, pos_weight, norm,val_nodes_idx)\n",
    "    \n",
    "    adj_recon=None\n",
    "#     val_nodes_idx=None\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    loss_val = loss_x_val+loss_a_val\n",
    "#     loss_val=loss_x_val\n",
    "#     loss_val=loss_a_val\n",
    "    if clf:\n",
    "        loss_clf_val=loss_clf(clfOut[val_nodes_idx],ct_train[val_nodes_idx])\n",
    "        loss_val=loss_val+clfweight*loss_clf_val\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        loss_adv_val=loss_adv(advOut,sampleLabel_ae,val_nodes_idx)\n",
    "        loss_val+=loss_adv_val*advWeight\n",
    "#     print(features_recon[:5,:5])\n",
    "    print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "          'loss_train: {:.4f}'.format(loss.item()),\n",
    "          'loss_kl_train: {:.4f}'.format(loss_kl_train.item()),\n",
    "          'loss_x_train: {:.4f}'.format(loss_x_train.item()),\n",
    "          'loss_a_train: {:.4f}'.format(loss_a_train.item()),\n",
    "#           'loss_clf_train: {:.4f}'.format(loss_clf_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'loss_x_val: {:.4f}'.format(loss_x_val.item()),\n",
    "          'loss_a_val: {:.4f}'.format(loss_a_val.item()),\n",
    "#           'loss_clf_val: {:.4f}'.format(loss_clf_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        print('loss_adv_train: {:.4f}'.format(loss_adv_train.item()),\n",
    "              'loss_adv_val: {:.4f}'.format(loss_adv_val.item())\n",
    "             )\n",
    "#     return loss.item(),loss_x_train.item(),loss_val.item(),loss_x_val.item()\n",
    "#     return loss.item(),loss_kl_train.item(),loss_x_train.item(),loss_val.item(),loss_x_val.item()\n",
    "    if clf:\n",
    "        return loss.item(),loss_kl_train.item(),loss_x_train.item(),loss_a_train.item(),loss_val.item(),loss_x_val.item(),loss_a_val.item(),loss_clf_train.item(),loss_clf_val.item()\n",
    "    elif adv:\n",
    "        if (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "            return float(loss),float(loss_kl_train),float(loss_x_train),float(loss_a_train),float(loss_val),float(loss_x_val),float(loss_a_val),float(loss_adv_train),float(loss_adv_val)        \n",
    "        else:\n",
    "            return float(loss),float(loss_kl_train),float(loss_x_train),float(loss_a_train),float(loss_val),float(loss_x_val),float(loss_a_val),None,None   \n",
    "    else:\n",
    "        return loss.item(),loss_kl_train.item(),loss_x_train.item(),loss_a_train.item(),loss_val.item(),loss_x_val.item(),loss_a_val.item()        \n",
    "\n",
    "def train_discriminator(epoch):\n",
    "    t = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    if adj_decodeName==None:\n",
    "        adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "    #     features_recon, z, mu, logvar=model(features.float())\n",
    "    else:\n",
    "        adj_recon,mu,logvar,z,features_recon = model(features, adj_norm,adj_decode)\n",
    "        \n",
    "    \n",
    "    if clf:\n",
    "        modelClf.eval()\n",
    "        clfOut=modelClf(z)\n",
    "        \n",
    "    modelAdv.train()\n",
    "    optimizerAdv.zero_grad()\n",
    "    advOut=modelAdv(z)\n",
    "    \n",
    "    loss_adv_train=loss_adv(advOut,sampleLabel_d,train_nodes_idx)\n",
    "    loss = loss_adv_train*advWeight\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizerAdv.step()\n",
    "\n",
    "    modelAdv.eval()\n",
    "    advOut=modelAdv(z)\n",
    "    loss_adv_val=loss_adv(advOut,sampleLabel_d,val_nodes_idx)\n",
    "    loss_val=loss_adv_val*advWeight\n",
    "    print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "          'loss_adv_train: {:.4f}'.format(loss_adv_train.item()),\n",
    "          'loss_adv_val: {:.4f}'.format(loss_adv_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    return float(loss_adv_train),float(loss_adv_val)\n",
    "    \n",
    "# print('cross-validation ',seti)\n",
    "train_loss_ep=[None]*epochs\n",
    "train_loss_kl_ep=[None]*epochs\n",
    "train_loss_x_ep=[None]*epochs\n",
    "train_loss_a_ep=[None]*epochs\n",
    "train_loss_clf_ep=[None]*epochs\n",
    "train_loss_adv_ep=[None]*epochs\n",
    "train_loss_advD_ep=[None]*epochs\n",
    "val_loss_ep=[None]*epochs\n",
    "val_loss_x_ep=[None]*epochs\n",
    "val_loss_a_ep=[None]*epochs\n",
    "val_loss_clf_ep=[None]*epochs\n",
    "val_loss_adv_ep=[None]*epochs\n",
    "val_loss_advD_ep=[None]*epochs\n",
    "t_ep=time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "# for ep in range(10000,20000):\n",
    "    t=int(ep/switchFreq)%len(training_samples)\n",
    "    training_samples_t=training_samples[t]\n",
    "    \n",
    "#     adj_norm=adjnormlist[training_samples_t]\n",
    "#     adj_label=adj_list[training_samples_t]\n",
    "#     features=featureslist[training_samples_t+'X_'+training_sample_X]\n",
    "    adj_norm=adjnormlist[training_samples_t].cuda().float()\n",
    "    adj_label=adj_list[training_samples_t].cuda().float()\n",
    "    features=featureslist[training_samples_t+'X_'+training_sample_X].cuda().float()\n",
    "    pos_weight=pos_weightlist[training_samples_t]\n",
    "    norm=normlist[training_samples_t]\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())):\n",
    "#         sampleLabel_ae=sampleLabellist_ae[training_samples_t]\n",
    "#         sampleLabel_d=sampleLabellist_d[training_samples_t]\n",
    "        sampleLabel_ae=sampleLabellist_ae[training_samples_t].cuda().float()\n",
    "        sampleLabel_d=sampleLabellist_d[training_samples_t].cuda().float()\n",
    "    if 'dca' in model_str:\n",
    "#         features_raw=features_raw_list[training_samples_t+'X_raw']\n",
    "        features_raw=features_raw_list[training_samples_t+'X_raw'].cuda()\n",
    "    num_nodes,_ = features.shape\n",
    "    \n",
    "    maskpath=os.path.join(savedir,'trainMask',training_samples_t+'_'+maskedgeName+'_seed'+str(seed)+'.pkl')\n",
    "    if useSavedMaskedEdges and os.path.exists(maskpath):\n",
    "#         print('opening saved')\n",
    "        with open(maskpath, 'rb') as input:\n",
    "            maskedgeres = pickle.load(input)\n",
    "    else:\n",
    "        # construct training, validation, and test sets\n",
    "        maskedgeres= preprocessing.mask_nodes_edges(features.shape[0],testNodeSize=testNodes,valNodeSize=valNodes,seed=seed)\n",
    "        with open(maskpath, 'wb') as output:\n",
    "            pickle.dump(maskedgeres, output, pickle.HIGHEST_PROTOCOL)\n",
    "    train_nodes_idx,val_nodes_idx,test_nodes_idx = maskedgeres\n",
    "    if use_cuda:\n",
    "        train_nodes_idx=train_nodes_idx.cuda()\n",
    "        val_nodes_idx=val_nodes_idx.cuda()\n",
    "        test_nodes_idx=test_nodes_idx.cuda()\n",
    "    \n",
    "    if clf:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep],train_loss_clf_ep[ep],val_loss_clf_ep[ep]=train(ep)\n",
    "    elif adv:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep],train_loss_adv_ep[ep],val_loss_adv_ep[ep]=train(ep)\n",
    "        if (training_samples_t in list(sampleLabellist_ae.keys())):\n",
    "            train_loss_advD_ep[ep],val_loss_advD_ep[ep]=train_discriminator(ep)\n",
    "    else:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep]=train(ep)\n",
    "\n",
    "        \n",
    "    if ep%saveFreq == 0:\n",
    "        torch.save(model.cpu().state_dict(), os.path.join(modelsavepath,str(ep)+'.pt'))\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()\n",
    "print(' total time: {:.4f}s'.format(time.time() - t_ep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(logsavepath,'train_loss'), 'rb') as input:\n",
    "#     train_loss_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_kl'), 'rb') as input:\n",
    "#     train_loss_kl_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_x'), 'rb') as input:\n",
    "#     train_loss_x_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_a'), 'rb') as input:\n",
    "#     train_loss_a_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss'), 'rb') as input:\n",
    "#     val_loss_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss_x'), 'rb') as input:\n",
    "#     val_loss_x_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss_a'), 'rb') as input:\n",
    "#     val_loss_a_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_adv'), 'rb') as input:\n",
    "#     train_loss_adv_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss_adv'), 'rb') as input:\n",
    "#     val_loss_adv_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'train_loss_advD'), 'rb') as input:\n",
    "#     train_loss_advD_ep[:9360]=pickle.load(input)\n",
    "# with open(os.path.join(logsavepath,'val_loss_advD'), 'rb') as input:\n",
    "#     val_loss_advD_ep[:9360]=pickle.load(input)\n",
    "    \n",
    "with open(os.path.join(logsavepath,'train_loss'), 'wb') as output:\n",
    "    pickle.dump(train_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_kl'), 'wb') as output:\n",
    "    pickle.dump(train_loss_kl_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_x'), 'wb') as output:\n",
    "    pickle.dump(train_loss_x_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_a'), 'wb') as output:\n",
    "    pickle.dump(train_loss_a_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss'), 'wb') as output:\n",
    "    pickle.dump(val_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss_x'), 'wb') as output:\n",
    "    pickle.dump(val_loss_x_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss_a'), 'wb') as output:\n",
    "    pickle.dump(val_loss_a_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "if clf:\n",
    "    with open(os.path.join(logsavepath,'train_loss_clf'), 'wb') as output:\n",
    "        pickle.dump(train_loss_clf_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_clf'), 'wb') as output:\n",
    "        pickle.dump(val_loss_clf_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "if adv:\n",
    "    with open(os.path.join(logsavepath,'train_loss_adv'), 'wb') as output:\n",
    "        pickle.dump(train_loss_adv_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_adv'), 'wb') as output:\n",
    "        pickle.dump(val_loss_adv_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'train_loss_advD'), 'wb') as output:\n",
    "        pickle.dump(train_loss_advD_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_advD'), 'wb') as output:\n",
    "        pickle.dump(val_loss_advD_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(os.path.join(logsavepath,'ct_unique'), 'wb') as output:\n",
    "#     pickle.dump(ct_unique, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(epochs),train_loss_ep)\n",
    "# plt.plot(np.arange(epochs),val_loss_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_x_ep)\n",
    "plt.plot(np.arange(epochs),val_loss_x_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_a_ep)\n",
    "plt.plot(np.arange(epochs),val_loss_a_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "# plt.plot(np.arange(epochs),np.array(train_loss_adv_ep)*advWeight)\n",
    "# plt.plot(np.arange(epochs),np.array(val_loss_adv_ep)*advWeight)\n",
    "# plt.plot(np.arange(epochs),np.array(train_loss_advD_ep)*advWeight)\n",
    "# plt.plot(np.arange(epochs),np.array(val_loss_advD_ep)*advWeight)\n",
    "# plt.plot(np.arange(epochs),np.array(train_loss_clf_ep)*clfweight)\n",
    "# plt.plot(np.arange(epochs),np.array(val_loss_clf_ep)*clfweight)\n",
    "# plt.ylim((0,15))\n",
    "# plt.xlim((0,3000))\n",
    "# plt.legend(['training loss','validation loss','training x recon loss','validation x recon loss','training kl loss'],loc='upper right')\n",
    "# plt.legend(['training x recon loss','validation x recon loss','training a recon loss','validation a recon loss','training kl loss','training classifier loss','validation classifier loss'],loc='upper right')\n",
    "plt.legend(['training x recon loss','validation x recon loss','training a recon loss','validation a recon loss','training kl loss','training discriminator ae','validation discriminator ae','training discriminator d','validation discriminator d'],loc='upper right')\n",
    "# plt.legend(['training loss','validation loss','training x recon loss','validation x recon loss','training a recon loss','validation a recon loss','training kl loss','training discriminator ae','validation discriminator ae','training discriminator d','validation discriminator d'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath,'loss_seed3_xa.jpg'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41656726598739624"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name='c13k20XA_07_dca'\n",
    "# logsavepath='/mnt/xinyi/pamrats/log/train_gae_starmap/'+name\n",
    "# with open(os.path.join(logsavepath,'val_loss_a'), 'rb') as output:\n",
    "#     val_loss_a_ep=pickle.load(output)\n",
    "np.argmin(val_loss_x_ep[:])\n",
    "# np.where(np.logical_not(np.isfinite(val_loss_ep[:])))\n",
    "val_loss_a_ep[8700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease13\n",
      "loss_adv_test: 0.8363\n",
      "test results loss_test: 1.9879 loss_x_test: 1.4408 loss_a_test: 0.5471\n",
      "control13\n",
      "loss_adv_test: 0.8378\n",
      "test results loss_test: 1.8927 loss_x_test: 1.3481 loss_a_test: 0.5446\n",
      "disease8\n",
      "loss_adv_test: 0.8383\n",
      "test results loss_test: 1.7926 loss_x_test: 1.2689 loss_a_test: 0.5237\n",
      "control8\n",
      "loss_adv_test: 0.8379\n",
      "test results loss_test: 1.9069 loss_x_test: 1.3716 loss_a_test: 0.5353\n"
     ]
    }
   ],
   "source": [
    "testepoch=9990\n",
    "model.load_state_dict(torch.load(os.path.join(modelsavepath,str(testepoch)+'.pt')))\n",
    "model.eval()\n",
    "for s in sampleidx.keys():\n",
    "    print(s)\n",
    "    \n",
    "    adj_norm=adjnormlist[s].cuda().float()\n",
    "    adj_label=adj_list[s].cuda().float()\n",
    "    features=featureslist[s+'X_'+training_sample_X].cuda().float()\n",
    "    pos_weight=pos_weightlist[s]\n",
    "    norm=normlist[s]\n",
    "    \n",
    "    if 'dca' in model_str:\n",
    "        features_raw=features_raw_list[s+'X_raw'].cuda()\n",
    "    num_nodes,num_features = features.shape\n",
    "    maskpath=os.path.join(savedir,'trainMask',s+'_'+maskedgeName+'_seed'+str(seed)+'.pkl')\n",
    "    if useSavedMaskedEdges and os.path.exists(maskpath):\n",
    "#         print('opening saved')\n",
    "        with open(maskpath, 'rb') as input:\n",
    "            maskedgeres = pickle.load(input)\n",
    "    else:\n",
    "        # construct training, validation, and test sets\n",
    "        maskedgeres= preprocessing.mask_nodes_edges(features.shape[0],testNodeSize=testNodes,valNodeSize=valNodes)\n",
    "        with open(maskpath, 'wb') as output:\n",
    "            pickle.dump(maskedgeres, output, pickle.HIGHEST_PROTOCOL)\n",
    "    train_nodes_idx,val_nodes_idx,test_nodes_idx = maskedgeres\n",
    "    \n",
    "    if s in training_samples:\n",
    "        test_nodes_idx_s=test_nodes_idx\n",
    "    else:\n",
    "        test_nodes_idx_s=torch.tensor(np.arange(num_nodes))\n",
    "        \n",
    "#     adj_decode=None\n",
    "#     if adj_decodeName == 'gala':\n",
    "#         adj_decode=preprocessing.preprocess_graph_sharp(adj)\n",
    "\n",
    "    if adj_decodeName==None:\n",
    "        adj_recon,mu,logvar,z, features_recon = model(features, adj_norm)\n",
    "    else:\n",
    "        adj_decode=adj_decode.cuda()\n",
    "        adj_recon,mu,logvar,z, features_recon = model(features, adj_norm,adj_decode)\n",
    "    if adv and (s in list(sampleLabellist_ae.keys())):\n",
    "        sampleLabel_ae=sampleLabellist_ae[s].cuda().float()\n",
    "        modelAdv.eval()\n",
    "        advOut=modelAdv(z)\n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_test=loss_x(features_recon, features,test_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_test=loss_x(features_recon, features,test_nodes_idx_s,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_test=loss_x(features_recon, features,test_nodes_idx_s,XreconWeight,mse)\n",
    "    loss_a_test=loss_a(adj_recon, adj_label, pos_weight, norm,test_nodes_idx_s)\n",
    "    loss_test = loss_x_test+loss_a_test\n",
    "    \n",
    "    if adv and (s in list(sampleLabellist_ae.keys())):\n",
    "        loss_adv_test=loss_adv(advOut,sampleLabel_ae,test_nodes_idx)\n",
    "        print('loss_adv_test: {:.4f}'.format(loss_adv_test.item()))\n",
    "        \n",
    "    print('test results',\n",
    "          'loss_test: {:.4f}'.format(loss_test.item()),\n",
    "          'loss_x_test: {:.4f}'.format(loss_x_test.item()),\n",
    "          'loss_a_test: {:.4f}'.format(loss_a_test.item()))\n",
    "#          'loss_adv_test: {:.4f}'.format(loss_adv_test.item()))\n",
    "    if protein:\n",
    "        test_nodes_idx_s_genes=torch.clone(test_nodes_idx_s)\n",
    "        test_nodes_idx_s_genes[2112:]=0\n",
    "        test_nodes_idx_s_proteins=torch.clone(test_nodes_idx_s)\n",
    "        test_nodes_idx_s_proteins[:2112]=0\n",
    "        if 'dca' in model_str:\n",
    "            loss_genes_test=loss_x(features_recon, features,test_nodes_idx_s_genes,XreconWeight,ridgeL,features_raw)\n",
    "            loss_proteins_test=loss_x(features_recon, features,test_nodes_idx_s_proteins,XreconWeight,ridgeL,features_raw)\n",
    "        print('loss_x_genes: {:.4f}'.format(loss_genes_test.item()),\n",
    "          'loss_x_proteins: {:.4f}'.format(loss_proteins_test.item()))\n",
    "    if 'dca' in model_str:\n",
    "        features_raw=features_raw.cpu()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=features\n",
    "y_true_raw=features_raw\n",
    "preds=features_recon\n",
    "mask=nodesmask=torch.tensor(np.arange(num_nodes))\n",
    "reconWeight=20\n",
    "eps = 1e-10\n",
    "\n",
    "output,pi,theta,y_pred=preds\n",
    "nb_case=optimizer.optimizer_nb(preds,y_true,mask,reconWeight,eps = 1e-10,ifmean=False)- torch.log(pi+eps)\n",
    "\n",
    "zero_nb = torch.pow(theta/(theta+y_pred+eps), theta)\n",
    "zero_case = -torch.log(pi + ((1.0-pi)*zero_nb)+eps)\n",
    "result = torch.where(torch.lt(y_true_raw.cuda(), 1), zero_case, nb_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000e+00, 2.1077e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "         -0.0000e+00, -0.0000e+00],\n",
       "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "         -0.0000e+00, -0.0000e+00],\n",
       "        [1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "         1.1921e-07, -0.0000e+00],\n",
       "        [1.7968e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.7740e+00,\n",
       "         1.1915e+00, -0.0000e+00],\n",
       "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "         1.1921e-07, -0.0000e+00],\n",
       "        [1.1921e-07, 1.1921e-07, 1.1921e-07, -0.0000e+00, 1.1921e-07, 1.1921e-07,\n",
       "         1.1921e-07, 1.1921e-07],\n",
       "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "         -0.0000e+00, -0.0000e+00],\n",
       "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "         1.4272e+00, -0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:8,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2504,  8.5853, -0.1941, -0.1474, -0.1934, -0.2717, -0.5645, -0.3465],\n",
       "        [-0.2504, -0.1938, -0.1941, -0.1474, -0.1934, -0.2717, -0.5645, -0.3465],\n",
       "        [-0.2504, -0.1938, -0.1941, -0.1474, -0.1934, -0.2717, -0.5645, -0.3465],\n",
       "        [ 5.5540, -0.1938, -0.1941, -0.1474, -0.1934,  3.1612,  1.2601, -0.3465],\n",
       "        [-0.2504, -0.1938, -0.1941, -0.1474, -0.1934, -0.2717, -0.5645, -0.3465],\n",
       "        [-0.2504, -0.1938, -0.1941, -0.1474, -0.1934, -0.2717, -0.5645, -0.3465],\n",
       "        [-0.2504, -0.1938, -0.1941, -0.1474, -0.1934, -0.2717, -0.5645, -0.3465],\n",
       "        [-0.2504, -0.1938, -0.1941, -0.1474, -0.1934, -0.2717,  2.5718, -0.3465]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:8,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8589e-09, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroidx=(features_raw==0)\n",
    "nonzeroidx=(features_raw!=0)\n",
    "torch.sum(result[zeroidx])/torch.sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(result[zeroidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4cb07b2960f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model.pi.weight[:8,:8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_str' is not defined"
     ]
    }
   ],
   "source": [
    "# model.pi.weight[:8,:8]\n",
    "model_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
